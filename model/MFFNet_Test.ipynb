{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = config.hidden_size\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key   = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.query_att = nn.Linear(self.head_dim, 1, bias=False)\n",
    "        self.m_att     = nn.Linear(self.head_dim, 1, bias=False)\n",
    "        self.out_proj = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "        self.merge_heads = nn.Linear(self.all_head_size, self.all_head_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        init_range = getattr(self.config, \"initializer_range\", 0.02)\n",
    "\n",
    "        nn.init.normal_(self.query.weight, mean=0.0, std=init_range)\n",
    "        if self.query.bias is not None:\n",
    "            nn.init.zeros_(self.query.bias)\n",
    "\n",
    "        nn.init.normal_(self.key.weight, mean=0.0, std=init_range)\n",
    "        if self.key.bias is not None:\n",
    "            nn.init.zeros_(self.key.bias)\n",
    "\n",
    "        nn.init.normal_(self.value.weight, mean=0.0, std=init_range)\n",
    "        if self.value.bias is not None:\n",
    "            nn.init.zeros_(self.value.bias)\n",
    "\n",
    "        nn.init.normal_(self.query_att.weight, mean=0.0, std=init_range)\n",
    "        \n",
    "        nn.init.normal_(self.m_att.weight, mean=0.0, std=init_range)\n",
    "\n",
    "        nn.init.normal_(self.out_proj.weight, mean=0.0, std=init_range)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.zeros_(self.out_proj.bias)\n",
    "\n",
    "        nn.init.normal_(self.merge_heads.weight, mean=0.0, std=init_range)\n",
    "        if self.merge_heads.bias is not None:\n",
    "            nn.init.zeros_(self.merge_heads.bias)\n",
    "\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        [B, S, D] -> [B, H, S, d]\n",
    "        \"\"\"\n",
    "        B, S, D = x.size()\n",
    "        x = x.view(B, S, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def merge_heads_fn(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        [B, H, S, d] -> [B, S, D]\n",
    "        \"\"\"\n",
    "        B, H, S, d = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # [B, S, H, d]\n",
    "        return x.view(B, S, H * d)  # [B, S, D]\n",
    "\n",
    "    def forward(self, query_states, key_states, value_states, attention_mask=None):\n",
    "        B, Sq, _ = query_states.shape\n",
    "        Sk = key_states.size(1)\n",
    "        Q = self.split_heads(self.query(query_states))   # [B, H, Sq, d]\n",
    "        K = self.split_heads(self.key(key_states))       # [B, H, Sk, d]\n",
    "        V = self.split_heads(self.value(value_states))   # [B, H, Sk, d]\n",
    "\n",
    "        # O_i = softmax( W_q^T q_i / sqrt(d) ),  q = sum_i O_i * q_i\n",
    "        aggregator_logits_q = self.query_att(Q) / math.sqrt(self.head_dim)\n",
    "        aggregator_logits_q = aggregator_logits_q.squeeze(-1)  # [B, H, Sq]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            aggregator_logits_q = aggregator_logits_q + attention_mask.view(B, 1, -1)\n",
    "\n",
    "        att_weights_q = self.softmax(aggregator_logits_q)   # [B, H, Sq]\n",
    "        # print(\"Q shape:\", Q.shape)\n",
    "        # print(\"att_weights_q shape:\", att_weights_q.shape)\n",
    "\n",
    "        q_global = torch.einsum('bhsd,bhs->bhd', Q, att_weights_q)  # [B, H, d]\n",
    "\n",
    "        # M = q_global * K (element-wise for each position i)\n",
    "        # M_i = q_global \\odot K_i\n",
    "        qg = q_global.unsqueeze(2)             # [B, H, 1, d]\n",
    "        M  = qg * K                            # [B, H, Sk, d]\n",
    "\n",
    "        # a_i = softmax( W_q^T m_i / sqrt(d) ) over i=1..Sk\n",
    "        aggregator_logits_m = self.m_att(M) / math.sqrt(self.head_dim)  # [B, H, Sk, 1]\n",
    "        aggregator_logits_m = aggregator_logits_m.squeeze(-1)           # [B, H, Sk]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            aggregator_logits_m = aggregator_logits_m + attention_mask.view(B, 1, -1)\n",
    "\n",
    "\n",
    "        att_weights_m = self.softmax(aggregator_logits_m)  # [B, H, Sk]\n",
    "\n",
    "        # 5) k_global = sum_i a_i * m_i  => [B, H, d]\n",
    "        k_global = torch.einsum('bhsd,bhs->bhd', M, att_weights_m)\n",
    " \n",
    "        kg = k_global.unsqueeze(2)                  # [B, H, 1, d]\n",
    "        KV_interaction = kg * V                     # [B, H, Sk, d]\n",
    "        E = self.out_proj(KV_interaction)           # [B, H, Sk, d]\n",
    "\n",
    "        if Sq != Sk:\n",
    "            raise ValueError(\"Fastformer aggregator: mismatch in seq_len (Sq vs Sk).\")\n",
    "\n",
    "        out_heads = E + Q  # shape [B, H, Sq, d]\n",
    "        out = self.merge_heads_fn(out_heads)  # [B, Sq, all_head_size]\n",
    "        #print(\"out after aggregator:\", out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "class FastAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps FastSelfAttention in a minimal 'attention + output-projection' block,\n",
    "    optionally adding a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = FastSelfAttention(config)\n",
    "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query_tensor, key_tensor, value_tensor, attention_mask=None):\n",
    "        self_output = self.self(query_tensor, key_tensor, value_tensor, attention_mask)\n",
    "        # typical residual:\n",
    "        attention_output = self.output(self_output) + query_tensor\n",
    "        attention_output = self.layernorm(attention_output)\n",
    "        return attention_output\n",
    "\n",
    "class FastformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = FastAttention(config)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, key_states, value_states, attention_mask=None):\n",
    "        # 1) Fastformer-based attention\n",
    "        attention_output = self.attention(\n",
    "            hidden_states, key_states, value_states, attention_mask\n",
    "        )\n",
    "        # 2) Feed-forward + residual\n",
    "        inter = self.intermediate(attention_output)\n",
    "        inter = self.activation(inter)\n",
    "        ff_out = self.output(inter)\n",
    "        ff_out = ff_out + attention_output\n",
    "        layer_output = self.layernorm(ff_out)\n",
    "        return layer_output\n",
    "\n",
    "# MSFastformer\n",
    "class MSFastformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Scale Fastformer block:\n",
    "      - LayerNorm + three different 1D Convs (kernel_size=1,3,5)\n",
    "      - Each conv output goes into a FastformerLayer call (paired as in eq. (2))\n",
    "      - Sum the resulting feature maps\n",
    "      - Then FC->GELU->FC + residual, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Single definitions of LayerNorm\n",
    "        self.layernorm_in = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_out = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=5, padding=2)\n",
    "\n",
    "        # Fastformer layers\n",
    "        self.fastformer_1 = FastformerLayer(config)\n",
    "        self.fastformer_3 = FastformerLayer(config)\n",
    "        self.fastformer_5 = FastformerLayer(config)\n",
    "    #     print(\"fastformer_1 out_proj weight:\", \n",
    "    # self.fastformer_1.attention.self.out_proj.weight.shape)\n",
    "\n",
    "    #     print(\"fastformer_3 out_proj weight:\", \n",
    "    # self.fastformer_3.attention.self.out_proj.weight.shape)\n",
    "\n",
    "    #     print(\"fastformer_5 out_proj weight:\", \n",
    "    # self.fastformer_5.attention.self.out_proj.weight.shape)\n",
    "\n",
    "\n",
    "        # Feed-forward\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.output_fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: [B, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        # 1) LN, then conv\n",
    "        x_norm = self.layernorm_in(x)                 # [B, S, D]\n",
    "        x_t = x_norm.transpose(1, 2)                  # [B, D, S]\n",
    "\n",
    "        U1 = self.conv1(x_t).transpose(1, 2)          # [B, S, D]\n",
    "        U3 = self.conv3(x_t).transpose(1, 2)          # [B, S, D]\n",
    "        U5 = self.conv5(x_t).transpose(1, 2)          # [B, S, D]\n",
    "\n",
    "        P1 = self.fastformer_1(U5, U3, U3, attention_mask)\n",
    "        #print(\"P1 shape:\", P1.shape)\n",
    "        P2 = self.fastformer_3(U3, U1, U1, attention_mask)\n",
    "        P3 = self.fastformer_5(U1, U5, U5, attention_mask)\n",
    "        P  = P1 + P2 + P3  # [B, S, D]\n",
    "        #print(\"P shape after sum:\", P.shape)\n",
    "        \n",
    "\n",
    "        P_norm = self.layernorm_out(P)\n",
    "        I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
    "        I = self.dropout(I)\n",
    "        J = I + x  # residual with original x\n",
    "        J_norm = self.layernorm_out(J)\n",
    "        y = self.output_fc(J_norm)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,\n",
    "                 num_attention_heads=16,\n",
    "                 intermediate_size=3072,\n",
    "                 num_labels=2,\n",
    "                 num_hidden_layers=12,\n",
    "                 hidden_dropout_prob=0.1,       # Use this for general dropout\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 layer_norm_eps=1e-12,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 pooler_type='weightpooler',\n",
    "                 num_attention_layers=12):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.initializer_range = initializer_range\n",
    "        self.hidden_act = hidden_act\n",
    "        self.pooler_type = pooler_type\n",
    "        self.num_attention_layers = num_attention_layers\n",
    "\n",
    "# Gated Fusion\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_prob=0.1):\n",
    "        super(GatedFusion, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc2 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc3 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, Et, Ea):\n",
    "        assert Et.shape == Ea.shape, \"Et and Ea must have the same shape.\"\n",
    "        concat = torch.cat((Et, Ea), dim=-1)  # [B, T, 2*hidden_dim]\n",
    "        out = self.fc1(concat)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "        g = self.sigmoid(out)\n",
    "        Ef = g * Ea + (1 - g) * Et\n",
    "        return Ef\n",
    "\n",
    "#Recurrent Pyramid Network\n",
    "class Conv1x1(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 1D conv block (kernel_size=1) for channel adjustment/fusion in 1D feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class FuseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A lightweight 'fusion' that combines two feature maps (same resolution) by:\n",
    "      1) elementwise add\n",
    "      2) optional 1x1 conv for channel mixing\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = Conv1x1(channels, channels)\n",
    "    \n",
    "    def forward(self, f_top, f_bottom):\n",
    "        # f_top, f_bottom shapes: [B, C, length]\n",
    "        fused = f_top + f_bottom\n",
    "        out = self.conv(fused)\n",
    "        return out\n",
    "\n",
    "class RecurrentPyramidModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A 'Recurrent Pyramid Model' (RPM) as per Fig. 6 in the paper:\n",
    "     - Input: three feature maps at different 1D scales: f1 (x1), f2 (x1/2), f3 (x1/4)\n",
    "     - Step 1 (Top-Down): upsample from f3->f2->f1, fusing at each step\n",
    "     - Step 2 (Bottom-Up): downsample from td1->td2->td3, fusing at each step\n",
    "     - Final: concatenate the outputs of these two paths along channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # For top-down and bottom-up, we'll define small modules:\n",
    "        self.fuse12 = FuseModule(in_channels)  # fuse f1<->f2 or td1<->td2\n",
    "        self.fuse23 = FuseModule(in_channels)  # fuse f2<->f3 or td2<->td3\n",
    "        \n",
    "        # If we want them separate for top-down vs bottom-up, we can do that:\n",
    "        self.fuse_td2 = FuseModule(in_channels)\n",
    "        self.fuse_td1 = FuseModule(in_channels)\n",
    "        self.fuse_bu2 = FuseModule(in_channels)\n",
    "        self.fuse_bu3 = FuseModule(in_channels)\n",
    "        \n",
    "        # 1D conv (stride=2) for downsampling,\n",
    "        # or we could use average pool for a simpler approach.\n",
    "        self.downsample = nn.Conv1d(\n",
    "            in_channels, in_channels,\n",
    "            kernel_size=3, stride=2, padding=1, bias=True\n",
    "        )\n",
    "        \n",
    "        self.upsample = lambda x, scale=2: F.interpolate(\n",
    "            x, scale_factor=scale, mode='nearest'\n",
    "        )\n",
    "        \n",
    "    def forward(self, f1, f2, f3):\n",
    "        \"\"\"\n",
    "        f1 -> shape [B, C, T1]\n",
    "        f2 -> shape [B, C, T2]\n",
    "        f3 -> shape [B, C, T3]\n",
    "        \n",
    "        Where T2 ~ T1/2, T3 ~ T1/4.\n",
    "        \"\"\"\n",
    "\n",
    "        td3 = f3\n",
    "        # Upsample f3 -> same size as f2\n",
    "        up3 = F.interpolate(td3, size=f2.shape[-1], mode='nearest')\n",
    "        td2 = self.fuse_td2(up3, f2)\n",
    "        \n",
    "        # Upsample td2 -> same size as f1\n",
    "        up2 = F.interpolate(td2, size=f1.shape[-1], mode='nearest')\n",
    "        td1 = self.fuse_td1(up2, f1)\n",
    "        bu1 = td1\n",
    "        \n",
    "        # Downsample td1 -> size of td2\n",
    "        down1 = F.interpolate(bu1, size=td2.shape[-1], mode='nearest')  # T1->T2\n",
    "        bu2 = self.fuse_bu2(td2, down1)\n",
    "        \n",
    "        # Downsample bu2 -> size of td3\n",
    "        down2 = F.interpolate(bu2, size=td3.shape[-1], mode='nearest')\n",
    "        bu3 = self.fuse_bu3(td3, down2)\n",
    "        \n",
    "        td3_up = F.interpolate(td3, size=f1.shape[-1], mode='nearest')\n",
    "        td2_up = F.interpolate(td2, size=f1.shape[-1], mode='nearest')\n",
    "        bu2_up = F.interpolate(bu2, size=f1.shape[-1], mode='nearest')\n",
    "        bu3_up = F.interpolate(bu3, size=f1.shape[-1], mode='nearest')\n",
    "\n",
    "        final = torch.cat([td1, td2_up, td3_up, bu1, bu2_up, bu3_up], dim=1)\n",
    "\n",
    "        final = torch.cat([td1, td2_up, td3_up, bu1, bu2_up, bu3_up], dim=1)\n",
    "        # final shape: [B, 6*C, T1]\n",
    "        \n",
    "        return final, (td1, td2, td3, bu1, bu2, bu3)\n",
    "\n",
    "# Adaptive Fusion Module\n",
    "class FeatureAlignment(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Align each of the 6 multi-resolution features in time dimension \n",
    "       by applying a 1D conv and (if needed) interpolate to the max length.\n",
    "    2) Return a single 4D tensor [B, 6, T, D], \n",
    "       matching the 'Feature Dimension Alignment' block in Figure 7.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(FeatureAlignment, self).__init__()\n",
    "        self.align_convs = nn.ModuleList([\n",
    "            nn.Conv1d(feature_dim, feature_dim, kernel_size=1)\n",
    "            for _ in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: list/tuple of 6 tensors, each shape [B, T_x, D], \n",
    "                  possibly different T_x lengths\n",
    "        Returns:  H of shape [B, 6, T_max, D]\n",
    "        \"\"\"\n",
    "        max_len = max(f.shape[1] for f in features)\n",
    "\n",
    "        aligned_list = []\n",
    "        for i, f in enumerate(features):\n",
    "            f_1d = f.transpose(1, 2)  # => [B, D, T_i]\n",
    "            # apply the 1x1 conv\n",
    "            f_aligned = self.align_convs[i](f_1d)  # still [B, D, T_i]\n",
    "\n",
    "            # if needed, interpolate to max_len\n",
    "            if f_aligned.shape[2] != max_len:\n",
    "                # use linear interpolation along the temporal axis\n",
    "                f_aligned = F.interpolate(\n",
    "                    f_aligned, \n",
    "                    size=max_len, \n",
    "                    mode='linear', \n",
    "                    align_corners=False\n",
    "                )\n",
    "            # now shape is [B, D, max_len]\n",
    "            # next we want to store it as [B, max_len, D]\n",
    "            f_aligned = f_aligned.transpose(1, 2)  # => [B, max_len, D]\n",
    "            aligned_list.append(f_aligned)\n",
    "\n",
    "        # Step 2: stack along a new dimension => [B, 6, max_len, D]\n",
    "        H = torch.stack(aligned_list, dim=1)\n",
    "        return H\n",
    "    \n",
    "class MultiFeatureFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Compute T-dim mean and D-dim mean over the stacked features => \n",
    "       [B, 6] + [B, 6] => [B, 12].\n",
    "    2) Generate an attention vector for the 6 channels via FC->ReLU->Softmax.\n",
    "    3) Apply the attention to H, sum across dimension=1 (the '6' dimension).\n",
    "    4) Add a residual connection from H (averaged or direct).\n",
    "    5) Output final fused feature => shape [B, T, D].\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(MultiFeatureFusion, self).__init__()\n",
    "        # We produce a single attention weight per each of the 6 features\n",
    "        # from a 12-dimensional input (concatenated means).\n",
    "        self.attention_fc = nn.Linear(12, 6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # normalize across the 6 dimension\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: [B, 6, T, D], from FeatureAlignment\n",
    "        Return fused: [B, T, D]\n",
    "        \"\"\"\n",
    "        B, N, T, D = H.shape  # N=6\n",
    "\n",
    "        # 1) Means along T and D:\n",
    "        # mean_T => [B, N, D], i.e. average over T dimension\n",
    "        mean_T = H.mean(dim=2)  \n",
    "        # mean_D => [B, N, T], i.e. average over D dimension\n",
    "        mean_D = H.mean(dim=3)\n",
    "\n",
    "        # 2) Then average again across the leftover dimension to get [B, N]\n",
    "        #    We want a single scalar per each of the N=6 slices, from T or D perspective\n",
    "        mean_TD = mean_T.mean(dim=2)  # => [B, N]\n",
    "        mean_DT = mean_D.mean(dim=2)  # => [B, N]\n",
    "\n",
    "        # 3) Concatenate them => [B, 2N] = [B, 12]\n",
    "        mean_features = torch.cat([mean_TD, mean_DT], dim=1)  # [B, 12]\n",
    "\n",
    "        # 4) FC -> ReLU -> Softmax => attention vector of shape [B, 6]\n",
    "        att_scores = self.attention_fc(mean_features)  # [B, 6]\n",
    "        att_scores = self.relu(att_scores)\n",
    "        att_weights = self.softmax(att_scores)  # [B, 6]\n",
    "        # expand to broadcast over T,D\n",
    "        att_weights = att_weights.view(B, N, 1, 1)  # [B, 6, 1, 1]\n",
    "\n",
    "        # 5) Weighted sum across N=6 dimension => [B, T, D]\n",
    "        weighted = H * att_weights  # [B, 6, T, D]\n",
    "        fused = weighted.sum(dim=1)  # [B, T, D]\n",
    "\n",
    "        # 6) Residual connection\n",
    "        #    We can simply average H across the 6 dimension to get a 'baseline'\n",
    "        #    shape => [B, T, D]\n",
    "        residual = H.mean(dim=1)  # [B, T, D]\n",
    "        fused = fused + residual\n",
    "\n",
    "        return fused\n",
    "\n",
    "class AdaptiveFusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Overall AFM from Figure 7:\n",
    "      - Feature dimension alignment via 1D conv + optional interpolation\n",
    "      - Concat into shape [B, 6, T, D]\n",
    "      - Multi-feature fusion with channel-wise attention + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AdaptiveFusionModule, self).__init__()\n",
    "        self.feature_alignment = FeatureAlignment(feature_dim)\n",
    "        self.feature_fusion = MultiFeatureFusion(feature_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: a list/tuple of 6 multi-resolution features, \n",
    "                  each shaped [B, T_i, D]\n",
    "        Returns:\n",
    "          fused_output: shape [B, T_max, D]\n",
    "        \"\"\"\n",
    "        # Step 1: Align to [B, 6, T_max, D]\n",
    "        H = self.feature_alignment(features)\n",
    "\n",
    "        # Step 2: Fuse => [B, T_max, D]\n",
    "        fused_output = self.feature_fusion(H)\n",
    "        return fused_output\n",
    "\n",
    "class MFFNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The overall MFFNet architecture:\n",
    "      1) Two MSFastformer encoders (one for text, one for speech)\n",
    "      2) Gated Fusion of text + speech -> fused rep\n",
    "      3) Recurrent Pyramid Model on that fused rep, generating multi-res outputs\n",
    "      4) Adaptive Fusion Module (AFM) on those 6 multi-res outputs\n",
    "      5) Final FC to produce binary depression classification\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.text_encoder = MSFastformer(config)\n",
    "        self.speech_encoder = MSFastformer(config)\n",
    "        self.gated_fusion = GatedFusion(hidden_dim=config.hidden_size,\n",
    "                                        dropout_prob=config.hidden_dropout_prob)\n",
    "\n",
    "        self.rpm = RecurrentPyramidModel(in_channels=config.hidden_size)\n",
    "        self.afm = AdaptiveFusionModule(feature_dim=config.hidden_size)\n",
    "\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, text_in, speech_in, attention_mask_text=None, attention_mask_speech=None):\n",
    "        \"\"\"\n",
    "        text_in, speech_in: [B, T, D] embeddings for text and speech\n",
    "        attention_mask_*: optional [B, T] for each\n",
    "        Returns: logits => [B, num_classes]\n",
    "        \"\"\"\n",
    "        text_rep = self.text_encoder(text_in, attention_mask_text)      # [B, T, D]\n",
    "        speech_rep = self.speech_encoder(speech_in, attention_mask_speech)  # [B, T, D]\n",
    "\n",
    "        fused_rep = self.gated_fusion(text_rep, speech_rep)\n",
    "\n",
    "        fused_rep_t = fused_rep.transpose(1, 2)  # => [B, D, T], the 'C' dimension is D\n",
    "\n",
    "        f1 = fused_rep_t  # full scale => shape [B, D, T]\n",
    "        f2 = F.avg_pool1d(f1, kernel_size=2, stride=2)  # => [B, D, T//2]\n",
    "        f3 = F.avg_pool1d(f2, kernel_size=2, stride=2)  # => [B, D, T//4]\n",
    "\n",
    "        rpm_out, (td1, td2, td3, bu1, bu2, bu3) = self.rpm(f1, f2, f3)\n",
    "\n",
    "        td1_ = td1.transpose(1, 2)  # => [B, T1, D]\n",
    "        td2_ = td2.transpose(1, 2)  # => [B, T2, D]\n",
    "        td3_ = td3.transpose(1, 2)  # => [B, T3, D]\n",
    "        bu1_ = bu1.transpose(1, 2)\n",
    "        bu2_ = bu2.transpose(1, 2)\n",
    "        bu3_ = bu3.transpose(1, 2)\n",
    "\n",
    "        fused_afm = self.afm([td1_, td2_, td3_, bu1_, bu2_, bu3_])  # => [B, T_max, D]\n",
    "\n",
    "        pooled = fused_afm.mean(dim=1)  # [B, D]\n",
    "\n",
    "        logits = self.classifier(pooled)  # => [B, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    hidden_size=128,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=256,\n",
    "    hidden_dropout_prob=0.1\n",
    ")\n",
    "MS = MSFastformer(config)\n",
    "\n",
    "B = 2            # batch size\n",
    "S = 10           # sequence length\n",
    "D = config.hidden_size\n",
    "text = torch.randn(B, S, D)  # [B, S, D]\n",
    "speech = torch.rand(B, S, D)\n",
    "mask_base = torch.ones(B, S)\n",
    "mask_base[:, 8:] = 0  # Suppose last 2 tokens are pads\n",
    "attention_mask = (1.0 - mask_base) * -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 128])\n",
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "out_text = MS.forward(text, mask_base)\n",
    "print(out_text.shape)\n",
    "out_speech = MS.forward(speech, mask_base)\n",
    "print(out_speech.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "gfm = GatedFusion(hidden_dim=D,dropout_prob=0.1)\n",
    "out_fused = gfm.forward(out_text,out_speech)\n",
    "print(out_fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_base = out_fused.transpose(1, 2)  # now [B, D, T]\n",
    "f1 = f_base\n",
    "\n",
    "# f2 = half resolution\n",
    "f2 = F.avg_pool1d(f1, kernel_size=2, stride=2)  # => ~T/2\n",
    "\n",
    "# f3 = quarter resolution\n",
    "f3 = F.avg_pool1d(f2, kernel_size=2, stride=2)  # => ~T/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm = RecurrentPyramidModel(in_channels=D)\n",
    "final, (td1, td2, td3, bu1, bu2, bu3) = rpm.forward(f1, f2, f3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_output.shape: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "afm = AdaptiveFusionModule(D)\n",
    "features_for_afm = [\n",
    "    td1.transpose(1,2),  # [B, T_i, C]\n",
    "    td2.transpose(1,2),\n",
    "    td3.transpose(1,2),\n",
    "    bu1.transpose(1,2),\n",
    "    bu2.transpose(1,2),\n",
    "    bu3.transpose(1,2)\n",
    "]\n",
    "\n",
    "# Now feed them into the AFM:\n",
    "fused_output = afm(features_for_afm)\n",
    "print(\"fused_output.shape:\", fused_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0614, -0.0619],\n",
      "        [-0.0921, -0.0371]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "MF = MFFNet(config)\n",
    "logit = MF.forward(text,speech)\n",
    "print(logit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
