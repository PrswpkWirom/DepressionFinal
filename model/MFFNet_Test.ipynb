{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Implementation of Fastformer (modified from  wuch15/Fastformer repository)\n",
    "class FastSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"hidden_size must be divisible by num_attention_heads\"\n",
    "            )\n",
    "        \n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = config.hidden_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key   = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # W_q^T q_i in eq. (3), W_q^T m_i in eq. (6)\n",
    "        self.query_att = nn.Linear(self.head_dim, 1, bias=False)\n",
    "        self.m_att     = nn.Linear(self.head_dim, 1, bias=False)\n",
    "\n",
    "        # last linear transform after element-wise product of k and V.\n",
    "        self.out_proj = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "\n",
    "        # Combine all heads back to hidden_size\n",
    "        self.merge_heads = nn.Linear(self.all_head_size, self.all_head_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Simple initialization scheme.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Split [B, seq_len, hidden_size] -> [B, num_heads, seq_len, head_dim].\n",
    "        \"\"\"\n",
    "        B, S, D = x.size()\n",
    "        x = x.view(B, S, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)  # [B, H, S, d]\n",
    "\n",
    "    def merge_heads_fn(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Merge [B, num_heads, seq_len, head_dim] -> [B, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "        B, H, S, d = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(B, S, H * d)\n",
    "\n",
    "    def forward(self, query_states, key_states, value_states, attention_mask=None):\n",
    "        B, Sq, _ = query_states.shape\n",
    "        Sk = key_states.size(1)\n",
    "        Q = self.split_heads(self.query(query_states))   # [B, H, Sq, d]\n",
    "        K = self.split_heads(self.key(key_states))       # [B, H, Sk, d]\n",
    "        V = self.split_heads(self.value(value_states))   # [B, H, Sk, d]\n",
    "\n",
    "        # O_i = softmax( W_q^T q_i / sqrt(d) ),  q = sum_i O_i * q_i\n",
    "        aggregator_logits_q = self.query_att(Q) / math.sqrt(self.head_dim)\n",
    "        aggregator_logits_q = aggregator_logits_q.squeeze(-1)  # [B, H, Sq]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            aggregator_logits_q = aggregator_logits_q + attention_mask.view(B, 1, -1)\n",
    "\n",
    "        att_weights_q = self.softmax(aggregator_logits_q)   # [B, H, Sq]\n",
    "        print(\"Q shape:\", Q.shape)\n",
    "        print(\"att_weights_q shape:\", att_weights_q.shape)\n",
    "\n",
    "        q_global = torch.einsum('bhsd,bhs->bhd', Q, att_weights_q)  # [B, H, d]\n",
    "\n",
    "        # M = q_global * K (element-wise for each position i)\n",
    "        # M_i = q_global \\odot K_i\n",
    "        qg = q_global.unsqueeze(2)             # [B, H, 1, d]\n",
    "        M  = qg * K                            # [B, H, Sk, d]\n",
    "\n",
    "        # a_i = softmax( W_q^T m_i / sqrt(d) ) over i=1..Sk\n",
    "        aggregator_logits_m = self.m_att(M) / math.sqrt(self.head_dim)  # [B, H, Sk, 1]\n",
    "        aggregator_logits_m = aggregator_logits_m.squeeze(-1)           # [B, H, Sk]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            pass\n",
    "\n",
    "        att_weights_m = self.softmax(aggregator_logits_m)  # [B, H, Sk]\n",
    "\n",
    "        # 5) k_global = sum_i a_i * m_i  => [B, H, d]\n",
    "        k_global = torch.einsum('bhsd,bhs->bhd', M, att_weights_m)\n",
    "        print(f\"k_global: {k_global.shape}\")\n",
    "        # 6) For each position i in V, do elementwise multiply with k_global\n",
    "        #    => e_i = out_proj( k_global \\odot v_i )\n",
    "        # so E has shape [B, H, Sk, d]. Then we'll combine E with the \"original Q\" somehow\n",
    "        kg = k_global.unsqueeze(2)                  # [B, H, 1, d]\n",
    "        KV_interaction = kg * V                     # [B, H, Sk, d]\n",
    "        E = self.out_proj(KV_interaction)           # [B, H, Sk, d]\n",
    "\n",
    "        if Sq != Sk:\n",
    "            raise ValueError(\"Fastformer aggregator: mismatch in seq_len (Sq vs Sk).\")\n",
    "\n",
    "        out_heads = E + Q  # shape [B, H, Sq, d]\n",
    "        out = self.merge_heads(out_heads)  # [B, Sq, all_head_size]\n",
    "        print(\"out after aggregator:\", out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "class FastAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps FastSelfAttention in a minimal 'attention + output-projection' block,\n",
    "    optionally adding a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = FastSelfAttention(config)\n",
    "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query_tensor, key_tensor, value_tensor, attention_mask=None):\n",
    "        self_output = self.self(query_tensor, key_tensor, value_tensor, attention_mask)\n",
    "        # typical residual:\n",
    "        attention_output = self.output(self_output) + query_tensor\n",
    "        attention_output = self.layernorm(attention_output)\n",
    "        return attention_output\n",
    "\n",
    "class FastformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = FastAttention(config)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, key_states, value_states, attention_mask=None):\n",
    "        # 1) Fastformer-based attention\n",
    "        attention_output = self.attention(\n",
    "            hidden_states, key_states, value_states, attention_mask\n",
    "        )\n",
    "        # 2) Feed-forward + residual\n",
    "        inter = self.intermediate(attention_output)\n",
    "        inter = self.activation(inter)\n",
    "        ff_out = self.output(inter)\n",
    "        ff_out = ff_out + attention_output\n",
    "        layer_output = self.layernorm(ff_out)\n",
    "        return layer_output\n",
    "\n",
    "# MSFastformer\n",
    "class MSFastformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Scale Fastformer block:\n",
    "      - LayerNorm + three different 1D Convs (kernel_size=1,3,5)\n",
    "      - Each conv output goes into a FastformerLayer call (paired as in eq. (2))\n",
    "      - Sum the resulting feature maps\n",
    "      - Then FC->GELU->FC + residual, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Single definitions of LayerNorm\n",
    "        self.layernorm_in = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_out = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=5, padding=2)\n",
    "\n",
    "        # Fastformer layers\n",
    "        self.fastformer_1 = FastformerLayer(config)\n",
    "        self.fastformer_3 = FastformerLayer(config)\n",
    "        self.fastformer_5 = FastformerLayer(config)\n",
    "\n",
    "        # Feed-forward\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.output_fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: [B, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        # 1) LN, then conv\n",
    "        x_norm = self.layernorm_in(x)                 # [B, S, D]\n",
    "        x_t = x_norm.transpose(1, 2)                  # [B, D, S]\n",
    "\n",
    "        U1 = self.conv1(x_t).transpose(1, 2)          # [B, S, D]\n",
    "        U3 = self.conv3(x_t).transpose(1, 2)          # [B, S, D]\n",
    "        U5 = self.conv5(x_t).transpose(1, 2)          # [B, S, D]\n",
    "\n",
    "        P1 = self.fastformer_1(U5, U3, U3, attention_mask)\n",
    "        print(\"P1 shape:\", P1.shape)\n",
    "        P2 = self.fastformer_3(U3, U1, U1, attention_mask)\n",
    "        P3 = self.fastformer_5(U1, U5, U5, attention_mask)\n",
    "        P  = P1 + P2 + P3  # [B, S, D]\n",
    "        print(\"P shape after sum:\", P.shape)\n",
    "\n",
    "        P_norm = self.layernorm_out(P)\n",
    "        I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
    "        I = self.dropout(I)\n",
    "        J = I + x  # residual with original x\n",
    "        J_norm = self.layernorm_out(J)\n",
    "        y = self.output_fc(J_norm)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,\n",
    "                 num_attention_heads=16,\n",
    "                 intermediate_size=3072,\n",
    "                 num_labels=2,\n",
    "                 num_hidden_layers=12,\n",
    "                 hidden_dropout_prob=0.1,       # Use this for general dropout\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 layer_norm_eps=1e-12,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 pooler_type='weightpooler',\n",
    "                 num_attention_layers=12):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.initializer_range = initializer_range\n",
    "        self.hidden_act = hidden_act\n",
    "        self.pooler_type = pooler_type\n",
    "        self.num_attention_layers = num_attention_layers\n",
    "\n",
    "# Gated Fusion\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_prob=0.1):\n",
    "        super(GatedFusion, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc2 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc3 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, Et, Ea):\n",
    "        assert Et.shape == Ea.shape, \"Et and Ea must have the same shape.\"\n",
    "        concat = torch.cat((Et, Ea), dim=-1)  # [B, T, 2*hidden_dim]\n",
    "        out = self.fc1(concat)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "        g = self.sigmoid(out)\n",
    "        Ef = g * Ea + (1 - g) * Et\n",
    "        return Ef\n",
    "\n",
    "#Recurrent Pyramid Network\n",
    "class Conv1x1(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 1D conv block (kernel_size=1) for channel adjustment/fusion in 1D feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class FuseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A lightweight 'fusion' that combines two feature maps (same resolution) by:\n",
    "      1) elementwise add\n",
    "      2) optional 1x1 conv for channel mixing\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = Conv1x1(channels, channels)\n",
    "    \n",
    "    def forward(self, f_top, f_bottom):\n",
    "        # f_top, f_bottom shapes: [B, C, length]\n",
    "        fused = f_top + f_bottom\n",
    "        out = self.conv(fused)\n",
    "        return out\n",
    "\n",
    "class RecurrentPyramidModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A 'Recurrent Pyramid Model' (RPM) as per Fig. 6 in the paper:\n",
    "     - Input: three feature maps at different 1D scales: f1 (x1), f2 (x1/2), f3 (x1/4)\n",
    "     - Step 1 (Top-Down): upsample from f3->f2->f1, fusing at each step\n",
    "     - Step 2 (Bottom-Up): downsample from td1->td2->td3, fusing at each step\n",
    "     - Final: concatenate the outputs of these two paths along channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # in_channels is the channel dimension for f1, f2, f3\n",
    "        # We assume all 3 features have the same # of channels \n",
    "        # (the difference is in their time resolution).\n",
    "        \n",
    "        # For top-down and bottom-up, we'll define small modules:\n",
    "        self.fuse12 = FuseModule(in_channels)  # fuse f1<->f2 or td1<->td2\n",
    "        self.fuse23 = FuseModule(in_channels)  # fuse f2<->f3 or td2<->td3\n",
    "        \n",
    "        # If we want them separate for top-down vs bottom-up, we can do that:\n",
    "        self.fuse_td2 = FuseModule(in_channels)\n",
    "        self.fuse_td1 = FuseModule(in_channels)\n",
    "        self.fuse_bu2 = FuseModule(in_channels)\n",
    "        self.fuse_bu3 = FuseModule(in_channels)\n",
    "        \n",
    "        # 1D conv (stride=2) for downsampling,\n",
    "        # or we could use average pool for a simpler approach.\n",
    "        self.downsample = nn.Conv1d(\n",
    "            in_channels, in_channels,\n",
    "            kernel_size=3, stride=2, padding=1, bias=True\n",
    "        )\n",
    "        \n",
    "        # For upsampling, we can use interpolation\n",
    "        # (or a conv-transpose if you prefer).\n",
    "        # We do nearest neighbor here.\n",
    "        self.upsample = lambda x, scale=2: F.interpolate(\n",
    "            x, scale_factor=scale, mode='nearest'\n",
    "        )\n",
    "        \n",
    "    def forward(self, f1, f2, f3):\n",
    "        \"\"\"\n",
    "        f1 -> shape [B, C, T1]\n",
    "        f2 -> shape [B, C, T2]\n",
    "        f3 -> shape [B, C, T3]\n",
    "        \n",
    "        Where T2 ~ T1/2, T3 ~ T1/4.\n",
    "        \"\"\"\n",
    "        # ============= STEP 1: Top-Down Pass =============\n",
    "        # We'll keep the original names:\n",
    "        #   td3 = f3\n",
    "        #   td2 = fuse( f2, upsample(f3) )\n",
    "        #   td1 = fuse( f1, upsample(td2) )\n",
    "        \n",
    "        td3 = f3\n",
    "        # Upsample f3 -> same size as f2\n",
    "        up3 = self.upsample(td3, scale=2)  # T3->T2\n",
    "        td2 = self.fuse_td2(up3, f2)\n",
    "        \n",
    "        # Upsample td2 -> same size as f1\n",
    "        up2 = self.upsample(td2, scale=2)  # T2->T1\n",
    "        td1 = self.fuse_td1(up2, f1)\n",
    "        \n",
    "        # ============= STEP 2: Bottom-Up Pass =============\n",
    "        # The paper says: \"the bottom fused features are combined with the top features\n",
    "        # through downsampling, and again top-down fusion is performed...\"\n",
    "        #\n",
    "        # A straightforward way: \n",
    "        #   bu1 = td1\n",
    "        #   bu2 = fuse( td2, downsample(td1) )\n",
    "        #   bu3 = fuse( td3, downsample(bu2) )\n",
    "        \n",
    "        bu1 = td1\n",
    "        \n",
    "        # Downsample td1 -> size of td2\n",
    "        down1 = self.downsample(bu1)  # T1->T2\n",
    "        bu2 = self.fuse_bu2(td2, down1)\n",
    "        \n",
    "        # Downsample bu2 -> size of td3\n",
    "        down2 = self.downsample(bu2)  # T2->T3\n",
    "        bu3 = self.fuse_bu3(td3, down2)\n",
    "        \n",
    "        # ============= STEP 3: Concatenate results =============\n",
    "        #\n",
    "        # \"Finally, the outputs of these two fusion paths are concatenated to obtain\n",
    "        #  the final feature representation of the RPM.\"\n",
    "        #\n",
    "        # We have two \"paths\":\n",
    "        #   - top-down path: td1, td2, td3\n",
    "        #   - bottom-up path: bu1, bu2, bu3\n",
    "        #\n",
    "        # We'll just concat them channel-wise. \n",
    "        # Often you'll then feed them into another 1x1 conv or your next network block.\n",
    "        \n",
    "        # Each of these has a different temporal length (td1 is T1, td2 is T2, td3 is T3),\n",
    "        # so typically you'd either keep them separate or upsample/downsample to unify dimension\n",
    "        # before final concatenation.\n",
    "        #\n",
    "        # If you want one big tensor, you must unify or flatten them. \n",
    "        # For the sake of demonstration, let's unify them to T1 by upsampling the smaller ones:\n",
    "        \n",
    "        td2_up = self.upsample(td2, scale=2)  # T2->T1\n",
    "        td3_up = self.upsample(td3, scale=4)  # T3->T1\n",
    "        bu2_up = self.upsample(bu2, scale=2)\n",
    "        bu3_up = self.upsample(bu3, scale=4)\n",
    "        \n",
    "        # Now all are [B, C, T1]. We can safely concat along channel dimension:\n",
    "        final = torch.cat([td1, td2_up, td3_up, bu1, bu2_up, bu3_up], dim=1)\n",
    "        # final shape: [B, 6*C, T1]\n",
    "        \n",
    "        return final, (td1, td2, td3, bu1, bu2, bu3)\n",
    "    \n",
    "class FeatureAlignment(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Align each of the 6 multi-resolution features in time dimension \n",
    "       by applying a 1D conv and (if needed) interpolate to the max length.\n",
    "    2) Return a single 4D tensor [B, 6, T, D], \n",
    "       matching the 'Feature Dimension Alignment' block in Figure 7.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(FeatureAlignment, self).__init__()\n",
    "        # We'll have 6 separate 1x1 convs (one per feature)\n",
    "        self.align_convs = nn.ModuleList([\n",
    "            nn.Conv1d(feature_dim, feature_dim, kernel_size=1)\n",
    "            for _ in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: list/tuple of 6 tensors, each shape [B, T_x, D], \n",
    "                  possibly different T_x lengths\n",
    "        Returns:  H of shape [B, 6, T_max, D]\n",
    "        \"\"\"\n",
    "        # Step 1: find the longest T among the 6 features\n",
    "        max_len = max(f.shape[1] for f in features)\n",
    "\n",
    "        aligned_list = []\n",
    "        for i, f in enumerate(features):\n",
    "            # f is [B, T_i, D], we want [B, D, T_i] for Conv1d\n",
    "            f_1d = f.transpose(1, 2)  # => [B, D, T_i]\n",
    "            # apply the 1x1 conv\n",
    "            f_aligned = self.align_convs[i](f_1d)  # still [B, D, T_i]\n",
    "\n",
    "            # if needed, interpolate to max_len\n",
    "            if f_aligned.shape[2] != max_len:\n",
    "                # use linear interpolation along the temporal axis\n",
    "                f_aligned = F.interpolate(\n",
    "                    f_aligned, \n",
    "                    size=max_len, \n",
    "                    mode='linear', \n",
    "                    align_corners=False\n",
    "                )\n",
    "            # now shape is [B, D, max_len]\n",
    "            # next we want to store it as [B, max_len, D]\n",
    "            f_aligned = f_aligned.transpose(1, 2)  # => [B, max_len, D]\n",
    "            aligned_list.append(f_aligned)\n",
    "\n",
    "        # Step 2: stack along a new dimension => [B, 6, max_len, D]\n",
    "        H = torch.stack(aligned_list, dim=1)\n",
    "        return H\n",
    "\n",
    "# Adaptive Fusion Module\n",
    "class MultiFeatureFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Compute T-dim mean and D-dim mean over the stacked features => \n",
    "       [B, 6] + [B, 6] => [B, 12].\n",
    "    2) Generate an attention vector for the 6 channels via FC->ReLU->Softmax.\n",
    "    3) Apply the attention to H, sum across dimension=1 (the '6' dimension).\n",
    "    4) Add a residual connection from H (averaged or direct).\n",
    "    5) Output final fused feature => shape [B, T, D].\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(MultiFeatureFusion, self).__init__()\n",
    "        # We produce a single attention weight per each of the 6 features\n",
    "        # from a 12-dimensional input (concatenated means).\n",
    "        self.attention_fc = nn.Linear(12, 6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # normalize across the 6 dimension\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: [B, 6, T, D], from FeatureAlignment\n",
    "        Return fused: [B, T, D]\n",
    "        \"\"\"\n",
    "        B, N, T, D = H.shape  # N=6\n",
    "\n",
    "        # 1) Means along T and D:\n",
    "        # mean_T => [B, N, D], i.e. average over T dimension\n",
    "        mean_T = H.mean(dim=2)  \n",
    "        # mean_D => [B, N, T], i.e. average over D dimension\n",
    "        mean_D = H.mean(dim=3)\n",
    "\n",
    "        # 2) Then average again across the leftover dimension to get [B, N]\n",
    "        #    We want a single scalar per each of the N=6 slices, from T or D perspective\n",
    "        mean_TD = mean_T.mean(dim=2)  # => [B, N]\n",
    "        mean_DT = mean_D.mean(dim=2)  # => [B, N]\n",
    "\n",
    "        # 3) Concatenate them => [B, 2N] = [B, 12]\n",
    "        mean_features = torch.cat([mean_TD, mean_DT], dim=1)  # [B, 12]\n",
    "\n",
    "        # 4) FC -> ReLU -> Softmax => attention vector of shape [B, 6]\n",
    "        att_scores = self.attention_fc(mean_features)  # [B, 6]\n",
    "        att_scores = self.relu(att_scores)\n",
    "        att_weights = self.softmax(att_scores)  # [B, 6]\n",
    "        # expand to broadcast over T,D\n",
    "        att_weights = att_weights.view(B, N, 1, 1)  # [B, 6, 1, 1]\n",
    "\n",
    "        # 5) Weighted sum across N=6 dimension => [B, T, D]\n",
    "        weighted = H * att_weights  # [B, 6, T, D]\n",
    "        fused = weighted.sum(dim=1)  # [B, T, D]\n",
    "\n",
    "        # 6) Residual connection\n",
    "        #    We can simply average H across the 6 dimension to get a 'baseline'\n",
    "        #    shape => [B, T, D]\n",
    "        residual = H.mean(dim=1)  # [B, T, D]\n",
    "        fused = fused + residual\n",
    "\n",
    "        return fused\n",
    "\n",
    "class AdaptiveFusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Overall AFM from Figure 7:\n",
    "      - Feature dimension alignment via 1D conv + optional interpolation\n",
    "      - Concat into shape [B, 6, T, D]\n",
    "      - Multi-feature fusion with channel-wise attention + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AdaptiveFusionModule, self).__init__()\n",
    "        self.feature_alignment = FeatureAlignment(feature_dim)\n",
    "        self.feature_fusion = MultiFeatureFusion(feature_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: a list/tuple of 6 multi-resolution features, \n",
    "                  each shaped [B, T_i, D]\n",
    "        Returns:\n",
    "          fused_output: shape [B, T_max, D]\n",
    "        \"\"\"\n",
    "        # Step 1: Align to [B, 6, T_max, D]\n",
    "        H = self.feature_alignment(features)\n",
    "\n",
    "        # Step 2: Fuse => [B, T_max, D]\n",
    "        fused_output = self.feature_fusion(H)\n",
    "        return fused_output\n",
    "\n",
    "class MFFNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The full Multi Fine-Grained Fusion Network:\n",
    "      1) MSFastformer(Speech)  -> speech_rep\n",
    "      2) MSFastformer(Text)    -> text_rep\n",
    "      3) GatedFusion           -> fused_rep\n",
    "      4) Convert fused_rep into 3 scales (f1, f2, f3) \n",
    "      5) RecurrentPyramidModel -> 6 multi-res outputs + final stacked feature\n",
    "      6) AdaptiveFusionModule  -> fused multi-resolution feature\n",
    "      7) Final FC              -> classification\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1) & 2) MSFastformer blocks for each modality\n",
    "        self.msfast_speech = MSFastformer(config)\n",
    "        self.msfast_text   = MSFastformer(config)\n",
    "\n",
    "        # 3) Gated Fusion to combine speech & text\n",
    "        self.gated_fusion = GatedFusion(\n",
    "            hidden_dim=config.hidden_size,\n",
    "            dropout_prob=config.hidden_dropout_prob\n",
    "        )\n",
    "\n",
    "        # 4) Convs to produce 3 scales from the fused representation\n",
    "        #    Following the paper's note: \n",
    "        #     - \"1D kernel size=1\" to get first layer (f1),\n",
    "        #     - \"two 1D kernels size=3, stride=2\" to get f2, f3.\n",
    "        self.conv_scale1 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, out_channels=config.hidden_size,\n",
    "            kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.conv_scale2 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, out_channels=config.hidden_size,\n",
    "            kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "        self.conv_scale3 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, out_channels=config.hidden_size,\n",
    "            kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        # 5) Recurrent Pyramid Model for multi-resolution feature fusion\n",
    "        self.rpm = RecurrentPyramidModel(in_channels=config.hidden_size)\n",
    "\n",
    "        # 6) Adaptive Fusion Module to do final channel-attention-based merging\n",
    "        self.afm = AdaptiveFusionModule(feature_dim=config.hidden_size)\n",
    "\n",
    "        # 7) Final classification head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, speech_x, text_x):\n",
    "        \"\"\"\n",
    "        speech_x: [B, T_speech, hidden_size]\n",
    "        text_x:   [B, T_text,   hidden_size]\n",
    "        Returns: logits => [B, num_labels]\n",
    "        \"\"\"\n",
    "        # --- 1) MSFastformer for speech ---\n",
    "        speech_rep = self.msfast_speech(speech_x)  # [B, T_speech, hidden_size]\n",
    "\n",
    "        # --- 2) MSFastformer for text ---\n",
    "        text_rep = self.msfast_text(text_x)        # [B, T_text, hidden_size]\n",
    "\n",
    "        # (Optionally) if T_speech != T_text, you might need to pad or align them\n",
    "        # to match. The GatedFusion code requires them to have the same shape.\n",
    "\n",
    "        # --- 3) Gated Fusion (must have same [B, T, hidden_size] shape) ---\n",
    "        fused_rep = self.gated_fusion(text_rep, speech_rep)  \n",
    "        # => [B, T, hidden_size]\n",
    "\n",
    "        # --- 4) Produce 3 scales for RPM input ---\n",
    "        #     Convert [B, T, hidden_size] => [B, hidden_size, T] for Conv1d\n",
    "        fused_rep_1d = fused_rep.transpose(1, 2)  # => [B, hidden_size, T]\n",
    "\n",
    "        # f1 (scale x1)\n",
    "        f1 = self.conv_scale1(fused_rep_1d)  # [B, hidden_size, T]\n",
    "        # f2 (scale x1/2)\n",
    "        f2 = self.conv_scale2(f1)           # [B, hidden_size, T//2]\n",
    "        # f3 (scale x1/4)\n",
    "        f3 = self.conv_scale3(f2)           # [B, hidden_size, T//4]\n",
    "\n",
    "        # The RPM expects [B, C, T_i], so rename 'hidden_size'->C dimension\n",
    "        # => each is shape [B, C, T_i]\n",
    "        # f1, f2, f3 are already in [B, C, T_i], so we can pass them directly.\n",
    "\n",
    "        # --- 5) Recurrent Pyramid Model ---\n",
    "        # rpm_out => [B, 6*C, T1], plus the 6 intermediate features\n",
    "        rpm_out, (td1, td2, td3, bu1, bu2, bu3) = self.rpm(f1, f2, f3)\n",
    "        # Note: each tdX/buX is [B, C, T_X]\n",
    "\n",
    "        # --- 6) Adaptive Fusion Module ---\n",
    "        # The AFM wants 6 separate features each in [B, T, C] shape,\n",
    "        # so we transpose them from [B, C, T] => [B, T, C].\n",
    "        td1_t = td1.transpose(1, 2)\n",
    "        td2_t = td2.transpose(1, 2)\n",
    "        td3_t = td3.transpose(1, 2)\n",
    "        bu1_t = bu1.transpose(1, 2)\n",
    "        bu2_t = bu2.transpose(1, 2)\n",
    "        bu3_t = bu3.transpose(1, 2)\n",
    "\n",
    "        afm_input = [td1_t, td2_t, td3_t, bu1_t, bu2_t, bu3_t]\n",
    "        fused_multires = self.afm(afm_input)  # => [B, T_max, C]\n",
    "\n",
    "        # --- 7) Final classification ---\n",
    "        # We might pool over time or just take the last state. \n",
    "        # For simplicity, let's do a mean-pool over the time dimension:\n",
    "        # fused_multires => [B, T_max, C]\n",
    "        pooled = fused_multires.mean(dim=1)  # => [B, C]\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))  # => [B, num_labels]\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape: torch.Size([2, 10, 64])\n",
      "conv1.weight.shape: torch.Size([64, 64, 1])\n",
      "Q shape: torch.Size([2, 8, 10, 8])\n",
      "att_weights_q shape: torch.Size([2, 8, 10])\n",
      "k_global: torch.Size([2, 8, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (160x8 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv1.weight.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, msfast\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[43mmsfast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout1.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out1\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 213\u001b[0m, in \u001b[0;36mMSFastformer.forward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m U3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x_t)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)          \u001b[38;5;66;03m# [B, S, D]\u001b[39;00m\n\u001b[1;32m    211\u001b[0m U5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x_t)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)          \u001b[38;5;66;03m# [B, S, D]\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m P1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfastformer_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP1 shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, P1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    215\u001b[0m P2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfastformer_3(U3, U1, U1, attention_mask)\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 142\u001b[0m, in \u001b[0;36mFastformerLayer.forward\u001b[0;34m(self, hidden_states, key_states, value_states, attention_mask)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, key_states, value_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# 1) Fastformer-based attention\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# 2) Feed-forward + residual\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     inter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 125\u001b[0m, in \u001b[0;36mFastAttention.forward\u001b[0;34m(self, query_tensor, key_tensor, value_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_tensor, key_tensor, value_tensor, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 125\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# typical residual:\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_output) \u001b[38;5;241m+\u001b[39m query_tensor\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m, in \u001b[0;36mFastSelfAttention.forward\u001b[0;34m(self, query_states, key_states, value_states, attention_mask)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastformer aggregator: mismatch in seq_len (Sq vs Sk).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m out_heads \u001b[38;5;241m=\u001b[39m E \u001b[38;5;241m+\u001b[39m Q  \u001b[38;5;66;03m# shape [B, H, Sq, d]\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_heads\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, Sq, all_head_size]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout after aggregator:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (160x8 and 64x64)"
     ]
    }
   ],
   "source": [
    "config = Config(hidden_size=64, num_attention_heads=8, intermediate_size=128)\n",
    "msfast = MSFastformer(config)\n",
    "\n",
    "B, T, D = 2, 10, 64\n",
    "x1 = torch.randn(B, T, D)\n",
    "\n",
    "print(\"x1.shape:\", x1.shape)\n",
    "print(\"conv1.weight.shape:\", msfast.conv1.weight.shape)\n",
    "\n",
    "out1 = msfast(x1)\n",
    "print(\"out1.shape:\", out1.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
