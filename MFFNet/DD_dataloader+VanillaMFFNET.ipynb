{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrswpkWirom/Depression-Detection-FinalProject/blob/main/DD_dataloader%2BVanillaMFFNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC6RqiAFMIKO",
        "outputId": "88765409-2d12-487b-cec2-cb2105a7c1cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xdI9tpzsXK6",
        "outputId": "6b774bc7-f25c-4dd1-965f-1cccc5b695ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "07MtDkE5Lnhd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/popsatorn/Desktop/DD_FinalProject/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wave\n",
        "#from pydub import AudioSegment\n",
        "from io import BytesIO\n",
        "from IPython.display import Audio, display\n",
        "from transformers import RobertaTokenizer, RobertaModel, Wav2Vec2FeatureExtractor\n",
        "from transformers.models.wavlm import WavLMModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wFHykGWNETs",
        "outputId": "9fe9cfa1-7f74-4f2a-94eb-61af0126440f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "database_dir = '/content/drive/MyDrive/DAIC_WOZ'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guR6ZO6xMFrE",
        "outputId": "1528e4f0-eac2-4fbb-9d64-03c071b79da1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-large\")\n",
        "wavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2fnlyeoHx6O",
        "outputId": "61756487-9c9e-440f-9ee8-bdbdeb10d9dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "model_name = \"roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "text_model = AutoModel.from_pretrained(model_name)\n",
        "text_model = text_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuM0stOUXTJb"
      },
      "source": [
        "# Attempt 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1j272yIM4Qz"
      },
      "outputs": [],
      "source": [
        "def get_sentence_level_text_embedding(sentence, tokenizer, text_model):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs)\n",
        "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
        "    return sentence_embedding.squeeze(0)\n",
        "\n",
        "def get_audio_embedding(audio_segment, feature_extractor, wavlm_model):\n",
        "    audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.int16)\n",
        "    audio_data_tensor = torch.tensor(audio_data, dtype=torch.float32).to(device)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        audio_data_tensor,\n",
        "        sampling_rate=audio_segment.frame_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = wavlm_model(**inputs)\n",
        "\n",
        "    # last_hidden_state: (1, time_steps, embedding_dim)\n",
        "    audio_embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "    # Average pooling over time\n",
        "    audio_embedding_vector = audio_embeddings.mean(dim=0)\n",
        "    return audio_embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SwRXOPkNN-k"
      },
      "outputs": [],
      "source": [
        "def load_label_dict(label_csv_path):\n",
        "    \"\"\"\n",
        "    Loads a CSV file that contains columns: \"ID\" and \"PHQ8_Binary\".\n",
        "    Returns a dictionary mapping the string ID (e.g., \"315\") to the binary label (0 or 1).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(label_csv_path)\n",
        "    label_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        base_id = str(row[\"Participant_ID\"])  # Ensure it's a string for indexing\n",
        "        label = int(row[\"PHQ8_Binary\"])\n",
        "        label_dict[base_id] = label\n",
        "    return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWP7WFINQdB"
      },
      "outputs": [],
      "source": [
        "class DAICDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_csv_path, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir='embeddings'):\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_model = text_model\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.wavlm_model = wavlm_model\n",
        "        self.device = device\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "\n",
        "        # Load labels as before\n",
        "        self.label_dict = load_label_dict(label_csv_path)\n",
        "\n",
        "        self.session_ids = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.data = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        # Ensure the embeddings directory for this split exists\n",
        "        if not os.path.exists(self.embeddings_dir):\n",
        "            os.makedirs(self.embeddings_dir)\n",
        "\n",
        "        split_name = os.path.basename(self.root_dir)  # e.g., 'train', 'validate', 'test'\n",
        "        split_embeddings_dir = os.path.join(self.embeddings_dir, split_name)\n",
        "        if not os.path.exists(split_embeddings_dir):\n",
        "            os.makedirs(split_embeddings_dir)\n",
        "\n",
        "        for session_id in self.session_ids:\n",
        "            base_id = session_id.split('_')[0]\n",
        "            session_path = os.path.join(self.root_dir, session_id)\n",
        "            embed_file_path = os.path.join(split_embeddings_dir, f\"{session_id}.pt\")\n",
        "\n",
        "            if os.path.exists(embed_file_path):\n",
        "                # Load precomputed embeddings\n",
        "                saved_data = torch.load(embed_file_path)\n",
        "                text_sequence = saved_data['text_sequence']\n",
        "                audio_sequence = saved_data['audio_sequence']\n",
        "                label = saved_data['label']\n",
        "                self.data.append((text_sequence, audio_sequence, label))\n",
        "                continue\n",
        "\n",
        "            # If embeddings file doesn't exist, proceed with computation\n",
        "            transcript_file, audio_file = self._find_files(session_path)\n",
        "            if transcript_file is None or audio_file is None:\n",
        "                print(f\"Missing transcript or audio for {session_id}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            filtered_df = self._load_and_filter_transcript(transcript_file)\n",
        "            if filtered_df is None:\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(audio_file):\n",
        "                print(f\"Audio file not found: {audio_file}\")\n",
        "                continue\n",
        "            full_audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "            aligned_data = []\n",
        "            for _, row in filtered_df.iterrows():\n",
        "                try:\n",
        "                    start_time = float(row[\"start_time\"])\n",
        "                    stop_time = float(row[\"stop_time\"])\n",
        "                    sentence = row[\"value\"]\n",
        "                    start_ms = int(start_time * 1000)\n",
        "                    stop_ms = int(stop_time * 1000)\n",
        "                    audio_segment = full_audio[start_ms:stop_ms]\n",
        "\n",
        "                    aligned_data.append({\n",
        "                        \"start_time\": start_time,\n",
        "                        \"stop_time\": stop_time,\n",
        "                        \"sentence\": sentence,\n",
        "                        \"audio_segment\": audio_segment\n",
        "                    })\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            if len(aligned_data) == 0:\n",
        "                continue\n",
        "\n",
        "            # Compute embeddings\n",
        "            text_embeddings = []\n",
        "            audio_embeddings = []\n",
        "            for entry in aligned_data:\n",
        "                text_emb = get_sentence_level_text_embedding(entry[\"sentence\"], self.tokenizer, self.text_model)\n",
        "                audio_emb = get_audio_embedding(entry[\"audio_segment\"], self.feature_extractor, self.wavlm_model)\n",
        "                text_embeddings.append(text_emb)\n",
        "                audio_embeddings.append(audio_emb)\n",
        "                #print(f\"Succesfully generate embedding of {entry}\")\n",
        "\n",
        "            if len(text_embeddings) == 0:\n",
        "                continue\n",
        "\n",
        "            text_sequence = torch.stack(text_embeddings, dim=0)\n",
        "            audio_sequence = torch.stack(audio_embeddings, dim=0)\n",
        "\n",
        "            if base_id not in self.label_dict:\n",
        "                print(f\"No label for {base_id}, defaulting to 0.\")\n",
        "                label = 0\n",
        "            else:\n",
        "                label = self.label_dict[base_id]\n",
        "\n",
        "            # Save embeddings to disk\n",
        "            torch.save({\n",
        "                'text_sequence': text_sequence,\n",
        "                'audio_sequence': audio_sequence,\n",
        "                'label': label\n",
        "            }, embed_file_path)\n",
        "\n",
        "            # Add to dataset memory\n",
        "            self.data.append((text_sequence, audio_sequence, label))\n",
        "\n",
        "    def _find_files(self, session_path):\n",
        "        transcript_file = None\n",
        "        audio_file = None\n",
        "        for fn in os.listdir(session_path):\n",
        "            if fn.endswith(\"_TRANSCRIPT.csv\"):\n",
        "                transcript_file = os.path.join(session_path, fn)\n",
        "            if fn.endswith(\"_AUDIO.wav\"):\n",
        "                audio_file = os.path.join(session_path, fn)\n",
        "        return transcript_file, audio_file\n",
        "\n",
        "    def _load_and_filter_transcript(self, transcript_file):\n",
        "        df = pd.read_csv(transcript_file, usecols=[0], header=None)\n",
        "        df_split = df[0].str.split('\\t', expand=True)\n",
        "        if df_split.shape[1] < 4:\n",
        "            print(f\"Transcript file {transcript_file} has unexpected format.\")\n",
        "            return None\n",
        "        df_split.columns = [\"start_time\", \"stop_time\", \"speaker\", \"value\"]\n",
        "        filtered_df = df_split[df_split[\"speaker\"] != \"Ellie\"]\n",
        "        return filtered_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPF3ycU-NThm"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    text_seqs = [item[0] for item in batch]\n",
        "    audio_seqs = [item[1] for item in batch]\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    # Pad sequences along time dimension\n",
        "    max_len_text = max(seq.size(0) for seq in text_seqs)\n",
        "    max_len_audio = max(seq.size(0) for seq in audio_seqs)\n",
        "\n",
        "    padded_text = []\n",
        "    for seq in text_seqs:\n",
        "        if seq.size(0) < max_len_text:\n",
        "            diff = max_len_text - seq.size(0)\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_text.append(seq.unsqueeze(0))\n",
        "    padded_text = torch.cat(padded_text, dim=0)  # [batch_size, max_len_text, D]\n",
        "\n",
        "    padded_audio = []\n",
        "    for seq in audio_seqs:\n",
        "        if seq.size(0) < max_len_audio:\n",
        "            diff = max_len_audio - seq.size(0)\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_audio.append(seq.unsqueeze(0))\n",
        "    padded_audio = torch.cat(padded_audio, dim=0) # [batch_size, max_len_audio, D]\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_text, padded_audio, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tES02nDBOU8w"
      },
      "outputs": [],
      "source": [
        "train_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/DD_final_project/csv/train_set.csv')\n",
        "val_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/DD_final_project/csv/validation_set.csv')\n",
        "test_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/DD_final_project/csv/full_test_split.csv')\n",
        "\n",
        "train_dir = os.path.join(database_dir, 'train')\n",
        "val_dir = os.path.join(database_dir, 'validate')\n",
        "test_dir = os.path.join(database_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "89Ao9LsxOvPO",
        "outputId": "c1d19027-1f9a-46f3-e958-7aec0dc40882"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No label for 344, defaulting to 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-455008569835>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAICDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavlm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-408e13facb2e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, label_csv_path, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-408e13facb2e>\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maligned_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_level_text_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0maudio_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_audio_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_segment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavlm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0maudio_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c09558a54ad8>\u001b[0m in \u001b[0;36mget_audio_embedding\u001b[0;34m(audio_segment, feature_extractor, wavlm_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavlm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# last_hidden_state: (1, time_steps, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/wavlm/modeling_wavlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/wavlm/modeling_wavlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 )\n\u001b[1;32m    367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/wavlm/modeling_wavlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_dataset = DAICDataset(train_dir, train_label_csv, tokenizer, text_model, feature_extractor, wavlm_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDsAV3w2Ttoq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(train_label_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26L-tQilT-Sc"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boR4Q2kUUAcu"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQpo28UpWlKF"
      },
      "source": [
        "# Attempt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzl2xMd8X_qM"
      },
      "outputs": [],
      "source": [
        "def get_sentence_level_text_embedding(sentence, tokenizer, text_model):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs)\n",
        "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
        "    return sentence_embedding.squeeze(0)\n",
        "\n",
        "def get_audio_embedding(audio_segment, feature_extractor, wavlm_model):\n",
        "    audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.int16)\n",
        "    audio_data_tensor = torch.tensor(audio_data, dtype=torch.float32).to(device)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        audio_data_tensor,\n",
        "        sampling_rate=audio_segment.frame_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = wavlm_model(**inputs)\n",
        "\n",
        "    # last_hidden_state: (1, time_steps, embedding_dim)\n",
        "    audio_embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "    # Average pooling over time\n",
        "    audio_embedding_vector = audio_embeddings.mean(dim=0)\n",
        "    return audio_embedding_vector\n",
        "\n",
        "def load_label_dict(label_csv_path):\n",
        "    \"\"\"\n",
        "    Loads a CSV file that contains at least these columns: \"Participant_ID\", \"PHQ8_Binary\".\n",
        "    Converts Participant_ID to an integer, then to a string to match how we handle directory-based IDs.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(label_csv_path)\n",
        "    label_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        # Convert participant_id to integer, then to string for consistency\n",
        "        base_id_str = str(int(row[\"Participant_ID\"]))  # Ensures \"315.0\" becomes \"315\"\n",
        "        label = int(row[\"PHQ8_Binary\"])\n",
        "        label_dict[base_id_str] = label\n",
        "        print(f\"Loaded label for {base_id_str}: {label}\")\n",
        "    return label_dict\n",
        "\n",
        "class DAICDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_csv_path, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir=None):\n",
        "        \"\"\"\n",
        "        root_dir: path to the split directory (train, validate, or test) containing session folders.\n",
        "        label_csv_path: path to the CSV file containing Participant_ID and PHQ8_Binary columns.\n",
        "        embeddings_dir: optional directory to save/load precomputed embeddings.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_model = text_model\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.wavlm_model = wavlm_model\n",
        "        self.device = device\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "\n",
        "        # Load the label dictionary\n",
        "        self.label_dict = load_label_dict(label_csv_path)\n",
        "\n",
        "        self.session_ids = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.data = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        # If embeddings_dir is provided, create split-specific dir\n",
        "        split_name = os.path.basename(self.root_dir)  # 'train', 'validate', 'test'\n",
        "        if self.embeddings_dir is not None:\n",
        "            split_embeddings_dir = os.path.join(self.embeddings_dir, split_name)\n",
        "            os.makedirs(split_embeddings_dir, exist_ok=True)\n",
        "        else:\n",
        "            split_embeddings_dir = None\n",
        "\n",
        "        for session_id in self.session_ids:\n",
        "            base_id_str = session_id.split('_')[0]  # e.g. \"315\" from \"315_P\"\n",
        "            print(base_id_str)\n",
        "            # Check if we have precomputed embeddings\n",
        "            embed_file_path = None\n",
        "            if split_embeddings_dir is not None:\n",
        "                embed_file_path = os.path.join(split_embeddings_dir, f\"{session_id}.pt\")\n",
        "                if os.path.exists(embed_file_path):\n",
        "                    # Load precomputed embeddings\n",
        "                    saved_data = torch.load(embed_file_path)\n",
        "                    text_sequence = saved_data['text_sequence']\n",
        "                    audio_sequence = saved_data['audio_sequence']\n",
        "                    label = saved_data['label']\n",
        "                    self.data.append((text_sequence, audio_sequence, label))\n",
        "                    continue\n",
        "\n",
        "            # If we don't have precomputed embeddings or embeddings_dir not used, process data\n",
        "            transcript_file, audio_file = self._find_files(session_id)\n",
        "            if transcript_file is None or audio_file is None:\n",
        "                print(f\"Transcript or audio missing for {session_id}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            filtered_df = self._load_and_filter_transcript(transcript_file)\n",
        "            if filtered_df is None or len(filtered_df) == 0:\n",
        "                # No valid transcripts after filtering Ellie\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(audio_file):\n",
        "                print(f\"Audio file not found: {audio_file}\")\n",
        "                continue\n",
        "\n",
        "            full_audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "            aligned_data = []\n",
        "            for _, row in filtered_df.iterrows():\n",
        "                try:\n",
        "                    start_time = float(row[\"start_time\"])\n",
        "                    stop_time = float(row[\"stop_time\"])\n",
        "                    sentence = row[\"value\"]\n",
        "                    start_ms = int(start_time * 1000)\n",
        "                    stop_ms = int(stop_time * 1000)\n",
        "                    audio_segment = full_audio[start_ms:stop_ms]\n",
        "\n",
        "                    aligned_data.append({\n",
        "                        \"start_time\": start_time,\n",
        "                        \"stop_time\": stop_time,\n",
        "                        \"sentence\": sentence,\n",
        "                        \"audio_segment\": audio_segment\n",
        "                    })\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            if len(aligned_data) == 0:\n",
        "                # No valid sentences\n",
        "                continue\n",
        "            min_duration = 0.1  # minimum duration in seconds\n",
        "\n",
        "            if (len(audio_segment) / 1000.0) < min_duration:\n",
        "            # Skip this segment\n",
        "                continue\n",
        "\n",
        "\n",
        "            text_embeddings = []\n",
        "            audio_embeddings = []\n",
        "            for entry in aligned_data:\n",
        "                text_emb = get_sentence_level_text_embedding(entry[\"sentence\"], self.tokenizer, self.text_model)\n",
        "                audio_emb = get_audio_embedding(entry[\"audio_segment\"], self.feature_extractor, self.wavlm_model)\n",
        "                text_embeddings.append(text_emb)\n",
        "                audio_embeddings.append(audio_emb)\n",
        "\n",
        "            if len(text_embeddings) == 0:\n",
        "                continue\n",
        "\n",
        "            # Stack embeddings: [num_sentences, embedding_dim]\n",
        "            text_sequence = torch.stack(text_embeddings, dim=0)\n",
        "            audio_sequence = torch.stack(audio_embeddings, dim=0)\n",
        "\n",
        "            # Move them to CPU before storing in self.data\n",
        "            text_sequence = text_sequence.cpu()\n",
        "            audio_sequence = audio_sequence.cpu()\n",
        "\n",
        "            if base_id_str not in self.label_dict:\n",
        "                print(f\"No label found for {base_id_str}\")\n",
        "                label = 0\n",
        "            else:\n",
        "                label = self.label_dict[base_id_str]\n",
        "                print(f\"{base_id_str}: {label}\")\n",
        "\n",
        "            # Save embeddings if path is given\n",
        "            if embed_file_path is not None:\n",
        "                torch.save({\n",
        "                    'text_sequence': text_sequence,\n",
        "                    'audio_sequence': audio_sequence,\n",
        "                    'label': label\n",
        "                }, embed_file_path)\n",
        "\n",
        "            self.data.append((text_sequence, audio_sequence, label))\n",
        "\n",
        "\n",
        "    def _find_files(self, session_id):\n",
        "        session_path = os.path.join(self.root_dir, session_id)\n",
        "        transcript_file = None\n",
        "        audio_file = None\n",
        "        for fn in os.listdir(session_path):\n",
        "            if fn.endswith(\"_TRANSCRIPT.csv\"):\n",
        "                transcript_file = os.path.join(session_path, fn)\n",
        "            if fn.endswith(\"_AUDIO.wav\"):\n",
        "                audio_file = os.path.join(session_path, fn)\n",
        "        return transcript_file, audio_file\n",
        "\n",
        "    def _load_and_filter_transcript(self, transcript_file):\n",
        "        df = pd.read_csv(transcript_file, usecols=[0], header=None)\n",
        "        df_split = df[0].str.split('\\t', expand=True)\n",
        "        if df_split.shape[1] < 4:\n",
        "            print(f\"Transcript file {transcript_file} has unexpected format.\")\n",
        "            return None\n",
        "        df_split.columns = [\"start_time\", \"stop_time\", \"speaker\", \"value\"]\n",
        "        filtered_df = df_split[df_split[\"speaker\"] != \"Ellie\"]\n",
        "        return filtered_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np3QNRF-iJy-"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    text_seqs = [item[0] for item in batch]  # CPU tensors\n",
        "    audio_seqs = [item[1] for item in batch] # CPU tensors\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    max_len_text = max(seq.size(0) for seq in text_seqs)\n",
        "    max_len_audio = max(seq.size(0) for seq in audio_seqs)\n",
        "\n",
        "    padded_text = []\n",
        "    for seq in text_seqs:\n",
        "        if seq.size(0) < max_len_text:\n",
        "            diff = max_len_text - seq.size(0)\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))  # should be CPU\n",
        "            # Debug prints for text sequence\n",
        "            #print(\"Text seq device before cat:\", seq.device)\n",
        "            #print(\"Text pad_tensor device before cat:\", pad_tensor.device)\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)  # Both seq and pad_tensor should be on CPU\n",
        "            #print(\"Text seq device after cat:\", seq.device)\n",
        "        padded_text.append(seq.unsqueeze(0))\n",
        "    padded_text = torch.cat(padded_text, dim=0)  # [batch_size, max_len_text, D]\n",
        "\n",
        "    padded_audio = []\n",
        "    for seq in audio_seqs:\n",
        "        if seq.size(0) < max_len_audio:\n",
        "            diff = max_len_audio - seq.size(0)\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))  # should be CPU\n",
        "            # Debug prints for audio sequence\n",
        "            #print(\"Audio seq device before cat:\", seq.device)\n",
        "            #print(\"Audio pad_tensor device before cat:\", pad_tensor.device)\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)  # Both seq and pad_tensor should be on CPU\n",
        "            #print(\"Audio seq device after cat:\", seq.device)\n",
        "        padded_audio.append(seq.unsqueeze(0))\n",
        "    padded_audio = torch.cat(padded_audio, dim=0) # [batch_size, max_len_audio, D]\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)  # CPU\n",
        "\n",
        "    #print(\"Final shapes:\", padded_text.shape, padded_audio.shape, labels.shape)\n",
        "    #print(\"Final devices:\", padded_text.device, padded_audio.device, labels.device)\n",
        "\n",
        "    return padded_text, padded_audio, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtCLLZ1zXYcK"
      },
      "outputs": [],
      "source": [
        "train_label_csv = \"/content/drive/MyDrive/pkl/train_set.csv\"\n",
        "val_label_csv = \"/content/drive/MyDrive/pkl/validation_set.csv\"\n",
        "test_label_csv = \"/content/drive/MyDrive/pkl/full_test_split.csv\"\n",
        "\n",
        "train_dir = os.path.join(database_dir, 'train')\n",
        "val_dir = os.path.join(database_dir, 'validate')\n",
        "test_dir = os.path.join(database_dir, 'test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPpF36nIEXB"
      },
      "source": [
        "# Generate .pt files to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N-dHK8yRJAWo"
      },
      "outputs": [],
      "source": [
        "train_embedding_dir = '/content/drive/MyDrive/Embeddings_sub/Embeddings_small/train'\n",
        "test_embedding_dir = '/content/drive/MyDrive/Embeddings_sub/Embeddings_small/test'\n",
        "val_embedding_dir = '/content/drive/MyDrive/Embeddings_sub/Embeddings_small/validate'\n",
        "embeddings_dir = '/content/drive/MyDrive/Embeddings_sub/Embeddings_small'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tcqY5LWFiq20"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'database_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_label_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdatabase_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/pkl/train_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m val_label_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(database_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/pkl/validation_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_label_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(database_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/pkl/full_test_split.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'database_dir' is not defined"
          ]
        }
      ],
      "source": [
        "train_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/pkl/train_set.csv')\n",
        "val_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/pkl/validation_set.csv')\n",
        "test_label_csv = os.path.join(database_dir, '/content/drive/MyDrive/pkl/full_test_split.csv')\n",
        "\n",
        "train_dir = os.path.join(database_dir, 'train')\n",
        "val_dir = os.path.join(database_dir, 'validate')\n",
        "test_dir = os.path.join(database_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4bAAsW0Jnin"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def get_sentence_level_text_embedding(sentence, tokenizer, text_model, device):\n",
        "    # Generate attention_mask for text embeddings\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs)\n",
        "    # Take [CLS] token embedding\n",
        "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return sentence_embedding.squeeze(0)\n",
        "\n",
        "def get_audio_embedding(audio_segment, feature_extractor, wavlm_model, device):\n",
        "    audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.int16)\n",
        "    audio_data_tensor = torch.tensor(audio_data, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Generate attention_mask for audio embeddings\n",
        "    inputs = feature_extractor(\n",
        "        audio_data_tensor,\n",
        "        sampling_rate=audio_segment.frame_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = wavlm_model(**inputs)\n",
        "\n",
        "    # outputs.last_hidden_state: (1, time_steps, embedding_dim)\n",
        "    audio_embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "    # Average pooling over time\n",
        "    audio_embedding_vector = audio_embeddings.mean(dim=0)\n",
        "    return audio_embedding_vector\n",
        "\n",
        "def load_label_dict(label_csv_path):\n",
        "    \"\"\"\n",
        "    Loads a CSV file that contains at least these columns: \"Participant_ID\", \"PHQ8_Binary\".\n",
        "    Converts Participant_ID to an integer, then to a string to match how we handle directory-based IDs.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(label_csv_path)\n",
        "    label_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        base_id_str = str(int(row[\"Participant_ID\"]))  # Ensures \"315.0\" becomes \"315\"\n",
        "        label = int(row[\"PHQ8_Binary\"])\n",
        "        label_dict[base_id_str] = label\n",
        "        print(f\"Loaded label for {base_id_str}: {label}\")\n",
        "    return label_dict\n",
        "\n",
        "class DAICDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_csv_path, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir=None):\n",
        "        \"\"\"\n",
        "        root_dir: path to the split directory (train, validate, or test) containing session folders.\n",
        "        label_csv_path: path to the CSV file containing Participant_ID and PHQ8_Binary columns.\n",
        "        embeddings_dir: optional directory to save/load precomputed embeddings.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_model = text_model\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.wavlm_model = wavlm_model\n",
        "        self.device = device\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "\n",
        "        # Load the label dictionary\n",
        "        self.label_dict = load_label_dict(label_csv_path)\n",
        "\n",
        "        self.session_ids = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.data = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        # If embeddings_dir is provided, create split-specific dir\n",
        "        split_name = os.path.basename(self.root_dir)  # 'train', 'validate', 'test'\n",
        "        if self.embeddings_dir is not None:\n",
        "            split_embeddings_dir = os.path.join(self.embeddings_dir, split_name)\n",
        "            os.makedirs(split_embeddings_dir, exist_ok=True)\n",
        "        else:\n",
        "            split_embeddings_dir = None\n",
        "\n",
        "        for session_id in self.session_ids:\n",
        "            base_id_str = session_id.split('_')[0]  # e.g. \"315\" from \"315_P\"\n",
        "            print(base_id_str)\n",
        "            # Check if we have precomputed embeddings\n",
        "            embed_file_path = None\n",
        "            if split_embeddings_dir is not None:\n",
        "                embed_file_path = os.path.join(split_embeddings_dir, f\"{session_id}.pt\")\n",
        "                if os.path.exists(embed_file_path):\n",
        "                    # Load precomputed embeddings\n",
        "                    saved_data = torch.load(embed_file_path)\n",
        "                    text_sequence = saved_data['text_sequence']\n",
        "                    audio_sequence = saved_data['audio_sequence']\n",
        "                    label = saved_data['label']\n",
        "                    self.data.append((text_sequence, audio_sequence, label))\n",
        "                    continue\n",
        "\n",
        "            # If we don't have precomputed embeddings or embeddings_dir not used, process data\n",
        "            transcript_file, audio_file = self._find_files(session_id)\n",
        "            if transcript_file is None or audio_file is None:\n",
        "                print(f\"Transcript or audio missing for {session_id}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            filtered_df = self._load_and_filter_transcript(transcript_file)\n",
        "            if filtered_df is None or len(filtered_df) == 0:\n",
        "                # No valid transcripts after filtering Ellie\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(audio_file):\n",
        "                print(f\"Audio file not found: {audio_file}\")\n",
        "                continue\n",
        "\n",
        "            full_audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "            aligned_data = []\n",
        "            for _, row in filtered_df.iterrows():\n",
        "                try:\n",
        "                    start_time = float(row[\"start_time\"])\n",
        "                    stop_time = float(row[\"stop_time\"])\n",
        "                    sentence = row[\"value\"]\n",
        "                    start_ms = int(start_time * 1000)\n",
        "                    stop_ms = int(stop_time * 1000)\n",
        "                    audio_segment = full_audio[start_ms:stop_ms]\n",
        "\n",
        "                    # Minimum duration\n",
        "                    min_duration = 0.1  # seconds\n",
        "                    if (len(audio_segment) / 1000.0) < min_duration:\n",
        "                        continue\n",
        "\n",
        "                    aligned_data.append({\n",
        "                        \"start_time\": start_time,\n",
        "                        \"stop_time\": stop_time,\n",
        "                        \"sentence\": sentence,\n",
        "                        \"audio_segment\": audio_segment\n",
        "                    })\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            if len(aligned_data) == 0:\n",
        "                # No valid sentences\n",
        "                continue\n",
        "\n",
        "            text_embeddings = []\n",
        "            audio_embeddings = []\n",
        "            for entry in aligned_data:\n",
        "                text_emb = get_sentence_level_text_embedding(entry[\"sentence\"], self.tokenizer, self.text_model, self.device)\n",
        "                audio_emb = get_audio_embedding(entry[\"audio_segment\"], self.feature_extractor, self.wavlm_model, self.device)\n",
        "                text_embeddings.append(text_emb)\n",
        "                audio_embeddings.append(audio_emb)\n",
        "\n",
        "            if len(text_embeddings) == 0:\n",
        "                continue\n",
        "\n",
        "            # Stack embeddings: [num_sentences, embedding_dim]\n",
        "            text_sequence = torch.stack(text_embeddings, dim=0)\n",
        "            audio_sequence = torch.stack(audio_embeddings, dim=0)\n",
        "\n",
        "            # Move them to CPU before storing in self.data\n",
        "            text_sequence = text_sequence.cpu()\n",
        "            audio_sequence = audio_sequence.cpu()\n",
        "\n",
        "            if base_id_str not in self.label_dict:\n",
        "                print(f\"No label found for {base_id_str}\")\n",
        "                label = 0\n",
        "            else:\n",
        "                label = self.label_dict[base_id_str]\n",
        "                print(f\"{base_id_str}: {label}\")\n",
        "\n",
        "            # Save embeddings if path is given\n",
        "            if embed_file_path is not None:\n",
        "                torch.save({\n",
        "                    'text_sequence': text_sequence,\n",
        "                    'audio_sequence': audio_sequence,\n",
        "                    'label': label\n",
        "                }, embed_file_path)\n",
        "\n",
        "            self.data.append((text_sequence, audio_sequence, label))\n",
        "\n",
        "    def _find_files(self, session_id):\n",
        "        session_path = os.path.join(self.root_dir, session_id)\n",
        "        transcript_file = None\n",
        "        audio_file = None\n",
        "        for fn in os.listdir(session_path):\n",
        "            if fn.endswith(\"_TRANSCRIPT.csv\"):\n",
        "                transcript_file = os.path.join(session_path, fn)\n",
        "            if fn.endswith(\"_AUDIO.wav\"):\n",
        "                audio_file = os.path.join(session_path, fn)\n",
        "        return transcript_file, audio_file\n",
        "\n",
        "    def _load_and_filter_transcript(self, transcript_file):\n",
        "        df = pd.read_csv(transcript_file, usecols=[0], header=None)\n",
        "        df_split = df[0].str.split('\\t', expand=True)\n",
        "        if df_split.shape[1] < 4:\n",
        "            print(f\"Transcript file {transcript_file} has unexpected format.\")\n",
        "            return None\n",
        "        df_split.columns = [\"start_time\", \"stop_time\", \"speaker\", \"value\"]\n",
        "        filtered_df = df_split[df_split[\"speaker\"] != \"Ellie\"]\n",
        "        return filtered_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    text_seqs = [item[0] for item in batch]  # CPU tensors [seq_len, text_dim]\n",
        "    audio_seqs = [item[1] for item in batch] # CPU tensors [seq_len, audio_dim]\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    # Compute max lengths for padding\n",
        "    max_len_text = max(seq.size(0) for seq in text_seqs)\n",
        "    max_len_audio = max(seq.size(0) for seq in audio_seqs)\n",
        "\n",
        "    # Initialize masks\n",
        "    text_mask = torch.zeros(len(text_seqs), max_len_text, dtype=torch.long)\n",
        "    audio_mask = torch.zeros(len(audio_seqs), max_len_audio, dtype=torch.long)\n",
        "\n",
        "    padded_text = []\n",
        "    for i, seq in enumerate(text_seqs):\n",
        "        original_len = seq.size(0)\n",
        "        if original_len < max_len_text:\n",
        "            diff = max_len_text - original_len\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))  # CPU\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_text.append(seq.unsqueeze(0))\n",
        "        text_mask[i, :original_len] = 1\n",
        "\n",
        "    padded_text = torch.cat(padded_text, dim=0)  # [batch_size, max_len_text, text_dim]\n",
        "\n",
        "    padded_audio = []\n",
        "    for i, seq in enumerate(audio_seqs):\n",
        "        original_len = seq.size(0)\n",
        "        if original_len < max_len_audio:\n",
        "            diff = max_len_audio - original_len\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))  # CPU\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_audio.append(seq.unsqueeze(0))\n",
        "        audio_mask[i, :original_len] = 1\n",
        "\n",
        "    padded_audio = torch.cat(padded_audio, dim=0) # [batch_size, max_len_audio, audio_dim]\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)  # CPU\n",
        "\n",
        "    return padded_text, padded_audio, text_mask, audio_mask, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3-AoFJ6a5Zcy",
        "outputId": "dc841ae9-597f-4f9c-bd7b-e99519382be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded label for 392: 0\n",
            "Loaded label for 488: 0\n",
            "Loaded label for 344: 1\n",
            "Loaded label for 368: 0\n",
            "Loaded label for 319: 1\n",
            "Loaded label for 313: 0\n",
            "Loaded label for 383: 0\n",
            "Loaded label for 385: 0\n",
            "Loaded label for 318: 0\n",
            "Loaded label for 370: 0\n",
            "Loaded label for 339: 1\n",
            "Loaded label for 336: 0\n",
            "Loaded label for 433: 1\n",
            "Loaded label for 356: 1\n",
            "Loaded label for 374: 0\n",
            "Loaded label for 486: 0\n",
            "Loaded label for 443: 0\n",
            "Loaded label for 414: 1\n",
            "Loaded label for 426: 1\n",
            "Loaded label for 352: 1\n",
            "Loaded label for 330: 1\n",
            "Loaded label for 324: 0\n",
            "Loaded label for 386: 1\n",
            "Loaded label for 321: 1\n",
            "Loaded label for 347: 1\n",
            "Loaded label for 310: 0\n",
            "Loaded label for 468: 0\n",
            "Loaded label for 428: 0\n",
            "Loaded label for 379: 0\n",
            "Loaded label for 474: 0\n",
            "Loaded label for 412: 1\n",
            "Loaded label for 475: 0\n",
            "Loaded label for 360: 0\n",
            "Loaded label for 402: 1\n",
            "Loaded label for 327: 0\n",
            "Loaded label for 427: 0\n",
            "Loaded label for 391: 0\n",
            "Loaded label for 441: 1\n",
            "Loaded label for 401: 0\n",
            "Loaded label for 345: 1\n",
            "Loaded label for 409: 0\n",
            "Loaded label for 447: 0\n",
            "Loaded label for 366: 0\n",
            "Loaded label for 457: 0\n",
            "Loaded label for 317: 0\n",
            "Loaded label for 350: 1\n",
            "Loaded label for 315: 0\n",
            "Loaded label for 364: 0\n",
            "Loaded label for 372: 1\n",
            "Loaded label for 448: 1\n",
            "Loaded label for 419: 0\n",
            "Loaded label for 303: 0\n",
            "Loaded label for 416: 0\n",
            "Loaded label for 369: 0\n",
            "Loaded label for 425: 0\n",
            "Loaded label for 445: 0\n",
            "Loaded label for 429: 0\n",
            "Loaded label for 333: 0\n",
            "Loaded label for 341: 0\n",
            "Loaded label for 397: 0\n",
            "Loaded label for 444: 0\n",
            "Loaded label for 320: 1\n",
            "Loaded label for 449: 0\n",
            "Loaded label for 430: 0\n",
            "Loaded label for 375: 0\n",
            "Loaded label for 326: 0\n",
            "Loaded label for 423: 0\n",
            "Loaded label for 312: 0\n",
            "Loaded label for 322: 0\n",
            "Loaded label for 478: 0\n",
            "Loaded label for 338: 1\n",
            "Loaded label for 464: 0\n",
            "Loaded label for 456: 0\n",
            "Loaded label for 415: 0\n",
            "Loaded label for 325: 1\n",
            "Loaded label for 400: 0\n",
            "Loaded label for 446: 0\n",
            "Loaded label for 491: 0\n",
            "Loaded label for 343: 0\n",
            "Loaded label for 487: 0\n",
            "Loaded label for 348: 1\n",
            "Loaded label for 376: 1\n",
            "Loaded label for 471: 0\n",
            "Loaded label for 316: 0\n",
            "Loaded label for 340: 0\n",
            "303\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0b1dd141c244>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Embeddings'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAICDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir_subset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavlm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-05546b9d57b9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, label_csv_path, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-05546b9d57b9>\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mfull_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0maligned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_wav\u001b[0;34m(cls, file, parameters)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmediainfo_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_ahead_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_ahead_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             audio_streams = [x for x in info['streams']\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydub/utils.py\u001b[0m in \u001b[0;36mmediainfo_json\u001b[0;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprober\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-of'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcommand_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin_parameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_dir_subset1 = '/content/drive/MyDrive/DAIC_WOZ/train'\n",
        "embeddings_dir = '/content/drive/MyDrive/Embeddings'\n",
        "\n",
        "train_dataset_1 = DAICDataset(train_dir_subset1, train_label_csv, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir=embeddings_dir)\n",
        "train_loader_1 = DataLoader(train_dataset_1, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for batch in train_loader_1:\n",
        "    # This ensures embeddings are generated and saved\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcuaxVDi8D7o"
      },
      "outputs": [],
      "source": [
        "val_dir_subset1 = '/content/drive/MyDrive/DAIC_WOZ/validate'\n",
        "val_dataset_1 = DAICDataset(val_dir_subset1, val_label_csv, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir=embeddings_dir)\n",
        "val_loader_1 = DataLoader(val_dataset_1, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for batch in val_loader_1:\n",
        "    # This ensures embeddings are generated and saved\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1DLl3vm9AG2"
      },
      "outputs": [],
      "source": [
        "test_dir_subset1 = '/content/drive/MyDrive/DAIC_WOZ/test'\n",
        "test_dataset_1 = DAICDataset(test_dir_subset1, test_label_csv, tokenizer, text_model, feature_extractor, wavlm_model, device, embeddings_dir=embeddings_dir)\n",
        "test_loader_1 = DataLoader(test_dataset_1, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for batch in val_loader_1:\n",
        "    # This ensures embeddings are generated and saved\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVy4lFVDWibo"
      },
      "source": [
        "# Loading Pre-computed .pt files from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9fTcnzIenBqT"
      },
      "outputs": [],
      "source": [
        "class PrecomputedEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, embeddings_dir):\n",
        "        \"\"\"\n",
        "        embeddings_dir: Directory containing .pt files.\n",
        "        Each .pt file should contain a dictionary:\n",
        "          {\n",
        "            'text_sequence': torch.Tensor, # [num_sentences, text_dim]\n",
        "            'audio_sequence': torch.Tensor, # [num_sentences, audio_dim]\n",
        "            'label': int\n",
        "          }\n",
        "        \"\"\"\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.data = []\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        # Get all .pt files in the directory\n",
        "        all_files = [f for f in os.listdir(self.embeddings_dir) if f.endswith('.pt')]\n",
        "        if not all_files:\n",
        "            print(f\"No .pt files found in {self.embeddings_dir}\")\n",
        "\n",
        "        for fname in all_files:\n",
        "            file_path = os.path.join(self.embeddings_dir, fname)\n",
        "            try:\n",
        "                saved_data = torch.load(file_path, map_location='cpu')\n",
        "                text_sequence = saved_data['text_sequence']\n",
        "                audio_sequence = saved_data['audio_sequence']\n",
        "                label = saved_data['label']\n",
        "                self.data.append((text_sequence, audio_sequence, label))\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} sessions from {self.embeddings_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns a tuple: (text_sequence, audio_sequence, label)\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    text_seqs = [item[0] for item in batch]  # text embeddings\n",
        "    audio_seqs = [item[1] for item in batch] # audio embeddings\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    # Compute padding lengths\n",
        "    max_len_text = max(seq.size(0) for seq in text_seqs) if len(text_seqs) > 0 else 0\n",
        "    max_len_audio = max(seq.size(0) for seq in audio_seqs) if len(audio_seqs) > 0 else 0\n",
        "\n",
        "    # Create masks\n",
        "    text_mask = torch.zeros(len(text_seqs), max_len_text, dtype=torch.long)\n",
        "    audio_mask = torch.zeros(len(audio_seqs), max_len_audio, dtype=torch.long)\n",
        "\n",
        "    padded_text = []\n",
        "    for i, seq in enumerate(text_seqs):\n",
        "        original_len = seq.size(0)\n",
        "        if original_len < max_len_text:\n",
        "            diff = max_len_text - original_len\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_text.append(seq.unsqueeze(0))\n",
        "        text_mask[i, :original_len] = 1\n",
        "\n",
        "    padded_text = torch.cat(padded_text, dim=0) if len(padded_text) > 0 else torch.empty(0)\n",
        "\n",
        "    padded_audio = []\n",
        "    for i, seq in enumerate(audio_seqs):\n",
        "        original_len = seq.size(0)\n",
        "        if original_len < max_len_audio:\n",
        "            diff = max_len_audio - original_len\n",
        "            pad_tensor = torch.zeros(diff, seq.size(1))\n",
        "            seq = torch.cat([seq, pad_tensor], dim=0)\n",
        "        padded_audio.append(seq.unsqueeze(0))\n",
        "        audio_mask[i, :original_len] = 1\n",
        "\n",
        "    padded_audio = torch.cat(padded_audio, dim=0) if len(padded_audio) > 0 else torch.empty(0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_text, padded_audio, text_mask, audio_mask, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPxgLOFYXZSD"
      },
      "outputs": [],
      "source": [
        "train_embedding_dir = '/home/popsatorn/Desktop/DD_FinalProject/Embeddings_Base'\n",
        "val_embedding_dir = '/home/popsatorn/Desktop/DD_FinalProject/Embeddings_Base'\n",
        "test_embedding_dir = '/home/popsatorn/Desktop/DD_FinalProject/Embeddings_Base'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O29y8JPmJ8uj",
        "outputId": "6c310704-5a33-4d99-9d7e-cf50685f49f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-a78ae2d4be15>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_data = torch.load(file_path, map_location='cpu')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 83 sessions from /content/drive/MyDrive/Embeddings_sub/train\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PrecomputedEmbeddingsDataset(train_embedding_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lapqKmzooOpo",
        "outputId": "083b7a40-2ac3-44f9-d858-79d33f7d8899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 21 sessions from /content/drive/MyDrive/Embeddings_sub/validate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-a78ae2d4be15>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_data = torch.load(file_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "val_dataset = PrecomputedEmbeddingsDataset(val_embedding_dir)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tISYjwLvM2S1",
        "outputId": "f31f72be-8b54-4756-9b09-98b5f894687d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-a78ae2d4be15>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_data = torch.load(file_path, map_location='cpu')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 47 sessions from /content/drive/MyDrive/Embeddings_sub/test\n"
          ]
        }
      ],
      "source": [
        "test_dataset = PrecomputedEmbeddingsDataset(test_embedding_dir)\n",
        "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gll_dvMMpAib",
        "outputId": "3363ab7d-03da-4ce9-fbc3-5c636e59e9ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1:\n",
            "  Padded Text Shape: torch.Size([32, 367, 1024])\n",
            "  Padded Audio Shape: torch.Size([32, 367, 1024])\n",
            "  Text Mask Shape: torch.Size([32, 367])\n",
            "  Audio Mask Shape: torch.Size([32, 367])\n",
            "  Labels Shape: torch.Size([32])\n",
            "  Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "--------------------------------------------------\n",
            "Batch 2:\n",
            "  Padded Text Shape: torch.Size([32, 386, 1024])\n",
            "  Padded Audio Shape: torch.Size([32, 386, 1024])\n",
            "  Text Mask Shape: torch.Size([32, 386])\n",
            "  Audio Mask Shape: torch.Size([32, 386])\n",
            "  Labels Shape: torch.Size([32])\n",
            "  Labels: [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "--------------------------------------------------\n",
            "Batch 3:\n",
            "  Padded Text Shape: torch.Size([19, 286, 1024])\n",
            "  Padded Audio Shape: torch.Size([19, 286, 1024])\n",
            "  Text Mask Shape: torch.Size([19, 286])\n",
            "  Audio Mask Shape: torch.Size([19, 286])\n",
            "  Labels Shape: torch.Size([19])\n",
            "  Labels: [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (padded_text, padded_audio, text_mask, audio_mask, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"  Padded Text Shape: {padded_text.shape}\")  # [batch_size, max_len_text, text_dim]\n",
        "    print(f\"  Padded Audio Shape: {padded_audio.shape}\")  # [batch_size, max_len_audio, audio_dim]\n",
        "    print(f\"  Text Mask Shape: {text_mask.shape}\")  # [batch_size, max_len_text]\n",
        "    print(f\"  Audio Mask Shape: {audio_mask.shape}\")  # [batch_size, max_len_audio]\n",
        "    print(f\"  Labels Shape: {labels.shape}\")  # [batch_size]\n",
        "    print(f\"  Labels: {labels.tolist()}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFJkuWKWsic"
      },
      "source": [
        "# Vanilla MFFNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P9doVnm5fvYz"
      },
      "outputs": [],
      "source": [
        "dropout_prob = 0.2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaTokenizer, RobertaModel, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Fastformer and Related Components\n",
        "class FastSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastSelfAttention, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" %\n",
        "                (config.hidden_size, config.num_attention_heads))\n",
        "        \n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.input_dim = config.hidden_size\n",
        "\n",
        "        self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                       self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        batch_size, seq_len_hidden, _ = hidden_states.size()\n",
        "        seq_len_context = context_states.size(1)\n",
        "\n",
        "        # Compute queries and keys\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(context_states)\n",
        "\n",
        "        # Compute attention scores\n",
        "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "        key_for_score = self.key_att(mixed_key_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "\n",
        "        attention_scores = torch.matmul(query_for_score.transpose(1, 2), key_for_score)  # [B, seq_len_hidden, seq_len_context]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores += attention_mask\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context_layer = torch.matmul(attention_weights, mixed_key_layer)  # [B, seq_len_hidden, hidden_size]\n",
        "\n",
        "        # Combine with value projection\n",
        "        output = self.transform(context_layer) + mixed_query_layer\n",
        "        return output\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastAttention, self).__init__()\n",
        "        self.self = FastSelfAttention(config)\n",
        "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, input_tensor, context_tensor, attention_mask=None):\n",
        "        self_output = self.self(input_tensor, context_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output) + input_tensor\n",
        "        return attention_output\n",
        "\n",
        "class FastformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastformerLayer, self).__init__()\n",
        "        self.attention = FastAttention(config)\n",
        "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.activation = nn.GELU()\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        attention_output = self.attention(hidden_states, context_states, attention_mask)\n",
        "        attention_output = self.layernorm(attention_output)\n",
        "        intermediate_output = self.activation(self.intermediate(attention_output))\n",
        "        layer_output = self.output(intermediate_output) + attention_output\n",
        "        layer_output = self.layernorm(layer_output)\n",
        "        return layer_output\n",
        "\n",
        "class MSFastformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MSFastformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.conv1 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=1, padding=0)\n",
        "        self.conv3 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=5, padding=2)\n",
        "        self.fastformer = FastformerLayer(config)\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.output_fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.apply(self.init_weights)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        batch_size, seq_len, hidden_size = x.size()\n",
        "        x = self.layernorm(x)\n",
        "        x_transposed = x.transpose(1, 2)  # [B, hidden_size, seq_len]\n",
        "\n",
        "        U1 = self.conv1(x_transposed)  # [B, hidden_size, seq_len]\n",
        "        U3 = self.conv3(x_transposed)\n",
        "        U5 = self.conv5(x_transposed)\n",
        "\n",
        "        U1 = U1.transpose(1, 2)\n",
        "        U3 = U3.transpose(1, 2)\n",
        "        U5 = U5.transpose(1, 2)\n",
        "\n",
        "        P1 = self.fastformer(U5, U3, attention_mask=None)\n",
        "        P2 = self.fastformer(U3, U1, attention_mask=None)\n",
        "        P3 = self.fastformer(U1, U5, attention_mask=None)\n",
        "        P = P1 + P2 + P3\n",
        "        P_norm = self.layernorm(P)\n",
        "\n",
        "        I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "        I = self.dropout(I)\n",
        "        J = I + x\n",
        "        J_norm = self.layernorm(J)\n",
        "        y = self.output_fc(J_norm)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class Config:\n",
        "    def __init__(self,\n",
        "                 hidden_size=768,\n",
        "                 num_attention_heads=16,\n",
        "                 intermediate_size=3072,\n",
        "                 num_labels=2,\n",
        "                 num_hidden_layers=12,\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 layer_norm_eps=1e-12,\n",
        "                 initializer_range=0.02,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 pooler_type='weightpooler',\n",
        "                 num_attention_layers=12):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_labels = num_labels\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.initializer_range = initializer_range\n",
        "        self.hidden_act = hidden_act\n",
        "        self.pooler_type = pooler_type\n",
        "        self.num_attention_layers = num_attention_layers\n",
        "\n",
        "# Gated Fusion\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.fc1 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
        "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
        "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, Et, Ea):\n",
        "        assert Et.shape == Ea.shape, \"Et and Ea must have the same shape.\"\n",
        "        concat = torch.cat((Et, Ea), dim=-1)  # [B, T, 2hidden_dim]\n",
        "        out = self.fc1(concat)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        g = self.sigmoid(out)\n",
        "        Ef = g * Ea + (1 - g) * Et\n",
        "        return Ef\n",
        "\n",
        "# Multimodal Feature Enhancer (MFE) with BiFPN\n",
        "class SeparableConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, norm=True, activation=True):\n",
        "        super(SeparableConvBlock, self).__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                        padding=padding, groups=in_channels, bias=False)\n",
        "        self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.norm = nn.BatchNorm1d(out_channels) if norm else None\n",
        "        self.activation = nn.ReLU() if activation else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class BiFPNLayer(nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super(BiFPNLayer, self).__init__()\n",
        "        self.conv_p2_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p1_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p0_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p0_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p1_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p2_out = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, p0, p1, p2):\n",
        "        p2_td = self.conv_p2_td(p2)\n",
        "        p2_upsampled = F.interpolate(p2_td, size=p1.size(-1), mode='linear', align_corners=False)\n",
        "        p1_td = self.conv_p1_td(p1 + p2_upsampled)\n",
        "\n",
        "        p1_upsampled = F.interpolate(p1_td, size=p0.size(-1), mode='linear', align_corners=False)\n",
        "        p0_td = self.conv_p0_td(p0 + p1_upsampled)\n",
        "\n",
        "        p0_downsampled = F.interpolate(p0_td, size=p1_td.size(-1), mode='linear', align_corners=False)\n",
        "        p0_out = self.conv_p0_out(p0_downsampled + p1_td)\n",
        "\n",
        "        p0_out_downsampled = F.interpolate(p0_out, size=p2_td.size(-1), mode='linear', align_corners=False)\n",
        "        p1_out = self.conv_p1_out(p0_out_downsampled + p2_td)\n",
        "\n",
        "        p1_out_downsampled = F.interpolate(p1_out, size=p2_td.size(-1), mode='linear', align_corners=False)\n",
        "        p2_out = self.conv_p2_out(p1_out_downsampled + p2_td)\n",
        "\n",
        "        return p0_td, p1_td, p2_td, p0_out, p1_out, p2_out\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    def __init__(self, feature_size=256, num_layers=1):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.bifpn_layers = nn.ModuleList([BiFPNLayer(feature_size) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, features):\n",
        "        p0, p1, p2 = features\n",
        "        for bifpn_layer in self.bifpn_layers:\n",
        "            p0_td, p1_td, p2_td, p0_out, p1_out, p2_out = bifpn_layer(p0, p1, p2)\n",
        "            p0, p1, p2 = p0_td, p1_td, p2_td\n",
        "        return [p0_td, p1_td, p2_td, p0_out, p1_out, p2_out]\n",
        "\n",
        "class MultimodalFeatureEnhancer(nn.Module):\n",
        "    def __init__(self, input_dim, feature_size=256, num_bifpn_layers=2):\n",
        "        super(MultimodalFeatureEnhancer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_size = feature_size\n",
        "        self.conv = nn.Conv1d(input_dim, feature_size, kernel_size=1)\n",
        "        self.conv_p1 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_p2 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "        self.bifpn = BiFPN(feature_size=feature_size, num_layers=num_bifpn_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        p0 = self.conv(x)\n",
        "        p1 = self.conv_p1(p0)\n",
        "        p2 = self.conv_p2(p1)\n",
        "        outputs = self.bifpn([p0, p1, p2])\n",
        "        # Transpose each back to [B, T, feature_size]\n",
        "        out_list = []\n",
        "        for out in outputs:\n",
        "            out_list.append(out.transpose(1, 2))\n",
        "        return out_list\n",
        "\n",
        "# AdaptiveFusionModule (AFM)\n",
        "class FeatureAlignment(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(FeatureAlignment, self).__init__()\n",
        "        self.align_convs = nn.ModuleList([\n",
        "            nn.Conv1d(feature_dim, feature_dim, kernel_size=1)\n",
        "            for _ in range(6)\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        max_len = max(f.shape[1] for f in features)\n",
        "        aligned_features = []\n",
        "        for i, f in enumerate(features):\n",
        "            f = f.transpose(1, 2)\n",
        "            f_aligned = self.align_convs[i](f)\n",
        "            if f_aligned.shape[2] != max_len:\n",
        "                f_aligned = F.interpolate(f_aligned, size=max_len, mode='linear', align_corners=False)\n",
        "            aligned_features.append(f_aligned)\n",
        "        H = torch.stack(aligned_features, dim=1)  # [B, 6, D, T]\n",
        "        return H\n",
        "\n",
        "class MultiFeatureFusion(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(MultiFeatureFusion, self).__init__()\n",
        "        self.attention_fc = nn.Linear(12, 6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, H):\n",
        "        B, N, D, T = H.shape\n",
        "        mean_T = H.mean(dim=3)   # [B, 6, D]\n",
        "        mean_D = H.mean(dim=2)   # [B, 6, T]\n",
        "        mean_TD = mean_T.mean(dim=2) # [B, 6]\n",
        "        mean_DT = mean_D.mean(dim=2) # [B, 6]\n",
        "        mean_features = torch.cat([mean_TD, mean_DT], dim=1) # [B, 12]\n",
        "        attention_scores = self.attention_fc(mean_features)  # [B, 6]\n",
        "        attention_scores = self.relu(attention_scores)\n",
        "        attention_weights = self.softmax(attention_scores).unsqueeze(-1).unsqueeze(-1) # [B, 6, 1, 1]\n",
        "        weighted_H = H * attention_weights\n",
        "        fused_features = weighted_H.sum(dim=1) # [B, D, T]\n",
        "        residual = H.mean(dim=1) # [B, D, T]\n",
        "        fused_features = fused_features + residual\n",
        "        fused_features = fused_features.transpose(1, 2) # [B, T, D]\n",
        "        return fused_features\n",
        "\n",
        "class AdaptiveFusionModule(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(AdaptiveFusionModule, self).__init__()\n",
        "        self.feature_alignment = FeatureAlignment(feature_dim)\n",
        "        self.feature_fusion = MultiFeatureFusion(feature_dim)\n",
        "\n",
        "    def forward(self, features):\n",
        "        H = self.feature_alignment(features)  # [B, 6, D, T]\n",
        "        fused_output = self.feature_fusion(H) # [B, T, D]\n",
        "        return fused_output\n",
        "\n",
        "##########################################################\n",
        "# Final MFFNet Model\n",
        "##########################################################\n",
        "class MFFNet(nn.Module):\n",
        "    def __init__(self, text_config, audio_config, hidden_dim=768, feature_size=256, num_bifpn_layers=2, num_labels=2):\n",
        "        super(MFFNet, self).__init__()\n",
        "        # Two MSFastformer models for text and audio\n",
        "        self.text_model = MSFastformer(text_config)\n",
        "        self.audio_model = MSFastformer(audio_config)\n",
        "\n",
        "        # Gated Fusion\n",
        "        self.gated_fusion = GatedFusion(hidden_dim=hidden_dim)\n",
        "\n",
        "        # Multimodal Feature Enhancer\n",
        "        self.mfe = MultimodalFeatureEnhancer(input_dim=hidden_dim, feature_size=feature_size, num_bifpn_layers=num_bifpn_layers)\n",
        "\n",
        "        # Adaptive Fusion Module\n",
        "        self.afm = AdaptiveFusionModule(feature_dim=feature_size)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(feature_size, num_labels)\n",
        "\n",
        "    def forward(self, input_text, input_audio, attention_mask_text=None, attention_mask_audio=None):\n",
        "        output_text = self.text_model(input_text, attention_mask_text)    # [B, T, hidden_dim]\n",
        "        output_audio = self.audio_model(input_audio, attention_mask_audio) # [B, T, hidden_dim]\n",
        "\n",
        "        # Fuse text and audio representations\n",
        "        Ef = self.gated_fusion(output_text, output_audio)  # [B, T, hidden_dim]\n",
        "\n",
        "        # Apply MultimodalFeatureEnhancer (MFE)\n",
        "        features = self.mfe(Ef)  # List of 6 feature maps: each [B, T_i, feature_size]\n",
        "\n",
        "        # Apply AdaptiveFusionModule (AFM)\n",
        "        fused_output = self.afm(features)  # [B, T, feature_size]\n",
        "\n",
        "        # Pool the fused output (mean pooling)\n",
        "        fused_pooled = fused_output.mean(dim=1)  # [B, feature_size]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_pooled) # [B, 2]\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xLmoVJShc-j3"
      },
      "outputs": [],
      "source": [
        "class MFFNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 text_config: Config,\n",
        "                 audio_config: Config,\n",
        "                 hidden_dim=768,\n",
        "                 feature_size=256,\n",
        "                 num_bifpn_layers=2,\n",
        "                 num_labels=2):\n",
        "        super(MFFNet, self).__init__()\n",
        "        self.text_model = MSFastformer(text_config)\n",
        "        self.audio_model = MSFastformer(audio_config)\n",
        "        self.gated_fusion = GatedFusion(hidden_dim=hidden_dim)\n",
        "        self.mfe = MultimodalFeatureEnhancer(input_dim=hidden_dim,\n",
        "                                             feature_size=feature_size,\n",
        "                                             num_bifpn_layers=num_bifpn_layers)\n",
        "        self.afm = AdaptiveFusionModule(feature_dim=feature_size)\n",
        "        self.classifier = nn.Linear(feature_size, num_labels)\n",
        "\n",
        "    def forward(self, input_text, input_audio, attention_mask_text=None, attention_mask_audio=None):\n",
        "\n",
        "        output_text = self.text_model(input_text, attention_mask_text)    # [B, T, H]\n",
        "        output_audio = self.audio_model(input_audio, attention_mask_audio) # [B, T, H]\n",
        "        Ef = self.gated_fusion(output_text, output_audio)  # [B, T, hidden_dim]\n",
        "        features = self.mfe(Ef)  # list of 6 feature maps with shape [B, T_i, feature_size]\n",
        "        fused_output = self.afm(features)  # [B, T, feature_size]\n",
        "        fused_pooled = fused_output.mean(dim=1)\n",
        "        logits = self.classifier(fused_pooled) # [B, 2]\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NDvro39Gflwp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "text_config = Config(hidden_size=768, intermediate_size=3072, num_labels=2)\n",
        "audio_config = Config(hidden_size=1024, intermediate_size=3072, num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0RTvuCSheXi"
      },
      "outputs": [],
      "source": [
        "model = MFFNet(text_config, audio_config, hidden_dim=1024, feature_size=256, num_bifpn_layers=3, num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXQ1irusOXOK",
        "outputId": "399e73eb-b65e-4839-8086-32d6ad789752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 68,427,152\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZIpEgZMhnFb"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()   # For binary classification, this expects labels as integers (0 or 1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRNrpvcRhrkf"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200\n",
        "# patience = 5\n",
        "# best_val_loss = float('inf')\n",
        "# no_improve_count = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "no_improve_count = 0\n",
        "patience = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCIAX4yEhucb",
        "outputId": "347f078f-714b-46d3-99d5-758ae752baad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Train Loss: 0.7068, Train Acc: 0.45\n",
            "Epoch [1/200], Val Loss: 0.6890, Val Acc: 0.71\n",
            "Epoch [2/200], Train Loss: 0.5929, Train Acc: 0.72\n",
            "Epoch [2/200], Val Loss: 0.6705, Val Acc: 0.71\n",
            "Epoch [3/200], Train Loss: 0.5555, Train Acc: 0.72\n",
            "Epoch [3/200], Val Loss: 0.6518, Val Acc: 0.71\n",
            "Epoch [4/200], Train Loss: 0.5314, Train Acc: 0.72\n",
            "Epoch [4/200], Val Loss: 0.6385, Val Acc: 0.71\n",
            "Epoch [5/200], Train Loss: 0.4944, Train Acc: 0.72\n",
            "Epoch [5/200], Val Loss: 0.6288, Val Acc: 0.71\n",
            "Epoch [6/200], Train Loss: 0.4507, Train Acc: 0.73\n",
            "Epoch [6/200], Val Loss: 0.6264, Val Acc: 0.71\n",
            "Epoch [7/200], Train Loss: 0.3942, Train Acc: 0.80\n",
            "Epoch [7/200], Val Loss: 0.6286, Val Acc: 0.71\n",
            "Epoch [8/200], Train Loss: 0.3187, Train Acc: 0.94\n",
            "Epoch [8/200], Val Loss: 0.6230, Val Acc: 0.71\n",
            "Epoch [9/200], Train Loss: 0.2606, Train Acc: 0.96\n",
            "Epoch [9/200], Val Loss: 0.6202, Val Acc: 0.71\n",
            "Epoch [10/200], Train Loss: 0.2313, Train Acc: 0.96\n",
            "Epoch [10/200], Val Loss: 0.6270, Val Acc: 0.71\n",
            "Epoch [11/200], Train Loss: 0.1480, Train Acc: 0.99\n",
            "Epoch [11/200], Val Loss: 0.6294, Val Acc: 0.71\n",
            "Epoch [12/200], Train Loss: 0.1403, Train Acc: 0.99\n",
            "Epoch [12/200], Val Loss: 0.6327, Val Acc: 0.67\n",
            "Epoch [13/200], Train Loss: 0.1079, Train Acc: 1.00\n",
            "Epoch [13/200], Val Loss: 1.0079, Val Acc: 0.33\n",
            "Epoch [14/200], Train Loss: 0.1106, Train Acc: 0.98\n",
            "Epoch [14/200], Val Loss: 0.6770, Val Acc: 0.67\n",
            "Epoch [15/200], Train Loss: 0.0497, Train Acc: 1.00\n",
            "Epoch [15/200], Val Loss: 0.9865, Val Acc: 0.71\n",
            "Epoch [16/200], Train Loss: 0.0799, Train Acc: 1.00\n",
            "Epoch [16/200], Val Loss: 1.0856, Val Acc: 0.76\n",
            "Epoch [17/200], Train Loss: 0.0632, Train Acc: 1.00\n",
            "Epoch [17/200], Val Loss: 0.8536, Val Acc: 0.76\n",
            "Epoch [18/200], Train Loss: 0.0317, Train Acc: 1.00\n",
            "Epoch [18/200], Val Loss: 0.7964, Val Acc: 0.71\n",
            "Epoch [19/200], Train Loss: 0.0292, Train Acc: 1.00\n",
            "Epoch [19/200], Val Loss: 0.8621, Val Acc: 0.62\n",
            "Epoch [20/200], Train Loss: 0.0223, Train Acc: 1.00\n",
            "Epoch [20/200], Val Loss: 1.0421, Val Acc: 0.71\n",
            "Epoch [21/200], Train Loss: 0.0164, Train Acc: 1.00\n",
            "Epoch [21/200], Val Loss: 1.1572, Val Acc: 0.71\n",
            "Epoch [22/200], Train Loss: 0.0220, Train Acc: 1.00\n",
            "Epoch [22/200], Val Loss: 1.1564, Val Acc: 0.71\n",
            "Epoch [23/200], Train Loss: 0.0092, Train Acc: 1.00\n",
            "Epoch [23/200], Val Loss: 1.1263, Val Acc: 0.71\n",
            "Epoch [24/200], Train Loss: 0.0099, Train Acc: 1.00\n",
            "Epoch [24/200], Val Loss: 1.0813, Val Acc: 0.71\n",
            "Epoch [25/200], Train Loss: 0.0079, Train Acc: 1.00\n",
            "Epoch [25/200], Val Loss: 1.0346, Val Acc: 0.71\n",
            "Epoch [26/200], Train Loss: 0.0067, Train Acc: 1.00\n",
            "Epoch [26/200], Val Loss: 0.9803, Val Acc: 0.71\n",
            "Epoch [27/200], Train Loss: 0.0065, Train Acc: 1.00\n",
            "Epoch [27/200], Val Loss: 0.9262, Val Acc: 0.71\n",
            "Epoch [28/200], Train Loss: 0.0061, Train Acc: 1.00\n",
            "Epoch [28/200], Val Loss: 0.9235, Val Acc: 0.71\n",
            "Epoch [29/200], Train Loss: 0.0043, Train Acc: 1.00\n",
            "Epoch [29/200], Val Loss: 0.9405, Val Acc: 0.71\n",
            "No improvement in validation loss for several epochs. Stopping early.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for text_batch, audio_batch, text_mask, audio_mask, label_batch in train_loader:\n",
        "        text_batch = text_batch.to(device)\n",
        "        audio_batch = audio_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(text_batch, audio_batch)\n",
        "        loss = criterion(logits, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * text_batch.size(0)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += label_batch.size(0)\n",
        "        correct += (predicted == label_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = total_loss / total\n",
        "    train_accuracy = correct / total\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text_val, audio_val, text_mask, audio_mask, label_val in val_loader:\n",
        "            text_val = text_val.to(device)\n",
        "            audio_val = audio_val.to(device)\n",
        "            label_val = label_val.to(device)\n",
        "\n",
        "            val_logits = model(text_val, audio_val)\n",
        "            val_batch_loss = criterion(val_logits, label_val)\n",
        "            val_loss += val_batch_loss.item() * text_val.size(0)\n",
        "\n",
        "            _, val_predicted = torch.max(val_logits, 1)\n",
        "            val_total += label_val.size(0)\n",
        "            val_correct += (val_predicted == label_val).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / val_total\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improve_count = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "    else:\n",
        "        no_improve_count += 1\n",
        "        if no_improve_count >= patience:\n",
        "            print(\"No improvement in validation loss for several epochs. Stopping early.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "mJoN7b9-boeA",
        "outputId": "32b37d1c-a32c-4c24-a63a-696660cec268"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Curve - Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve - Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAxomJjXMmih",
        "outputId": "21577a84-f272-455a-e287-1d47009d03bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-a95518f0c72b>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MFFNet(\n",
              "  (text_model): MSFastformer(\n",
              "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "    (conv3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (conv5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (fastformer): FastformerLayer(\n",
              "      (attention): FastAttention(\n",
              "        (self): FastSelfAttention(\n",
              "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (query_att): Linear(in_features=1024, out_features=16, bias=True)\n",
              "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (key_att): Linear(in_features=1024, out_features=16, bias=True)\n",
              "          (transform): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "        )\n",
              "        (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (intermediate): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "      (output): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "      (activation): GELU(approximate='none')\n",
              "      (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (fc1): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "    (fc2): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "    (output_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (gelu): GELU(approximate='none')\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (audio_model): MSFastformer(\n",
              "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (conv1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "    (conv3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (conv5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (fastformer): FastformerLayer(\n",
              "      (attention): FastAttention(\n",
              "        (self): FastSelfAttention(\n",
              "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (query_att): Linear(in_features=1024, out_features=16, bias=True)\n",
              "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (key_att): Linear(in_features=1024, out_features=16, bias=True)\n",
              "          (transform): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (softmax): Softmax(dim=-1)\n",
              "        )\n",
              "        (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (intermediate): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "      (output): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "      (activation): GELU(approximate='none')\n",
              "      (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (fc1): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "    (fc2): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "    (output_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (gelu): GELU(approximate='none')\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (gated_fusion): GatedFusion(\n",
              "    (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (sigmoid): Sigmoid()\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (mfe): MultimodalFeatureEnhancer(\n",
              "    (conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
              "    (conv_p1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "    (conv_p2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "    (bifpn): BiFPN(\n",
              "      (bifpn_layers): ModuleList(\n",
              "        (0-2): 3 x BiFPNLayer(\n",
              "          (conv_p2_td): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "          (conv_p1_td): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "          (conv_p0_td): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "          (conv_p0_out): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "          (conv_p1_out): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "          (conv_p2_out): SeparableConvBlock(\n",
              "            (depthwise_conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "            (pointwise_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (activation): ReLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (afm): AdaptiveFusionModule(\n",
              "    (feature_alignment): FeatureAlignment(\n",
              "      (align_convs): ModuleList(\n",
              "        (0-5): 6 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (feature_fusion): MultiFeatureFusion(\n",
              "      (attention_fc): Linear(in_features=12, out_features=6, bias=True)\n",
              "      (relu): ReLU()\n",
              "      (softmax): Softmax(dim=1)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkIIVTuaNgMB"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text_batch, audio_batch, text_mask, audio_mask, label_batch in test_loader:\n",
        "        # Move to GPU/CPU device\n",
        "        text_batch = text_batch.to(device)\n",
        "        audio_batch = audio_batch.to(device)\n",
        "        text_mask = text_mask.to(device)\n",
        "        audio_mask = audio_mask.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(text_batch, audio_batch, text_mask, audio_mask)\n",
        "\n",
        "        # Predicted labels\n",
        "        _, predicted = torch.max(logits, dim=1)\n",
        "\n",
        "        # Collect predictions and labels on CPU\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(label_batch.cpu().numpy())\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YaFDDP5Nlbu",
        "outputId": "518b243f-bf8b-4c51-82fe-65d8b316cad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.71      1.00      0.83        15\n",
            "     Class 1       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.71        21\n",
            "   macro avg       0.36      0.50      0.42        21\n",
            "weighted avg       0.51      0.71      0.60        21\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zuM0stOUXTJb",
        "BQpo28UpWlKF",
        "YGPpF36nIEXB"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
