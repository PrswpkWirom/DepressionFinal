{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1m8cYU_xyzZkpHpFCCP3hS9brHau-pg0F",
      "authorship_tag": "ABX9TyOSYCVpYDypIjb+hl3Yr61K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrswpkWirom/Depression-Detection-FinalProject/blob/main/DD_vanilla_MFFNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader, mini batch"
      ],
      "metadata": {
        "id": "ZyGzAgiZR8t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import files\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "3WXV52DVSAz_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file_path = '/content/drive/MyDrive/DD_input_text_tensor_cpu.pkl'\n",
        "input_text_tensor = torch.load(pickle_file_path)\n",
        "print(input_text_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSWiISZ9SbF9",
        "outputId": "ee93cf15-238d-42f4-8ac8-5264ecff39de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1ce23ae293e7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_text_tensor = torch.load(pickle_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0584,  0.0803, -0.0016,  ..., -0.0751, -0.0839, -0.0127],\n",
            "         [-0.0734,  0.0904,  0.0002,  ..., -0.0699, -0.0782, -0.0195],\n",
            "         [-0.0787,  0.0642, -0.0052,  ..., -0.0863, -0.0811, -0.0373],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0524,  0.0920, -0.0036,  ..., -0.0810, -0.0577, -0.0343],\n",
            "         [-0.0517,  0.0909, -0.0059,  ..., -0.0802, -0.0536, -0.0325],\n",
            "         [-0.0736,  0.0564,  0.0038,  ..., -0.1028, -0.0545, -0.0117],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0596,  0.1051,  0.0067,  ..., -0.0871, -0.0634, -0.0286],\n",
            "         [-0.0597,  0.0772, -0.0034,  ..., -0.0932, -0.0878, -0.0135],\n",
            "         [-0.0702,  0.0533, -0.0156,  ..., -0.1149, -0.0809, -0.0233],\n",
            "         ...,\n",
            "         [-0.0460,  0.0664, -0.0231,  ..., -0.1232, -0.0775, -0.0196],\n",
            "         [-0.0610,  0.0796, -0.0111,  ..., -0.0789, -0.0850, -0.0119],\n",
            "         [-0.0580,  0.0784, -0.0025,  ..., -0.0796, -0.0856, -0.0070]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0517,  0.0909, -0.0059,  ..., -0.0802, -0.0536, -0.0325],\n",
            "         [-0.0698,  0.0731, -0.0061,  ..., -0.0762, -0.0706, -0.0378],\n",
            "         [-0.0463,  0.0804,  0.0125,  ..., -0.0582, -0.0430, -0.0036],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0493,  0.1006,  0.0038,  ..., -0.0843, -0.0667, -0.0391],\n",
            "         [-0.0596,  0.1051,  0.0067,  ..., -0.0871, -0.0634, -0.0286],\n",
            "         [-0.0409,  0.0749,  0.0080,  ..., -0.1013, -0.0521, -0.0279],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0701,  0.0617, -0.0055,  ..., -0.0795, -0.0607, -0.0137],\n",
            "         [-0.0715,  0.0823, -0.0005,  ..., -0.0860, -0.0770, -0.0524],\n",
            "         [-0.0524,  0.0909, -0.0215,  ..., -0.0936, -0.0636, -0.0368],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_text_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mim0HpOFAfKI",
        "outputId": "eeb7e728-642c-48f1-b878-d1bdba689277"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file_path = '/content/drive/MyDrive/DD_input_audio_tensor_cpu.pkl'\n",
        "input_audio_tensor = torch.load(pickle_file_path)\n",
        "print(input_audio_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-0KZhIAlzo_",
        "outputId": "892dc8be-404f-4816-baaf-733a27b59307"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a07edf880e79>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_audio_tensor = torch.load(pickle_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.9665e-02,  1.0969e-02, -1.3585e-01,  ...,  1.5336e-01,\n",
            "          -6.4800e-02, -1.2271e-01],\n",
            "         [-1.2926e-01,  6.6667e-02,  5.7625e-02,  ..., -9.6505e-02,\n",
            "          -3.8745e-02, -6.6007e-02],\n",
            "         [-2.2797e-01,  2.4531e-02, -3.4593e-01,  ...,  2.6354e-02,\n",
            "          -5.2208e-03,  7.0064e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-3.6057e-02, -4.6827e-01, -3.0396e-01,  ..., -1.4735e-03,\n",
            "          -2.0177e-01,  2.6604e-01],\n",
            "         [ 2.9317e-02, -9.1001e-02, -2.2257e-01,  ...,  2.9565e-01,\n",
            "           2.8292e-01,  4.7135e-02],\n",
            "         [-4.0544e-02, -1.4174e-01, -2.1277e-01,  ...,  1.6574e-01,\n",
            "           7.5368e-02, -1.2303e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 1.3164e-01,  1.8790e-01, -7.1662e-02,  ...,  7.1214e-01,\n",
            "          -3.8415e-02,  1.3904e-01],\n",
            "         [ 2.6844e-02, -1.7935e-01, -2.0488e-01,  ...,  4.2304e-01,\n",
            "           1.1810e-01,  1.1619e-01],\n",
            "         [-4.8594e-03,  2.1199e-02, -1.2241e-01,  ...,  1.3504e-01,\n",
            "          -9.2803e-02,  2.8375e-03],\n",
            "         ...,\n",
            "         [-5.1537e-02, -6.2501e-02, -1.0511e-01,  ...,  2.3004e-01,\n",
            "           2.8352e-02,  9.5858e-03],\n",
            "         [-5.1166e-02, -6.3456e-02, -1.2669e-01,  ...,  6.5678e-01,\n",
            "           5.5907e-02,  7.6772e-02],\n",
            "         [-5.4123e-02, -2.7001e-02, -1.7002e-01,  ...,  7.7147e-01,\n",
            "           5.5882e-02, -5.3254e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1145e-01, -2.3549e-01, -5.7890e-01,  ...,  4.7636e-01,\n",
            "           2.0650e-01, -1.8260e-02],\n",
            "         [-8.8131e-02, -8.3028e-02, -2.4597e-01,  ...,  8.0168e-02,\n",
            "           2.9653e-04,  1.1521e-01],\n",
            "         [ 5.4863e-02,  2.8735e-02, -2.4022e-02,  ...,  1.2826e-01,\n",
            "           1.4760e-01,  2.4071e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-4.6817e-02,  8.8982e-02, -2.3981e-01,  ...,  3.3690e-02,\n",
            "           1.5145e-01,  3.9254e-01],\n",
            "         [ 1.1978e-04,  2.8045e-01, -1.1938e-01,  ...,  6.9086e-01,\n",
            "          -2.0461e-02, -7.2930e-02],\n",
            "         [-1.2003e-01,  4.4289e-02, -1.9590e-01,  ...,  4.6255e-01,\n",
            "          -1.2342e-02, -1.5045e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-1.3376e-01, -1.1681e-01,  1.4575e-02,  ...,  1.4943e-01,\n",
            "          -5.0159e-02,  4.3392e-02],\n",
            "         [-8.3008e-02,  3.1844e-02, -1.9864e-01,  ...,  2.1912e-01,\n",
            "          -8.1102e-03,  7.1779e-02],\n",
            "         [-4.5985e-02, -6.8465e-02, -3.4092e-01,  ..., -3.0347e-01,\n",
            "           1.4356e-01, -8.8124e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_audio_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGYQD0hql9EO",
        "outputId": "c40b254d-e03d-4b98-9e63-40c3320933df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fastformer Implementation"
      ],
      "metadata": {
        "id": "24ZJahr2XrXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This fastformer module is obtained from github: https://github.com/wuch15/Fastformer"
      ],
      "metadata": {
        "id": "1iNQWS19Xw1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "from transformers.models.bert.modeling_bert import BertSelfOutput, BertIntermediate, BertOutput\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.att_fc2 = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        bz = x.shape[0]\n",
        "        e = self.att_fc1(x)\n",
        "        e = nn.Tanh()(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "        alpha = torch.exp(alpha)\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "        x = torch.reshape(x, (bz, -1))\n",
        "        return x\n",
        "\n",
        "class FastSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastSelfAttention, self).__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" %\n",
        "                (config.hidden_size, config.num_attention_heads))\n",
        "        self.attention_head_size = int(config.hidden_size /config.num_attention_heads)\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.input_dim= config.hidden_size\n",
        "\n",
        "        self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                       self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        # batch_size, seq_len, num_head * head_dim, batch_size, seq_len\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        # batch_size, num_head, seq_len\n",
        "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5\n",
        "        # add attention mask\n",
        "        query_for_score += attention_mask\n",
        "\n",
        "        # batch_size, num_head, 1, seq_len\n",
        "        query_weight = self.softmax(query_for_score).unsqueeze(2)\n",
        "\n",
        "        # batch_size, num_head, seq_len, head_dim\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # batch_size, num_head, head_dim, 1\n",
        "        pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).view(-1,1,self.num_attention_heads*self.attention_head_size)\n",
        "        pooled_query_repeat= pooled_query.repeat(1, seq_len,1)\n",
        "        # batch_size, num_head, seq_len, head_dim\n",
        "\n",
        "        # batch_size, num_head, seq_len\n",
        "        mixed_query_key_layer=mixed_key_layer* pooled_query_repeat\n",
        "\n",
        "        query_key_score=(self.key_att(mixed_query_key_layer)/ self.attention_head_size**0.5).transpose(1, 2)\n",
        "\n",
        "        # add attention mask\n",
        "        query_key_score +=attention_mask\n",
        "\n",
        "        # batch_size, num_head, 1, seq_len\n",
        "        query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
        "        pooled_key = torch.matmul(query_key_weight, key_layer)\n",
        "\n",
        "        #query = value\n",
        "        weighted_value =(pooled_key * query_layer).transpose(1, 2)\n",
        "        weighted_value = weighted_value.reshape(\n",
        "            weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
        "        weighted_value = self.transform(weighted_value) + mixed_query_layer\n",
        "\n",
        "        return weighted_value\n",
        "\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastAttention, self).__init__()\n",
        "        self.self = FastSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "class FastformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastformerLayer, self).__init__()\n",
        "        self.attention = FastAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "class FastformerEncoder(nn.Module):\n",
        "    def __init__(self, config, pooler_count=1):\n",
        "        super(FastformerEncoder, self).__init__()\n",
        "        self.config = config\n",
        "        self.encoders = nn.ModuleList([FastformerLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # support multiple different poolers with shared bert encoder.\n",
        "        self.poolers = nn.ModuleList()\n",
        "        if config.pooler_type == 'weightpooler':\n",
        "            for _ in range(pooler_count):\n",
        "                self.poolers.append(AttentionPooling(config))\n",
        "        logging.info(f\"This model has {len(self.poolers)} poolers.\")\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Embedding)) and module.padding_idx is not None:\n",
        "                with torch.no_grad():\n",
        "                    module.weight[module.padding_idx].fill_(0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self,\n",
        "                input_embs,\n",
        "                attention_mask,\n",
        "                pooler_index=0):\n",
        "        #input_embs: batch_size, seq_len, emb_dim\n",
        "        #attention_mask: batch_size, seq_len, emb_dim\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        batch_size, seq_length, emb_dim = input_embs.shape\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_embs.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = input_embs + position_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        #print(embeddings.size())\n",
        "        all_hidden_states = [embeddings]\n",
        "\n",
        "        for i, layer_module in enumerate(self.encoders):\n",
        "            layer_outputs = layer_module(all_hidden_states[-1], extended_attention_mask)\n",
        "            all_hidden_states.append(layer_outputs)\n",
        "        assert len(self.poolers) > pooler_index\n",
        "        output = self.poolers[pooler_index](all_hidden_states[-1], attention_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.dense_linear = nn.Linear(config.hidden_size,4)\n",
        "        self.word_embedding = nn.Embedding(len(word_dict),256,padding_idx=0)\n",
        "        self.fastformer_model = FastformerEncoder(config)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Embedding)) and module.padding_idx is not None:\n",
        "                with torch.no_grad():\n",
        "                    module.weight[module.padding_idx].fill_(0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self,input_ids,targets):\n",
        "        mask=input_ids.bool().float()\n",
        "        embds=self.word_embedding(input_ids)\n",
        "        text_vec = self.fastformer_model(embds,mask)\n",
        "        score = self.dense_linear(text_vec)\n",
        "        loss = self.criterion(score, targets)\n",
        "        return loss, score\n",
        "\n",
        "def acc(y_true, y_hat):\n",
        "    y_hat = torch.argmax(y_hat, dim=-1)\n",
        "    tot = y_true.shape[0]\n",
        "    hit = torch.sum(y_true == y_hat)\n",
        "    return hit.data.float() * 1.0 / tot"
      ],
      "metadata": {
        "id": "BFV0rTrlXvrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla MSFastformer: Attempt 1"
      ],
      "metadata": {
        "id": "X7olyFrUOUR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class AttentionPooling(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(AttentionPooling, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.att_fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "#         self.att_fc2 = nn.Linear(config.hidden_size, 1)\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x, attn_mask=None):\n",
        "#         bz = x.shape[0]\n",
        "#         e = self.att_fc1(x)\n",
        "#         e = nn.Tanh()(e)\n",
        "#         alpha = self.att_fc2(e)\n",
        "#         alpha = torch.exp(alpha)\n",
        "#         if attn_mask is not None:\n",
        "#             alpha = alpha * attn_mask.unsqueeze(2)\n",
        "#         alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "#         x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "#         x = x.reshape(bz, -1)\n",
        "#         return x\n",
        "\n",
        "# class FastSelfAttention(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastSelfAttention, self).__init__()\n",
        "#         self.config = config\n",
        "#         if config.hidden_size % config.num_attention_heads != 0:\n",
        "#             raise ValueError(\n",
        "#                 \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "#                 \"heads (%d)\" %\n",
        "#                 (config.hidden_size, config.num_attention_heads))\n",
        "#         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "#         self.num_attention_heads = config.num_attention_heads\n",
        "#         self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "#         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "#         self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "#         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "#         self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "#         self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "#         self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "\n",
        "#     def transpose_for_scores(self, x):\n",
        "#         new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "#                                        self.attention_head_size)\n",
        "#         x = x.view(*new_x_shape)\n",
        "#         return x.permute(0, 2, 1, 3)\n",
        "\n",
        "#     def forward(self, q_input, k_input, attention_mask=None):\n",
        "#         batch_size, seq_len, _ = q_input.shape\n",
        "#         # Project queries and keys\n",
        "#         mixed_query_layer = self.query(q_input)\n",
        "#         mixed_key_layer = self.key(k_input)\n",
        "\n",
        "#         # Compute query scores\n",
        "#         query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5\n",
        "#         if attention_mask is not None:\n",
        "#             query_for_score = query_for_score + attention_mask\n",
        "\n",
        "#         query_weight = self.softmax(query_for_score).unsqueeze(2)\n",
        "#         query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "#         pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).reshape(\n",
        "#             batch_size, 1, self.all_head_size)\n",
        "#         pooled_query_repeat = pooled_query.repeat(1, seq_len, 1)\n",
        "\n",
        "#         # Modulate keys with pooled queries\n",
        "#         mixed_query_key_layer = mixed_key_layer * pooled_query_repeat\n",
        "\n",
        "#         # Compute key scores\n",
        "#         query_key_score = (self.key_att(mixed_query_key_layer) / self.attention_head_size**0.5).transpose(1, 2)\n",
        "#         if attention_mask is not None:\n",
        "#             query_key_score = query_key_score + attention_mask\n",
        "\n",
        "#         query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
        "#         key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
        "#         pooled_key = torch.matmul(query_key_weight, key_layer)\n",
        "\n",
        "#         # Compute final attention output\n",
        "#         weighted_value = (pooled_key * query_layer).transpose(1, 2)\n",
        "#         weighted_value = weighted_value.reshape(\n",
        "#             weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
        "#         attention_output = self.transform(weighted_value) + mixed_query_layer\n",
        "\n",
        "#         return attention_output\n",
        "\n",
        "# class FastAttention(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastAttention, self).__init__()\n",
        "#         self.self = FastSelfAttention(config)\n",
        "#         self.output = nn.Identity()  # Replace BertSelfOutput with Identity for simplicity\n",
        "\n",
        "#     def forward(self, input_tensor, attention_mask):\n",
        "#         self_output = self.self(input_tensor, attention_mask)\n",
        "#         attention_output = self.output(self_output)  # Assuming residual connection is handled elsewhere\n",
        "#         return attention_output\n",
        "\n",
        "# class FastformerLayer(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastformerLayer, self).__init__()\n",
        "#         self.attention = FastAttention(config)\n",
        "#         self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "#         self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "#         self.LayerNorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.LayerNorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#         self.activation = nn.GELU()\n",
        "\n",
        "#     def forward(self, hidden_states, attention_mask):\n",
        "#         attention_output = self.attention(hidden_states, attention_mask)\n",
        "#         attention_output = self.dropout(attention_output)\n",
        "#         attention_output = self.LayerNorm1(hidden_states + attention_output)\n",
        "\n",
        "#         intermediate_output = self.intermediate(attention_output)\n",
        "#         intermediate_output = self.activation(intermediate_output)\n",
        "\n",
        "#         layer_output = self.output(intermediate_output)\n",
        "#         layer_output = self.dropout(layer_output)\n",
        "#         layer_output = self.LayerNorm2(attention_output + layer_output)\n",
        "\n",
        "#         return layer_output\n",
        "\n",
        "# class FastformerEncoder(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastformerEncoder, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.encoders = nn.ModuleList([FastformerLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "#         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "#         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if isinstance(module, nn.Embedding) and module.padding_idx is not None:\n",
        "#                 with torch.no_grad():\n",
        "#                     module.weight[module.padding_idx].fill_(0)\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "#         if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "#             module.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, input_embs, attention_mask):\n",
        "#         batch_size, seq_length, emb_dim = input_embs.shape\n",
        "#         position_ids = torch.arange(seq_length, dtype=torch.long, device=input_embs.device)\n",
        "#         position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "#         position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "#         embeddings = input_embs + position_embeddings\n",
        "#         embeddings = self.LayerNorm(embeddings)\n",
        "#         embeddings = self.dropout(embeddings)\n",
        "\n",
        "#         extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "#         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "#         hidden_states = embeddings\n",
        "#         for layer_module in self.encoders:\n",
        "#             hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
        "\n",
        "#         return hidden_states\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f4QNrjKr92no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomModel(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(CustomModel, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.fastformer = FastSelfAttention(config)\n",
        "#         self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "#         self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "#         self.output_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
        "#         self.gelu = nn.GELU()\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, (nn.Linear)):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "\n",
        "#     def compute_covariance(self, x):\n",
        "#         # x: [batch_size, seq_len, hidden_size]\n",
        "#         batch_size, seq_len, hidden_size = x.size()\n",
        "#         x_centered = x - x.mean(dim=1, keepdim=True)\n",
        "#         covariance = torch.bmm(x_centered.transpose(1, 2), x_centered) / (seq_len - 1 + 1e-5)\n",
        "#         # Reshape to [batch_size, 1, hidden_size]\n",
        "#         covariance = covariance.mean(dim=2, keepdim=True).permute(0, 2, 1)\n",
        "#         return covariance  # [batch_size, 1, hidden_size]\n",
        "\n",
        "#     def forward(self, x, attention_mask):\n",
        "#         # x: [batch_size, seq_len, embedding_dim]\n",
        "#         batch_size, seq_len, _ = x.size()\n",
        "\n",
        "#         # Normalize x\n",
        "#         x_norm = self.layernorm(x)\n",
        "\n",
        "#         # Compute U_i for i = 1, 3, 5\n",
        "#         U_indices = [1, 3, 5]\n",
        "#         U_list = []\n",
        "#         for i in U_indices:\n",
        "#             if i <= seq_len:\n",
        "#                 U_i = self.compute_covariance(x_norm[:, :i, :])\n",
        "#             else:\n",
        "#                 U_i = self.compute_covariance(x_norm)\n",
        "#             U_list.append(U_i)\n",
        "\n",
        "#         U1, U3, U5 = U_list\n",
        "\n",
        "#         # Prepare attention mask\n",
        "#         extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(dtype=x.dtype)\n",
        "#         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "#         # Compute P\n",
        "#         P1 = self.fastformer(U5, U3, extended_attention_mask)\n",
        "#         P2 = self.fastformer(U3, U1, extended_attention_mask)\n",
        "#         P3 = self.fastformer(U1, U5, extended_attention_mask)\n",
        "#         P = P1 + P2 + P3\n",
        "#         P_norm = self.layernorm(P)\n",
        "\n",
        "#         # Compute I = FC(GELU(FC(Norm(P))))\n",
        "#         I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "\n",
        "#         # Compute J = I + X\n",
        "#         seq_len_I = I.size(1)\n",
        "#         J = I + x[:, :seq_len_I, :]\n",
        "\n",
        "#         # Final output y = FC(Norm(J))\n",
        "#         J_norm = self.layernorm(J)\n",
        "#         y = self.output_fc(J_norm)\n",
        "#         y = y.mean(dim=1)  # Aggregate over sequence length\n",
        "\n",
        "#         return y\n",
        "\n",
        "# # Example usage:\n",
        "# # Assuming you have input embeddings x of shape [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "# batch_size = 10\n",
        "# sequence_length = 169  # Adjust as needed\n",
        "# embedding_dim = 768\n",
        "\n",
        "# # Create random input embeddings\n",
        "# x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# # Create attention mask\n",
        "# attention_mask = torch.ones(batch_size, sequence_length)\n",
        "\n",
        "# # Instantiate the configuration\n",
        "# class Config:\n",
        "#     def __init__(self):\n",
        "#         self.hidden_size = 768\n",
        "#         self.num_attention_heads = 12\n",
        "#         self.num_hidden_layers = 12\n",
        "#         self.intermediate_size = 3072\n",
        "#         self.hidden_dropout_prob = 0.1\n",
        "#         self.layer_norm_eps = 1e-12\n",
        "#         self.initializer_range = 0.02\n",
        "#         self.num_labels = 4\n",
        "#         self.max_position_embeddings = 512\n",
        "#         self.pooler_type = 'weightpooler'\n",
        "\n",
        "# config = Config()"
      ],
      "metadata": {
        "id": "izMmnkXBACEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 10\n",
        "# sequence_length = 1  # Adjust as needed\n",
        "# embedding_dim = config.hidden_size\n",
        "\n",
        "# x = input_text_tensor\n",
        "\n",
        "# # Create attention mask\n",
        "# attention_mask = torch.ones(batch_size, sequence_length)\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = CustomModel(config)\n",
        "\n",
        "# # Forward pass\n",
        "# output = model(x, attention_mask)\n",
        "\n",
        "# print(\"Output shape:\", output.shape)  # Should be [batch_size, num_labels]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "BcyKUWuuBu3w",
        "outputId": "bce44e6a-7882-4999-aefb-9e1ac0548b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[10, 1, 768]' is invalid for input of size 921600",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-343add14cc6c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should be [batch_size, num_labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e51ecba7da9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compute P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mP2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mP3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1a6f3be939dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_input, k_input, attention_mask)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).reshape(\n\u001b[0m\u001b[1;32m     77\u001b[0m             batch_size, 1, self.all_head_size)\n\u001b[1;32m     78\u001b[0m         \u001b[0mpooled_query_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 1, 768]' is invalid for input of size 921600"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla MSFastformer: Attempt 2 (completed)"
      ],
      "metadata": {
        "id": "yt9wkZg-Ojig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastSelfAttention, self).__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" %\n",
        "                (config.hidden_size, config.num_attention_heads))\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.input_dim = config.hidden_size\n",
        "\n",
        "        self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                       self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        # hidden_states: [batch_size, seq_len_hidden, hidden_size]\n",
        "        # context_states: [batch_size, seq_len_context, hidden_size]\n",
        "        batch_size, seq_len_hidden, _ = hidden_states.size()\n",
        "        seq_len_context = context_states.size(1)\n",
        "\n",
        "        # Compute queries and keys\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(context_states)\n",
        "\n",
        "        # Compute attention scores\n",
        "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "        key_for_score = self.key_att(mixed_key_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(query_for_score.transpose(1, 2), key_for_score)  # [batch_size, seq_len_hidden, seq_len_context]\n",
        "\n",
        "        # If attention_mask is provided, add it\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask should be broadcastable to [batch_size, seq_len_hidden, seq_len_context]\n",
        "            attention_scores += attention_mask\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_layer = torch.matmul(attention_weights, mixed_key_layer)  # [batch_size, seq_len_hidden, hidden_size]\n",
        "\n",
        "        # Combine with value projection\n",
        "        output = self.transform(context_layer) + mixed_query_layer\n",
        "\n",
        "        return output\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastAttention, self).__init__()\n",
        "        self.self = FastSelfAttention(config)\n",
        "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, input_tensor, context_tensor, attention_mask=None):\n",
        "        self_output = self.self(input_tensor, context_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output) + input_tensor\n",
        "        return attention_output\n",
        "\n",
        "class FastformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastformerLayer, self).__init__()\n",
        "        self.attention = FastAttention(config)\n",
        "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.activation = nn.GELU()\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        attention_output = self.attention(hidden_states, context_states, attention_mask)\n",
        "        attention_output = self.layernorm(attention_output)\n",
        "        intermediate_output = self.activation(self.intermediate(attention_output))\n",
        "        layer_output = self.output(intermediate_output) + attention_output\n",
        "        layer_output = self.layernorm(layer_output)\n",
        "        return layer_output"
      ],
      "metadata": {
        "id": "aV8rvRAFOqaf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MSFastformer_notused(nn.Module):\n",
        "#   #is not used\n",
        "#     def __init__(self, config):\n",
        "#         super(MSFastformer, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.fastformer = FastformerLayer(config)\n",
        "#         self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "#         self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "#         self.output_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
        "#         self.gelu = nn.GELU()\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, (nn.Linear)):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "\n",
        "#     def compute_covariance(self, x):\n",
        "#         # x: [batch_size, seq_len, hidden_size]\n",
        "#         batch_size, seq_len, hidden_size = x.size()\n",
        "#         x_centered = x - x.mean(dim=1, keepdim=True)\n",
        "#         covariance = torch.zeros_like(x)\n",
        "#         for i in range(seq_len):\n",
        "#           xi = x_centered[:, i, :].unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
        "#           covariance[:, i, :] = torch.bmm(xi.transpose(1, 2), xi).squeeze(1)  # [batch_size, hidden_size]\n",
        "#         return covariance  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "\n",
        "#     def forward(self, x, attention_mask):\n",
        "#         # dimension of x is [batch_size, seq_len, embedding_dim]\n",
        "#         batch_size, seq_len, _ = x.size()\n",
        "\n",
        "#         # Normalize x\n",
        "#         x_norm = self.layernorm(x)\n",
        "\n",
        "#         #U_i for i = 1, 3, 5\n",
        "#         U_indices = [1, 3, 5]\n",
        "#         U_list = []\n",
        "#         for i in U_indices:\n",
        "#             if i <= seq_len:\n",
        "#                 U_i = self.compute_covariance(x_norm[:, :i, :])\n",
        "#             else:\n",
        "#                 U_i = self.compute_covariance(x_norm)\n",
        "#             U_list.append(U_i)\n",
        "\n",
        "#         U1, U3, U5 = U_list\n",
        "\n",
        "#         #P\n",
        "#         P1 = self.fastformer(U5, U3, attention_mask=None)\n",
        "#         P2 = self.fastformer(U3, U1, attention_mask=None)\n",
        "#         P3 = self.fastformer(U1, U5, attention_mask=None)\n",
        "#         P = P1 + P2 + P3\n",
        "#         P_norm = self.layernorm(P)\n",
        "\n",
        "#         #I = FC(GELU(FC(Norm(P))))\n",
        "#         I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "\n",
        "#         #J = I + x\n",
        "#         seq_len_I = I.size(1)\n",
        "#         J = I + x[:, :seq_len_I, :]\n",
        "\n",
        "#         #y = FC(Norm(J))\n",
        "#         J_norm = self.layernorm(J)\n",
        "#         y = self.output_fc(J_norm)\n",
        "#         # y = y.mean(dim=1)  # Remove this line\n",
        "\n",
        "#         return y  # y now has shape (batch_size, seq_len_I, num_labels)\n"
      ],
      "metadata": {
        "id": "MJk0m6hKYmeo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSFastformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MSFastformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        # Define convolutional layers for multi-scale feature extraction\n",
        "        self.conv1 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=1, padding=0)\n",
        "        self.conv3 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=5, padding=2)\n",
        "        self.fastformer = FastformerLayer(config)\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        #self.output_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.output_fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        # Initialization code remains the same\n",
        "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        # x: [batch_size, seq_len, hidden_size]\n",
        "        batch_size, seq_len, hidden_size = x.size()\n",
        "\n",
        "        # Transpose x to [batch_size, hidden_size, seq_len] for Conv1d\n",
        "        x_transposed = x.transpose(1, 2)  # [batch_size, hidden_size, seq_len]\n",
        "        print(x_transposed.shape)\n",
        "\n",
        "        # Apply convolutional layers to extract multi-scale features\n",
        "        U1 = self.conv1(x_transposed)  # [batch_size, hidden_size, seq_len]\n",
        "        U3 = self.conv3(x_transposed)  # [batch_size, hidden_size, seq_len]\n",
        "        U5 = self.conv5(x_transposed)  # [batch_size, hidden_size, seq_len]\n",
        "        print(f'U: {U5.shape}\\n')\n",
        "\n",
        "        # dimension [batch_size, seq_len, hidden_size]\n",
        "        U1 = U1.transpose(1, 2)\n",
        "        print(f'U1: {U1.shape}')\n",
        "        U3 = U3.transpose(1, 2)\n",
        "        print(f'U3: {U3.shape}')\n",
        "        U5 = U5.transpose(1, 2)\n",
        "        print(f'U5: {U5.shape}\\n')\n",
        "\n",
        "        # layer normalization\n",
        "        U1 = self.layernorm(U1)\n",
        "        U3 = self.layernorm(U3)\n",
        "        U5 = self.layernorm(U5)\n",
        "\n",
        "        P1 = self.fastformer(U5, U3, attention_mask=None)\n",
        "        P2 = self.fastformer(U3, U1, attention_mask=None)\n",
        "        P3 = self.fastformer(U1, U5, attention_mask=None)\n",
        "        P = P1 + P2 + P3\n",
        "        print(f'P: {P.shape}')\n",
        "        P_norm = self.layernorm(P)\n",
        "\n",
        "        # I = FC(GELU(FC(Norm(P))))\n",
        "        I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "        print(f'I: {I.shape}')\n",
        "        # J = I + x\n",
        "        J = I + x\n",
        "\n",
        "        # y = FC(Norm(J))\n",
        "        J_norm = self.layernorm(J)\n",
        "        y = self.output_fc(J_norm)\n",
        "        # y = y.mean(dim=1)\n",
        "\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "vXXv4tLeHQ43"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                 hidden_size=768,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 num_labels=2,\n",
        "                 num_hidden_layers=12,\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 layer_norm_eps=1e-12,\n",
        "                 initializer_range=0.02,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 pooler_type='weightpooler',\n",
        "                 num_attention_layers=12):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_labels = num_labels\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.initializer_range = initializer_range\n",
        "        self.hidden_act = hidden_act\n",
        "        self.pooler_type = pooler_type\n",
        "        self.num_attention_layers = num_attention_layers"
      ],
      "metadata": {
        "id": "lPtYPFJXYj5R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(hidden_size=768, intermediate_size=3072, num_labels=2)\n",
        "model = MSFastformer(config)\n",
        "\n",
        "batch_size = 10\n",
        "seq_length = 169\n",
        "hidden_dim = 768\n",
        "x = input_text_tensor\n",
        "attention_mask = torch.ones(batch_size, seq_length)\n",
        "\n",
        "output_text = model(x, attention_mask)\n",
        "\n",
        "print(output_text.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCzn1OGqRFTR",
        "outputId": "27276ba7-5ebd-4800-e44d-89896545dfce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768, 169])\n",
            "U: torch.Size([10, 768, 169])\n",
            "\n",
            "U1: torch.Size([10, 169, 768])\n",
            "U3: torch.Size([10, 169, 768])\n",
            "U5: torch.Size([10, 169, 768])\n",
            "\n",
            "P: torch.Size([10, 169, 768])\n",
            "I: torch.Size([10, 169, 768])\n",
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ujAczEvC9KM",
        "outputId": "2aec5ac8-3df5-4cff-8fef-822113733808"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-5.5241e-01,  7.2888e-02,  5.1301e-01,  ...,  1.8822e-02,\n",
            "          -1.2583e-02, -1.4850e-01],\n",
            "         [-3.4366e-01,  1.0303e-01,  2.8130e-01,  ...,  6.8154e-02,\n",
            "          -1.5241e-01, -2.1640e-01],\n",
            "         [-1.7501e-01, -8.6412e-02,  2.3417e-01,  ...,  6.9463e-02,\n",
            "          -1.9146e-01, -1.2644e-01],\n",
            "         ...,\n",
            "         [-8.0110e-01, -2.9434e-01, -8.2705e-02,  ...,  3.5220e-01,\n",
            "          -5.9444e-01, -3.7240e-01],\n",
            "         [-8.0110e-01, -2.9434e-01, -8.2705e-02,  ...,  3.5220e-01,\n",
            "          -5.9444e-01, -3.7240e-01],\n",
            "         [-8.0110e-01, -2.9434e-01, -8.2705e-02,  ...,  3.5220e-01,\n",
            "          -5.9444e-01, -3.7240e-01]],\n",
            "\n",
            "        [[-5.5229e-01,  5.5865e-02,  5.2544e-01,  ...,  6.3426e-02,\n",
            "          -2.8681e-02, -1.5805e-01],\n",
            "         [-3.3801e-01,  7.1541e-02,  3.0725e-01,  ...,  8.7184e-02,\n",
            "          -1.4270e-01, -2.1353e-01],\n",
            "         [-1.6732e-01, -1.1816e-01,  2.5417e-01,  ...,  2.9650e-02,\n",
            "          -1.6902e-01, -9.8687e-02],\n",
            "         ...,\n",
            "         [-8.0697e-01, -2.9887e-01, -6.7791e-02,  ...,  3.4500e-01,\n",
            "          -5.9440e-01, -3.7877e-01],\n",
            "         [-8.0697e-01, -2.9887e-01, -6.7791e-02,  ...,  3.4500e-01,\n",
            "          -5.9440e-01, -3.7877e-01],\n",
            "         [-8.0697e-01, -2.9887e-01, -6.7791e-02,  ...,  3.4500e-01,\n",
            "          -5.9440e-01, -3.7877e-01]],\n",
            "\n",
            "        [[-5.6223e-01,  3.3610e-02,  5.4311e-01,  ...,  3.7970e-02,\n",
            "          -3.7580e-02, -1.8746e-01],\n",
            "         [-3.3207e-01, -4.3619e-04,  2.4941e-01,  ...,  7.2858e-02,\n",
            "          -1.1882e-01, -2.1451e-01],\n",
            "         [-1.9447e-01, -1.5210e-01,  2.3123e-01,  ...,  7.0638e-02,\n",
            "          -1.4407e-01, -1.6244e-01],\n",
            "         ...,\n",
            "         [-2.4593e-01, -1.3254e-01,  2.5147e-01,  ...,  7.1771e-02,\n",
            "          -1.4446e-01, -1.5355e-01],\n",
            "         [-1.6070e-01, -1.9964e-01,  7.5660e-02,  ...,  1.4354e-01,\n",
            "          -2.1000e-01, -1.6626e-01],\n",
            "         [ 2.6039e-01, -4.1071e-01, -3.8927e-02,  ...,  2.4203e-01,\n",
            "          -1.6164e-01, -1.5522e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-5.4253e-01,  4.8739e-02,  5.3999e-01,  ...,  5.5362e-02,\n",
            "          -4.2676e-02, -1.5397e-01],\n",
            "         [-3.2796e-01,  6.4848e-02,  2.6733e-01,  ...,  5.8729e-02,\n",
            "          -1.2112e-01, -1.9775e-01],\n",
            "         [-1.7071e-01, -1.0875e-01,  2.5161e-01,  ...,  7.5158e-02,\n",
            "          -1.9290e-01, -1.3056e-01],\n",
            "         ...,\n",
            "         [-8.0944e-01, -3.1852e-01, -6.4294e-02,  ...,  3.1855e-01,\n",
            "          -5.9886e-01, -3.6349e-01],\n",
            "         [-8.0944e-01, -3.1852e-01, -6.4294e-02,  ...,  3.1855e-01,\n",
            "          -5.9886e-01, -3.6349e-01],\n",
            "         [-8.0944e-01, -3.1852e-01, -6.4294e-02,  ...,  3.1855e-01,\n",
            "          -5.9886e-01, -3.6349e-01]],\n",
            "\n",
            "        [[-5.5777e-01,  6.4753e-02,  5.3672e-01,  ...,  4.4580e-02,\n",
            "          -5.9461e-02, -2.1232e-01],\n",
            "         [-2.9534e-01,  3.7350e-02,  2.8369e-01,  ...,  7.1283e-02,\n",
            "          -1.1906e-01, -2.0439e-01],\n",
            "         [-1.6769e-01, -8.6249e-02,  2.5354e-01,  ...,  4.5819e-02,\n",
            "          -1.7753e-01, -1.5483e-01],\n",
            "         ...,\n",
            "         [-8.0292e-01, -3.1458e-01, -7.1700e-02,  ...,  3.2427e-01,\n",
            "          -5.9672e-01, -3.6226e-01],\n",
            "         [-8.0292e-01, -3.1458e-01, -7.1700e-02,  ...,  3.2427e-01,\n",
            "          -5.9672e-01, -3.6226e-01],\n",
            "         [-8.0292e-01, -3.1458e-01, -7.1700e-02,  ...,  3.2427e-01,\n",
            "          -5.9672e-01, -3.6226e-01]],\n",
            "\n",
            "        [[-5.0452e-01,  9.1442e-02,  5.1712e-01,  ...,  2.0175e-02,\n",
            "          -5.9740e-02, -1.2309e-01],\n",
            "         [-3.2519e-01,  7.0363e-02,  2.7216e-01,  ...,  8.7408e-02,\n",
            "          -1.8142e-01, -2.3323e-01],\n",
            "         [-1.9007e-01, -8.8669e-02,  2.5617e-01,  ...,  6.3252e-02,\n",
            "          -1.8843e-01, -1.4550e-01],\n",
            "         ...,\n",
            "         [-7.9701e-01, -3.2009e-01, -7.7857e-02,  ...,  3.0968e-01,\n",
            "          -5.9995e-01, -3.4900e-01],\n",
            "         [-7.9701e-01, -3.2009e-01, -7.7857e-02,  ...,  3.0968e-01,\n",
            "          -5.9995e-01, -3.4900e-01],\n",
            "         [-7.9701e-01, -3.2009e-01, -7.7857e-02,  ...,  3.0968e-01,\n",
            "          -5.9995e-01, -3.4900e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_audio = Config(hidden_size=768, intermediate_size=3072, num_labels=2)\n",
        "model_audio = MSFastformer(config_audio)\n",
        "\n",
        "batch_size = 10\n",
        "seq_length = 169\n",
        "hidden_dim = 768\n",
        "x = input_audio_tensor\n",
        "attention_mask = torch.ones(batch_size, seq_length)\n",
        "\n",
        "output_audio = model(x, attention_mask)\n",
        "\n",
        "print(output_audio.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odO5lDmBmFtx",
        "outputId": "a2312554-217b-4ece-95a0-64c4e5bec4b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768, 169])\n",
            "U: torch.Size([10, 768, 169])\n",
            "\n",
            "U1: torch.Size([10, 169, 768])\n",
            "U3: torch.Size([10, 169, 768])\n",
            "U5: torch.Size([10, 169, 768])\n",
            "\n",
            "P: torch.Size([10, 169, 768])\n",
            "I: torch.Size([10, 169, 768])\n",
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87rtTs9Jl4du",
        "outputId": "ad80f688-5f87-4693-b21e-05b82da4a335"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0341, -0.3448, -0.9732,  ...,  0.1939,  0.2449, -0.7762],\n",
            "         [-1.0147,  0.2915,  0.6839,  ...,  0.6794,  0.5997, -0.3294],\n",
            "         [-1.3602,  0.5690,  0.3968,  ...,  1.0700, -0.1319, -0.5826],\n",
            "         ...,\n",
            "         [ 0.7656,  1.0003, -0.6664,  ..., -0.4867, -0.1963,  0.9169],\n",
            "         [ 0.7656,  1.0003, -0.6664,  ..., -0.4867, -0.1963,  0.9169],\n",
            "         [ 0.7656,  1.0003, -0.6664,  ..., -0.4867, -0.1963,  0.9169]],\n",
            "\n",
            "        [[-1.0047,  0.4414, -0.2059,  ..., -0.1818,  0.2430, -0.3856],\n",
            "         [-0.9392,  0.1628, -0.4186,  ...,  0.2227,  0.5497, -0.5415],\n",
            "         [-0.4418,  0.7456,  0.1293,  ...,  0.2034, -0.7693, -0.1733],\n",
            "         ...,\n",
            "         [ 0.2942,  0.7039, -0.3680,  ..., -0.4776, -0.1457,  0.5669],\n",
            "         [ 0.2942,  0.7039, -0.3680,  ..., -0.4776, -0.1457,  0.5669],\n",
            "         [ 0.2942,  0.7039, -0.3680,  ..., -0.4776, -0.1457,  0.5669]],\n",
            "\n",
            "        [[ 0.4109,  0.7545,  0.4385,  ...,  0.8327, -0.3042,  0.0377],\n",
            "         [-0.9220,  1.5821,  0.2727,  ..., -0.5905, -0.3779, -0.6755],\n",
            "         [ 0.2242,  0.5334, -0.5538,  ...,  0.5995, -0.5653, -0.0177],\n",
            "         ...,\n",
            "         [-0.1213,  1.0504, -0.8547,  ...,  0.2929, -0.6587, -0.4393],\n",
            "         [-0.5011,  1.1667,  0.4135,  ...,  0.4149, -0.3719, -0.4652],\n",
            "         [-0.0616,  0.8897, -0.4838,  ..., -0.4439, -0.3233, -0.1106]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.2910,  0.4497, -0.1012,  ...,  0.7770,  0.0274, -0.3145],\n",
            "         [-0.6340,  0.8833,  0.1899,  ...,  0.8572,  0.0657, -0.9379],\n",
            "         [-0.7543,  0.5004,  0.2454,  ...,  0.1661,  0.1692,  0.0998],\n",
            "         ...,\n",
            "         [-0.0203,  0.7790, -0.0983,  ..., -0.2127, -0.1549,  0.3775],\n",
            "         [-0.0203,  0.7790, -0.0983,  ..., -0.2127, -0.1549,  0.3775],\n",
            "         [-0.0203,  0.7790, -0.0983,  ..., -0.2127, -0.1549,  0.3775]],\n",
            "\n",
            "        [[-1.0123,  0.4171,  0.1598,  ..., -0.6377, -0.5184,  0.2356],\n",
            "         [-0.5032,  0.6780, -0.2479,  ...,  0.2144,  0.1534,  1.2569],\n",
            "         [-0.4557,  0.8250,  0.2054,  ...,  0.6120,  0.5434, -0.7119],\n",
            "         ...,\n",
            "         [-0.1626,  0.9868, -0.2735,  ..., -0.4983, -0.3988,  0.6993],\n",
            "         [-0.1626,  0.9868, -0.2735,  ..., -0.4983, -0.3988,  0.6993],\n",
            "         [-0.1626,  0.9868, -0.2735,  ..., -0.4983, -0.3988,  0.6993]],\n",
            "\n",
            "        [[-0.7823,  0.4658, -0.3965,  ...,  0.0196,  0.0043, -0.1768],\n",
            "         [-0.7267,  0.4148,  0.5183,  ...,  0.8107,  0.1160, -0.3517],\n",
            "         [-1.3964,  0.3052,  0.3214,  ...,  0.5559, -0.0285, -0.7157],\n",
            "         ...,\n",
            "         [ 0.1899,  0.8062, -0.7897,  ..., -0.5485,  0.1964,  0.2775],\n",
            "         [ 0.1899,  0.8062, -0.7897,  ..., -0.5485,  0.1964,  0.2775],\n",
            "         [ 0.1899,  0.8062, -0.7897,  ..., -0.5485,  0.1964,  0.2775]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gated Fusion Module: Attempt 1 (completed)"
      ],
      "metadata": {
        "id": "EXyWLUMUkGl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(GatedFusion, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
        "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
        "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, Et, Ea):\n",
        "        #check the shape\n",
        "        assert Et.shape == Ea.shape, \"Et and Ea must have the same shape.\"\n",
        "\n",
        "        concat = torch.cat((Et, Ea), dim=-1)  # [batch_size, seq_len, 2hidden_dim]\n",
        "\n",
        "        out = self.fc1(concat)                # Shape: [batch_size, seq_len, 2hidden_dim]\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)                   # Shape: [batch_size, seq_len, 2hidden_dim]\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)                   # Shape: [batch_size, seq_len, hidden_dim]\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc4(out)                   # Shape: [batch_size, seq_len, hidden_dim]\n",
        "        print(out.shape)\n",
        "        g = self.sigmoid(out)                 # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Compute the fused representation Ef\n",
        "        Ef = g*Ea + (1-g)*Et           # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return Ef"
      ],
      "metadata": {
        "id": "s5NnEfK6kOBv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 768\n",
        "gated_fusion = GatedFusion(hidden_dim=hidden_dim)\n",
        "Ef = gated_fusion(output_text, output_audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_W_-D9oqOPk",
        "outputId": "787cdf9e-fd91-4e5d-c3a2-0c4082ec25dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 1536])\n",
            "torch.Size([10, 169, 1536])\n",
            "torch.Size([10, 169, 768])\n",
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Ef.shape)\n",
        "print(Ef)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl_6qv1gwUAo",
        "outputId": "2bcdb3f0-7357-4105-9471-c47064ef1117"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 768])\n",
            "tensor([[[-0.2612, -0.1381, -0.2209,  ...,  0.1068,  0.1156, -0.4640],\n",
            "         [-0.6774,  0.1985,  0.4808,  ...,  0.3764,  0.2262, -0.2735],\n",
            "         [-0.7694,  0.2456,  0.3151,  ...,  0.5752, -0.1613, -0.3583],\n",
            "         ...,\n",
            "         [-0.0319,  0.3623, -0.3729,  ..., -0.0680, -0.3963,  0.2828],\n",
            "         [-0.0319,  0.3623, -0.3729,  ..., -0.0680, -0.3963,  0.2828],\n",
            "         [-0.0319,  0.3623, -0.3729,  ..., -0.0680, -0.3963,  0.2828]],\n",
            "\n",
            "        [[-0.7775,  0.2505,  0.1643,  ..., -0.0603,  0.1094, -0.2743],\n",
            "         [-0.6360,  0.1179, -0.0496,  ...,  0.1554,  0.2043, -0.3791],\n",
            "         [-0.3038,  0.3195,  0.1930,  ...,  0.1173, -0.4719, -0.1364],\n",
            "         ...,\n",
            "         [-0.2676,  0.2114, -0.2159,  ..., -0.0645, -0.3690,  0.1016],\n",
            "         [-0.2676,  0.2114, -0.2159,  ..., -0.0645, -0.3690,  0.1016],\n",
            "         [-0.2676,  0.2114, -0.2159,  ..., -0.0645, -0.3690,  0.1016]],\n",
            "\n",
            "        [[-0.0764,  0.3985,  0.4913,  ...,  0.4354, -0.1714, -0.0742],\n",
            "         [-0.6262,  0.7993,  0.2608,  ..., -0.2626, -0.2499, -0.4475],\n",
            "         [ 0.0137,  0.1960, -0.1539,  ...,  0.3356, -0.3547, -0.0891],\n",
            "         ...,\n",
            "         [-0.1836,  0.4645, -0.2915,  ...,  0.1827, -0.4039, -0.2981],\n",
            "         [-0.3300,  0.4871,  0.2400,  ...,  0.2806, -0.2915, -0.3182],\n",
            "         [ 0.1007,  0.2511, -0.2572,  ..., -0.1022, -0.2430, -0.1326]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.4175,  0.2528,  0.2246,  ...,  0.4161, -0.0073, -0.2346],\n",
            "         [-0.4803,  0.4762,  0.2289,  ...,  0.4583, -0.0268, -0.5737],\n",
            "         [-0.4591,  0.1981,  0.2486,  ...,  0.1208, -0.0107, -0.0141],\n",
            "         ...,\n",
            "         [-0.4213,  0.2406, -0.0810,  ...,  0.0537, -0.3749,  0.0138],\n",
            "         [-0.4213,  0.2406, -0.0810,  ...,  0.0537, -0.3749,  0.0138],\n",
            "         [-0.4213,  0.2406, -0.0810,  ...,  0.0537, -0.3749,  0.0138]],\n",
            "\n",
            "        [[-0.7846,  0.2428,  0.3514,  ..., -0.3007, -0.2905,  0.0126],\n",
            "         [-0.3985,  0.3604,  0.0201,  ...,  0.1429,  0.0168,  0.5397],\n",
            "         [-0.3105,  0.3729,  0.2299,  ...,  0.3283,  0.1876, -0.4366],\n",
            "         ...,\n",
            "         [-0.4899,  0.3479, -0.1709,  ..., -0.0858, -0.4970,  0.1765],\n",
            "         [-0.4899,  0.3479, -0.1709,  ..., -0.0858, -0.4970,  0.1765],\n",
            "         [-0.4899,  0.3479, -0.1709,  ..., -0.0858, -0.4970,  0.1765]],\n",
            "\n",
            "        [[-0.6434,  0.2807,  0.0681,  ...,  0.0199, -0.0275, -0.1501],\n",
            "         [-0.5238,  0.2439,  0.3922,  ...,  0.4515, -0.0312, -0.2934],\n",
            "         [-0.7934,  0.1098,  0.2886,  ...,  0.3103, -0.1081, -0.4353],\n",
            "         ...,\n",
            "         [-0.3116,  0.2486, -0.4292,  ..., -0.1189, -0.2006, -0.0309],\n",
            "         [-0.3116,  0.2486, -0.4292,  ..., -0.1189, -0.2006, -0.0309],\n",
            "         [-0.3116,  0.2486, -0.4292,  ..., -0.1189, -0.2006, -0.0309]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiFPN (alternative of Recurrent Pyramid Model)"
      ],
      "metadata": {
        "id": "jJqEt1H3xj7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A depthwise separable convolution block that consists of a depthwise convolution\n",
        "    followed by a pointwise convolution, with batch normalization and activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, norm=True, activation=True):\n",
        "        super(SeparableConvBlock, self).__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels, bias=False)\n",
        "        self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.norm = nn.BatchNorm1d(out_channels) if norm else None\n",
        "        self.activation = nn.ReLU() if activation else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class BiFPNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of BiFPN that performs bidirectional feature fusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size, epsilon=1e-4):\n",
        "        super(BiFPNLayer, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Weights for feature fusion\n",
        "        self.w1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
        "        self.w3 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "        # Convolution blocks\n",
        "        self.conv3 = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv2 = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv1 = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv0 = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, p0, p1, p2, p3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            p0, p1, p2, p3: Feature maps at different scales.\n",
        "        Returns:\n",
        "            Updated feature maps after bidirectional fusion.\n",
        "        \"\"\"\n",
        "\n",
        "        w1 = F.relu(self.w1)\n",
        "        w1 = w1 / (torch.sum(w1, dim=0) + self.epsilon)\n",
        "\n",
        "        w2 = F.relu(self.w2)\n",
        "        w2 = w2 / (torch.sum(w2, dim=0) + self.epsilon)\n",
        "\n",
        "        w3 = F.relu(self.w3)\n",
        "        w3 = w3 / (torch.sum(w3, dim=0) + self.epsilon)\n",
        "        p3_td = p3\n",
        "\n",
        "        p3_upsampled = F.interpolate(p3_td, size=p2.size(-1), mode='nearest')\n",
        "        p2_td = self.conv2(w1[0] * p2 + w1[1] * p3_upsampled)\n",
        "\n",
        "        p2_upsampled = F.interpolate(p2_td, size=p1.size(-1), mode='nearest')\n",
        "        p1_td = self.conv1(w2[0] * p1 + w2[1] * p2_upsampled)\n",
        "\n",
        "        p1_upsampled = F.interpolate(p1_td, size=p0.size(-1), mode='nearest')\n",
        "        p0_td = self.conv0(w3[0] * p0 + w3[1] * p1_upsampled)\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        # Downsample p0_td to match p1_td's sequence length\n",
        "        p0_downsampled = F.interpolate(p0_td, size=p1_td.size(-1), mode='nearest')\n",
        "        p1_out = self.conv1(w3[0] * p1_td + w3[1] * p0_downsampled)\n",
        "\n",
        "        # Downsample p1_out to match p2_td's sequence length\n",
        "        p1_downsampled = F.interpolate(p1_out, size=p2_td.size(-1), mode='nearest')\n",
        "        p2_out = self.conv2(w2[0] * p2_td + w2[1] * p1_downsampled)\n",
        "\n",
        "        # Downsample p2_out to match p3's sequence length\n",
        "        p2_downsampled = F.interpolate(p2_out, size=p3.size(-1), mode='nearest')\n",
        "        p3_out = self.conv3(w1[0] * p3 + w1[1] * p2_downsampled)\n",
        "\n",
        "        return p0_td, p1_out, p2_out, p3_out\n",
        "\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    BiFPN module that can be stacked multiple times.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size=256, num_layers=1):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.bifpn_layers = nn.ModuleList([BiFPNLayer(feature_size) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: List of feature maps [p0, p1, p2, p3] at different resolutions.\n",
        "        Returns:\n",
        "            List of enhanced feature maps at different resolutions.\n",
        "        \"\"\"\n",
        "        p0, p1, p2, p3 = features\n",
        "\n",
        "        for bifpn_layer in self.bifpn_layers:\n",
        "            p0, p1, p2, p3 = bifpn_layer(p0, p1, p2, p3)\n",
        "        print(p0.shape)\n",
        "\n",
        "        return [p0, p1, p2, p3]\n",
        "\n",
        "# Example usage\n",
        "class MultimodalFeatureEnhancer(nn.Module):\n",
        "    def __init__(self, input_dim, feature_size=256, num_bifpn_layers=2):\n",
        "        super(MultimodalFeatureEnhancer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        # Initial convolution to reduce dimensionality\n",
        "        self.conv = nn.Conv1d(input_dim, feature_size, kernel_size=1)\n",
        "\n",
        "        # Create feature maps at different scales using MaxPool\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # BiFPN\n",
        "        self.bifpn = BiFPN(feature_size=feature_size, num_layers=num_bifpn_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Multimodal representation tensor of shape [batch_size, seq_len, input_dim]\n",
        "        Returns:\n",
        "            Enhanced feature representation.\n",
        "        \"\"\"\n",
        "        # Transpose to [batch_size, input_dim, seq_len] for Conv1d\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Initial convolution\n",
        "        p0 = self.conv(x)  # [batch_size, feature_size, seq_len]\n",
        "\n",
        "        # Generate feature maps at different scales\n",
        "        p1 = self.pool1(p0)\n",
        "        p2 = self.pool2(p1)\n",
        "        p3 = self.pool3(p2)\n",
        "\n",
        "        # Apply BiFPN\n",
        "        p0, p1, p2, p3 = self.bifpn([p0, p1, p2, p3])\n",
        "\n",
        "        # Upsample and concatenate features\n",
        "        p1_upsampled = F.interpolate(p1, size=p0.size(-1), mode='nearest')\n",
        "        p2_upsampled = F.interpolate(p2, size=p0.size(-1), mode='nearest')\n",
        "        p3_upsampled = F.interpolate(p3, size=p0.size(-1), mode='nearest')\n",
        "\n",
        "        # Fuse features\n",
        "        fused_features = p0 + p1_upsampled + p2_upsampled + p3_upsampled\n",
        "        print(fused_features.shape)\n",
        "\n",
        "        # Transpose back to [batch_size, seq_len, feature_size]\n",
        "        fused_features = fused_features.transpose(1, 2)\n",
        "\n",
        "        return fused_features"
      ],
      "metadata": {
        "id": "vtJMHi-Xo1Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhancer = MultimodalFeatureEnhancer(768, 256, 2)\n",
        "enhanced_features = enhancer(Ef)"
      ],
      "metadata": {
        "id": "uQW-x0RDpDDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c3d2e0-e31e-4c9d-946f-603ecb90be40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 256, 169])\n",
            "torch.Size([10, 256, 169])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enhanced_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyXfia2GqXVI",
        "outputId": "ab23d02e-7133-4fcc-dcc7-2033cebb3c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enhanced_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n552G8YeB7dM",
        "outputId": "36946b8d-5b0d-4281-df38-9b03c78a09fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3603, 2.8288, 2.2495,  ..., 3.6032, 0.3137, 5.9101],\n",
            "         [2.8127, 1.9771, 0.9010,  ..., 2.3469, 0.3137, 6.6594],\n",
            "         [1.8809, 1.8876, 0.9010,  ..., 2.3469, 0.3137, 6.5438],\n",
            "         ...,\n",
            "         [2.9743, 0.5957, 0.4998,  ..., 1.9544, 1.9153, 1.3009],\n",
            "         [5.2078, 0.7545, 0.2383,  ..., 3.1715, 2.8257, 0.1243],\n",
            "         [4.5438, 0.7545, 0.2383,  ..., 1.6581, 1.2463, 0.1243]],\n",
            "\n",
            "        [[0.5775, 1.5946, 1.8539,  ..., 2.2678, 0.2885, 6.5705],\n",
            "         [1.9302, 0.4068, 1.4391,  ..., 1.2080, 0.2885, 5.4702],\n",
            "         [2.5697, 0.3496, 1.4391,  ..., 1.2080, 0.2885, 4.8957],\n",
            "         ...,\n",
            "         [1.7862, 0.8385, 1.3480,  ..., 3.3230, 1.9547, 3.0728],\n",
            "         [4.1367, 0.8385, 0.6889,  ..., 4.3716, 2.1790, 1.6705],\n",
            "         [4.1940, 0.8385, 0.5198,  ..., 1.6238, 1.6801, 1.6705]],\n",
            "\n",
            "        [[0.0000, 0.4910, 1.7939,  ..., 0.7575, 0.9360, 4.9426],\n",
            "         [1.3692, 0.4910, 1.5875,  ..., 0.0510, 0.9360, 4.8335],\n",
            "         [1.5396, 1.0990, 1.5875,  ..., 0.0000, 0.9360, 5.0860],\n",
            "         ...,\n",
            "         [0.7055, 2.8981, 2.1974,  ..., 0.8356, 0.8942, 0.7973],\n",
            "         [1.4107, 2.8981, 0.6282,  ..., 1.4458, 2.8379, 0.7973],\n",
            "         [1.4107, 2.8981, 0.0130,  ..., 1.0966, 1.9509, 0.7973]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0000, 0.2140, 4.1259,  ..., 1.8111, 1.3576, 5.2599],\n",
            "         [0.7726, 0.0000, 3.1333,  ..., 1.2761, 1.3576, 4.8589],\n",
            "         [0.7348, 0.3778, 3.1333,  ..., 1.2761, 1.3576, 3.6817],\n",
            "         ...,\n",
            "         [1.3802, 0.0529, 1.3606,  ..., 1.1930, 0.7008, 1.2303],\n",
            "         [2.5744, 0.0529, 0.0000,  ..., 2.4672, 0.9984, 1.2303],\n",
            "         [3.5928, 0.0529, 0.0000,  ..., 0.8885, 0.7008, 1.2566]],\n",
            "\n",
            "        [[0.2236, 0.1250, 2.0903,  ..., 0.5059, 1.3579, 4.2285],\n",
            "         [1.0752, 0.0000, 1.7548,  ..., 0.4256, 1.3579, 3.7914],\n",
            "         [1.5626, 0.4256, 1.7548,  ..., 0.4256, 1.3579, 3.4625],\n",
            "         ...,\n",
            "         [1.3340, 1.8101, 2.3085,  ..., 2.5913, 2.1949, 0.0000],\n",
            "         [3.6387, 1.8101, 1.2588,  ..., 3.7117, 2.1949, 0.0000],\n",
            "         [4.6748, 1.8101, 0.8440,  ..., 1.5128, 2.1949, 0.0000]],\n",
            "\n",
            "        [[0.0635, 0.4924, 1.0958,  ..., 1.6733, 0.6374, 3.4594],\n",
            "         [1.8237, 1.2051, 0.0000,  ..., 1.6733, 0.6374, 3.0861],\n",
            "         [2.1015, 1.5574, 0.5452,  ..., 1.6733, 0.6374, 1.4467],\n",
            "         ...,\n",
            "         [1.4915, 0.8528, 0.9712,  ..., 2.4328, 1.4244, 1.1599],\n",
            "         [1.9909, 0.8528, 0.0000,  ..., 2.3741, 3.4142, 1.1599],\n",
            "         [2.2357, 0.8528, 0.0000,  ..., 1.0700, 1.5432, 1.1599]]],\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiFPN Update"
      ],
      "metadata": {
        "id": "v3RrveK2IqGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A depthwise separable convolution block that consists of a depthwise convolution\n",
        "    followed by a pointwise convolution, with batch normalization and activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, norm=True, activation=True):\n",
        "        super(SeparableConvBlock, self).__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size=kernel_size,\n",
        "            padding=padding, groups=in_channels, bias=False\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.norm = nn.BatchNorm1d(out_channels) if norm else None\n",
        "        self.activation = nn.ReLU() if activation else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class BiFPNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of BiFPN that performs bidirectional feature fusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size, epsilon=1e-4):\n",
        "        super(BiFPNLayer, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Weights for top-down pathway\n",
        "        self.w_p2_td = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "        self.w_p1_td = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "        self.w_p0_td = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "\n",
        "        # Weights for bottom-up pathway\n",
        "        self.w_p1_out = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "        self.w_p2_out = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "        self.w_p3_out = nn.Parameter(torch.ones(2), requires_grad=True)\n",
        "\n",
        "        # Convolution blocks for each fusion node\n",
        "        self.conv_p2_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p1_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p0_td = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "        self.conv_p1_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p2_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p3_out = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, p0, p1, p2, p3):\n",
        "        # Normalize weights for top-down pathway\n",
        "        def normalize_weights(w):\n",
        "            return F.relu(w) / (torch.sum(F.relu(w)) + self.epsilon)\n",
        "\n",
        "        w_p2_td = normalize_weights(self.w_p2_td)\n",
        "        w_p1_td = normalize_weights(self.w_p1_td)\n",
        "        w_p0_td = normalize_weights(self.w_p0_td)\n",
        "\n",
        "        # Top-down pathway\n",
        "        p3_td = p3\n",
        "\n",
        "        # Level p2_td\n",
        "        p3_upsampled = F.interpolate(p3_td, size=p2.size(-1), mode='linear', align_corners=False)\n",
        "        p2_td = w_p2_td[0] * p2 + w_p2_td[1] * p3_upsampled\n",
        "        p2_td = self.conv_p2_td(p2_td)\n",
        "\n",
        "        # Level p1_td\n",
        "        p2_upsampled = F.interpolate(p2_td, size=p1.size(-1), mode='linear', align_corners=False)\n",
        "        p1_td = w_p1_td[0] * p1 + w_p1_td[1] * p2_upsampled\n",
        "        p1_td = self.conv_p1_td(p1_td)\n",
        "\n",
        "        # Level p0_td\n",
        "        p1_upsampled = F.interpolate(p1_td, size=p0.size(-1), mode='linear', align_corners=False)\n",
        "        p0_td = w_p0_td[0] * p0 + w_p0_td[1] * p1_upsampled\n",
        "        p0_td = self.conv_p0_td(p0_td)\n",
        "\n",
        "        # Normalize weights for bottom-up pathway\n",
        "        w_p1_out = normalize_weights(self.w_p1_out)\n",
        "        w_p2_out = normalize_weights(self.w_p2_out)\n",
        "        w_p3_out = normalize_weights(self.w_p3_out)\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        # Level p1_out\n",
        "        p0_downsampled = F.interpolate(p0_td, size=p1_td.size(-1), mode='linear', align_corners=False)\n",
        "        p1_out = w_p1_out[0] * p1_td + w_p1_out[1] * p0_downsampled\n",
        "        p1_out = self.conv_p1_out(p1_out)\n",
        "\n",
        "        # Level p2_out\n",
        "        p1_downsampled = F.interpolate(p1_out, size=p2_td.size(-1), mode='linear', align_corners=False)\n",
        "        p2_out = w_p2_out[0] * p2_td + w_p2_out[1] * p1_downsampled\n",
        "        p2_out = self.conv_p2_out(p2_out)\n",
        "\n",
        "        # Level p3_out\n",
        "        p2_downsampled = F.interpolate(p2_out, size=p3.size(-1), mode='linear', align_corners=False)\n",
        "        p3_out = w_p3_out[0] * p3 + w_p3_out[1] * p2_downsampled\n",
        "        p3_out = self.conv_p3_out(p3_out)\n",
        "\n",
        "        return p0_td, p1_out, p2_out, p3_out\n",
        "\n",
        "class BiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    BiFPN module that can be stacked multiple times.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size=256, num_layers=1, epsilon=1e-4):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.bifpn_layers = nn.ModuleList([\n",
        "            BiFPNLayer(feature_size, epsilon) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        p0, p1, p2, p3 = features\n",
        "\n",
        "        for bifpn_layer in self.bifpn_layers:\n",
        "            p0, p1, p2, p3 = bifpn_layer(p0, p1, p2, p3)\n",
        "\n",
        "        return [p0, p1, p2, p3]\n",
        "\n",
        "class MultimodalFeatureEnhancer(nn.Module):\n",
        "    def __init__(self, input_dim, feature_size=256, num_bifpn_layers=2):\n",
        "        super(MultimodalFeatureEnhancer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        # Initial convolution to reduce dimensionality\n",
        "        self.conv = nn.Conv1d(input_dim, feature_size, kernel_size=3, padding=1)\n",
        "\n",
        "        # Create feature maps at different scales using strided convolutions\n",
        "        self.conv_p1 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_p2 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_p3 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN\n",
        "        self.bifpn = BiFPN(feature_size=feature_size, num_layers=num_bifpn_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transpose to [batch_size, input_dim, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Initial convolution\n",
        "        p0 = self.conv(x)  # [batch_size, feature_size, seq_len]\n",
        "\n",
        "        # Generate feature maps at different scales\n",
        "        p1 = self.conv_p1(p0)\n",
        "        p2 = self.conv_p2(p1)\n",
        "        p3 = self.conv_p3(p2)\n",
        "\n",
        "        # Apply BiFPN\n",
        "        p0, p1, p2, p3 = self.bifpn([p0, p1, p2, p3])\n",
        "\n",
        "        # Transpose each feature map back to [batch_size, seq_len, feature_size]\n",
        "        p0 = p0.transpose(1, 2)\n",
        "        p1 = p1.transpose(1, 2)\n",
        "        p2 = p2.transpose(1, 2)\n",
        "        p3 = p3.transpose(1, 2)\n",
        "\n",
        "        # Return the multi-resolution features as a list or tuple\n",
        "        return [p0, p1, p2, p3]\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEZqs4OJIxIo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mme = MultimodalFeatureEnhancer(768, 256, 2)\n",
        "multi_res_features_list = mme(Ef)"
      ],
      "metadata": {
        "id": "dQ2hBUquKmjy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(multi_res_features_list[0].shape)\n",
        "print(multi_res_features_list[1].shape)\n",
        "print(multi_res_features_list[2].shape)\n",
        "print(multi_res_features_list[3].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UprDU_VOOZ-p",
        "outputId": "311ebdd2-3dde-4e40-953e-e44abba139d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 256])\n",
            "torch.Size([10, 85, 256])\n",
            "torch.Size([10, 43, 256])\n",
            "torch.Size([10, 22, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiFPN Update 2"
      ],
      "metadata": {
        "id": "yezoxWiXVjn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A depthwise separable convolution block that consists of a depthwise convolution\n",
        "    followed by a pointwise convolution, with batch normalization and activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, norm=True, activation=True):\n",
        "        super(SeparableConvBlock, self).__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size=kernel_size,\n",
        "            padding=padding, groups=in_channels, bias=False\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.norm = nn.BatchNorm1d(out_channels) if norm else None\n",
        "        self.activation = nn.ReLU() if activation else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LdPnmINoVny5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiFPNLayer(nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super(BiFPNLayer, self).__init__()\n",
        "\n",
        "        # Convolution blocks for each fusion node in the top-down path\n",
        "        self.conv_p2_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p1_td = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p0_td = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "        # Convolution blocks for each fusion node in the bottom-up path\n",
        "        self.conv_p0_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p1_out = SeparableConvBlock(feature_size, feature_size)\n",
        "        self.conv_p2_out = SeparableConvBlock(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, p0, p1, p2):\n",
        "        # Top-down pathway\n",
        "        p2_td = self.conv_p2_td(p2)\n",
        "\n",
        "        p2_upsampled = F.interpolate(p2_td, size=p1.size(-1), mode='linear', align_corners=False)\n",
        "        p1_td = self.conv_p1_td(p1 + p2_upsampled)\n",
        "\n",
        "        p1_upsampled = F.interpolate(p1_td, size=p0.size(-1), mode='linear', align_corners=False)\n",
        "        p0_td = self.conv_p0_td(p0 + p1_upsampled)\n",
        "\n",
        "        # Bottom-up pathway\n",
        "        p0_downsampled = F.interpolate(p0_td, size=p1_td.size(-1), mode='linear', align_corners=False)\n",
        "        p0_out = self.conv_p0_out(p0_downsampled + p1_td)\n",
        "\n",
        "        p0_out_downsampled = F.interpolate(p0_out, size=p2_td.size(-1), mode='linear', align_corners=False)\n",
        "        p1_out = self.conv_p1_out(p0_out_downsampled + p2_td)\n",
        "\n",
        "        p1_out_downsampled = F.interpolate(p1_out, size=p2_td.size(-1), mode='linear', align_corners=False)\n",
        "        p2_out = self.conv_p2_out(p1_out_downsampled + p2_td)\n",
        "\n",
        "        return p0_td, p1_td, p2_td, p0_out, p1_out, p2_out\n"
      ],
      "metadata": {
        "id": "fiAWKbi9X9nv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiFPN(nn.Module):\n",
        "    def __init__(self, feature_size=256, num_layers=1):\n",
        "        super(BiFPN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.bifpn_layers = nn.ModuleList([\n",
        "            BiFPNLayer(feature_size) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        p0, p1, p2 = features\n",
        "\n",
        "        for bifpn_layer in self.bifpn_layers:\n",
        "            p0_td, p1_td, p2_td, p0_out, p1_out, p2_out = bifpn_layer(p0, p1, p2)\n",
        "            # Update p0, p1, p2 for the next layer if stacking\n",
        "            p0, p1, p2 = p0_td, p1_td, p2_td\n",
        "\n",
        "        return [p0_td, p1_td, p2_td, p0_out, p1_out, p2_out]"
      ],
      "metadata": {
        "id": "wHOUo7vrYJ3F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalFeatureEnhancer(nn.Module):\n",
        "    def __init__(self, input_dim, feature_size=256, num_bifpn_layers=2):\n",
        "        super(MultimodalFeatureEnhancer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        # First layer: 1D convolutional kernel of size 1\n",
        "        self.conv = nn.Conv1d(input_dim, feature_size, kernel_size=1)\n",
        "\n",
        "        # Second layer: 1D convolutional kernel of size 3, stride 2\n",
        "        self.conv_p1 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Third layer: 1D convolutional kernel of size 3, stride 2\n",
        "        self.conv_p2 = nn.Conv1d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # BiFPN\n",
        "        self.bifpn = BiFPN(feature_size=feature_size, num_layers=num_bifpn_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transpose to [batch_size, input_dim, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # First layer\n",
        "        p0 = self.conv(x)  # [batch_size, feature_size, seq_len]\n",
        "\n",
        "        # Second layer\n",
        "        p1 = self.conv_p1(p0)\n",
        "\n",
        "        # Third layer\n",
        "        p2 = self.conv_p2(p1)\n",
        "\n",
        "        # Apply BiFPN\n",
        "        outputs = self.bifpn([p0, p1, p2])\n",
        "\n",
        "        # Unpack outputs\n",
        "        p0_td, p1_td, p2_td, p0_out, p1_out, p2_out = outputs\n",
        "\n",
        "        # Transpose each feature map back to [batch_size, seq_len, feature_size]\n",
        "        p0_td = p0_td.transpose(1, 2)\n",
        "        p1_td = p1_td.transpose(1, 2)\n",
        "        p2_td = p2_td.transpose(1, 2)\n",
        "        p0_out = p0_out.transpose(1, 2)\n",
        "        p1_out = p1_out.transpose(1, 2)\n",
        "        p2_out = p2_out.transpose(1, 2)\n",
        "\n",
        "        # Return the six feature maps\n",
        "        return [p0_td, p1_td, p2_td, p0_out, p1_out, p2_out]"
      ],
      "metadata": {
        "id": "dEPntC9PYf5Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top-Down Path Outputs:\n",
        "- **p0_td**: Fine-grained feature map from the top-down path.\n",
        "- **p1_td**: Medium-grained feature map from the top-down path.\n",
        "- **p2_td**: Coarse-grained feature map from the top-down path.\n",
        "\n",
        "Bottom-Up Path Outputs:\n",
        "- **p0_out**: Fine-grained feature map from the bottom-up path.\n",
        "- **p1_out**: Medium-grained feature map from the bottom-up path.\n",
        "- **p2_out**: Coarse-grained feature map from the bottom-up path.\n"
      ],
      "metadata": {
        "id": "CeLupgHpYlk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bifpn = BiFPN(feature_size=256, num_layers=2)"
      ],
      "metadata": {
        "id": "Lf-QMgYKZC-3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MFE = MultimodalFeatureEnhancer(input_dim=768, feature_size=256, num_bifpn_layers=2)"
      ],
      "metadata": {
        "id": "rCAQLOhyZdD6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = mme(Ef)\n"
      ],
      "metadata": {
        "id": "87k3yVUDaB-Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p0_td, p1_td, p2_td, p0_out, p1_out, p2_out = outputs\n",
        "print(\"Shape of p0_td (top-down fine-grained):\", p0_td.shape)  # [batch_size, seq_len, feature_size]\n",
        "print(\"Shape of p1_td (top-down medium-grained):\", p1_td.shape)  # [batch_size, seq_len//2, feature_size]\n",
        "print(\"Shape of p2_td (top-down coarse-grained):\", p2_td.shape)  # [batch_size, seq_len//4, feature_size]\n",
        "print(\"Shape of p0_out (bottom-up fine-grained):\", p0_out.shape)  # [batch_size, seq_len, feature_size]\n",
        "print(\"Shape of p1_out (bottom-up medium-grained):\", p1_out.shape)  # [batch_size, seq_len//2, feature_size]\n",
        "print(\"Shape of p2_out (bottom-up coarse-grained):\", p2_out.shape)  # [batch_size, seq_len//4, feature_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEPg6mpfaKvg",
        "outputId": "47833f0e-a320-42ed-cc51-4206fc3c959a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of p0_td (top-down fine-grained): torch.Size([10, 169, 256])\n",
            "Shape of p1_td (top-down medium-grained): torch.Size([10, 85, 256])\n",
            "Shape of p2_td (top-down coarse-grained): torch.Size([10, 43, 256])\n",
            "Shape of p0_out (bottom-up fine-grained): torch.Size([10, 85, 256])\n",
            "Shape of p1_out (bottom-up medium-grained): torch.Size([10, 43, 256])\n",
            "Shape of p2_out (bottom-up coarse-grained): torch.Size([10, 43, 256])\n"
          ]
        }
      ]
    }
  ]
}