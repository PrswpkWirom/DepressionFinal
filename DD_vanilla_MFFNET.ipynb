{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1m8cYU_xyzZkpHpFCCP3hS9brHau-pg0F",
      "authorship_tag": "ABX9TyOoKlzgJ1QJchOS2tt0Q+IF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrswpkWirom/Depression-Detection-FinalProject/blob/main/DD_vanilla_MFFNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader, mini batch"
      ],
      "metadata": {
        "id": "ZyGzAgiZR8t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import files\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "3WXV52DVSAz_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file_path = '/content/drive/MyDrive/DD_input_text_tensor_cpu.pkl'\n",
        "input_text_tensor = torch.load(pickle_file_path)\n",
        "print(input_text_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSWiISZ9SbF9",
        "outputId": "91fc8352-c4fa-469c-aed0-a67ae104261b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0584,  0.0803, -0.0016,  ..., -0.0751, -0.0839, -0.0127],\n",
            "         [-0.0734,  0.0904,  0.0002,  ..., -0.0699, -0.0782, -0.0195],\n",
            "         [-0.0787,  0.0642, -0.0052,  ..., -0.0863, -0.0811, -0.0373],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0524,  0.0920, -0.0036,  ..., -0.0810, -0.0577, -0.0343],\n",
            "         [-0.0517,  0.0909, -0.0059,  ..., -0.0802, -0.0536, -0.0325],\n",
            "         [-0.0736,  0.0564,  0.0038,  ..., -0.1028, -0.0545, -0.0117],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0596,  0.1051,  0.0067,  ..., -0.0871, -0.0634, -0.0286],\n",
            "         [-0.0597,  0.0772, -0.0034,  ..., -0.0932, -0.0878, -0.0135],\n",
            "         [-0.0702,  0.0533, -0.0156,  ..., -0.1149, -0.0809, -0.0233],\n",
            "         ...,\n",
            "         [-0.0460,  0.0664, -0.0231,  ..., -0.1232, -0.0775, -0.0196],\n",
            "         [-0.0610,  0.0796, -0.0111,  ..., -0.0789, -0.0850, -0.0119],\n",
            "         [-0.0580,  0.0784, -0.0025,  ..., -0.0796, -0.0856, -0.0070]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0517,  0.0909, -0.0059,  ..., -0.0802, -0.0536, -0.0325],\n",
            "         [-0.0698,  0.0731, -0.0061,  ..., -0.0762, -0.0706, -0.0378],\n",
            "         [-0.0463,  0.0804,  0.0125,  ..., -0.0582, -0.0430, -0.0036],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0493,  0.1006,  0.0038,  ..., -0.0843, -0.0667, -0.0391],\n",
            "         [-0.0596,  0.1051,  0.0067,  ..., -0.0871, -0.0634, -0.0286],\n",
            "         [-0.0409,  0.0749,  0.0080,  ..., -0.1013, -0.0521, -0.0279],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0701,  0.0617, -0.0055,  ..., -0.0795, -0.0607, -0.0137],\n",
            "         [-0.0715,  0.0823, -0.0005,  ..., -0.0860, -0.0770, -0.0524],\n",
            "         [-0.0524,  0.0909, -0.0215,  ..., -0.0936, -0.0636, -0.0368],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-1ce23ae293e7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_text_tensor = torch.load(pickle_file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_text_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mim0HpOFAfKI",
        "outputId": "4bfdf0c5-1ad1-41d3-ec45-d6458e15ad70"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 169, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fastformer Implementation"
      ],
      "metadata": {
        "id": "24ZJahr2XrXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This fastformer module is obtained from github: https://github.com/wuch15/Fastformer"
      ],
      "metadata": {
        "id": "1iNQWS19Xw1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "from transformers.models.bert.modeling_bert import BertSelfOutput, BertIntermediate, BertOutput\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.att_fc2 = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        bz = x.shape[0]\n",
        "        e = self.att_fc1(x)\n",
        "        e = nn.Tanh()(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "        alpha = torch.exp(alpha)\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "        x = torch.reshape(x, (bz, -1))\n",
        "        return x\n",
        "\n",
        "class FastSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastSelfAttention, self).__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" %\n",
        "                (config.hidden_size, config.num_attention_heads))\n",
        "        self.attention_head_size = int(config.hidden_size /config.num_attention_heads)\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.input_dim= config.hidden_size\n",
        "\n",
        "        self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                       self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        # batch_size, seq_len, num_head * head_dim, batch_size, seq_len\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        # batch_size, num_head, seq_len\n",
        "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5\n",
        "        # add attention mask\n",
        "        query_for_score += attention_mask\n",
        "\n",
        "        # batch_size, num_head, 1, seq_len\n",
        "        query_weight = self.softmax(query_for_score).unsqueeze(2)\n",
        "\n",
        "        # batch_size, num_head, seq_len, head_dim\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # batch_size, num_head, head_dim, 1\n",
        "        pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).view(-1,1,self.num_attention_heads*self.attention_head_size)\n",
        "        pooled_query_repeat= pooled_query.repeat(1, seq_len,1)\n",
        "        # batch_size, num_head, seq_len, head_dim\n",
        "\n",
        "        # batch_size, num_head, seq_len\n",
        "        mixed_query_key_layer=mixed_key_layer* pooled_query_repeat\n",
        "\n",
        "        query_key_score=(self.key_att(mixed_query_key_layer)/ self.attention_head_size**0.5).transpose(1, 2)\n",
        "\n",
        "        # add attention mask\n",
        "        query_key_score +=attention_mask\n",
        "\n",
        "        # batch_size, num_head, 1, seq_len\n",
        "        query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
        "        pooled_key = torch.matmul(query_key_weight, key_layer)\n",
        "\n",
        "        #query = value\n",
        "        weighted_value =(pooled_key * query_layer).transpose(1, 2)\n",
        "        weighted_value = weighted_value.reshape(\n",
        "            weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
        "        weighted_value = self.transform(weighted_value) + mixed_query_layer\n",
        "\n",
        "        return weighted_value\n",
        "\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastAttention, self).__init__()\n",
        "        self.self = FastSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "class FastformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastformerLayer, self).__init__()\n",
        "        self.attention = FastAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "class FastformerEncoder(nn.Module):\n",
        "    def __init__(self, config, pooler_count=1):\n",
        "        super(FastformerEncoder, self).__init__()\n",
        "        self.config = config\n",
        "        self.encoders = nn.ModuleList([FastformerLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # support multiple different poolers with shared bert encoder.\n",
        "        self.poolers = nn.ModuleList()\n",
        "        if config.pooler_type == 'weightpooler':\n",
        "            for _ in range(pooler_count):\n",
        "                self.poolers.append(AttentionPooling(config))\n",
        "        logging.info(f\"This model has {len(self.poolers)} poolers.\")\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Embedding)) and module.padding_idx is not None:\n",
        "                with torch.no_grad():\n",
        "                    module.weight[module.padding_idx].fill_(0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self,\n",
        "                input_embs,\n",
        "                attention_mask,\n",
        "                pooler_index=0):\n",
        "        #input_embs: batch_size, seq_len, emb_dim\n",
        "        #attention_mask: batch_size, seq_len, emb_dim\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        batch_size, seq_length, emb_dim = input_embs.shape\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_embs.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = input_embs + position_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        #print(embeddings.size())\n",
        "        all_hidden_states = [embeddings]\n",
        "\n",
        "        for i, layer_module in enumerate(self.encoders):\n",
        "            layer_outputs = layer_module(all_hidden_states[-1], extended_attention_mask)\n",
        "            all_hidden_states.append(layer_outputs)\n",
        "        assert len(self.poolers) > pooler_index\n",
        "        output = self.poolers[pooler_index](all_hidden_states[-1], attention_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.dense_linear = nn.Linear(config.hidden_size,4)\n",
        "        self.word_embedding = nn.Embedding(len(word_dict),256,padding_idx=0)\n",
        "        self.fastformer_model = FastformerEncoder(config)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Embedding)) and module.padding_idx is not None:\n",
        "                with torch.no_grad():\n",
        "                    module.weight[module.padding_idx].fill_(0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self,input_ids,targets):\n",
        "        mask=input_ids.bool().float()\n",
        "        embds=self.word_embedding(input_ids)\n",
        "        text_vec = self.fastformer_model(embds,mask)\n",
        "        score = self.dense_linear(text_vec)\n",
        "        loss = self.criterion(score, targets)\n",
        "        return loss, score\n",
        "\n",
        "def acc(y_true, y_hat):\n",
        "    y_hat = torch.argmax(y_hat, dim=-1)\n",
        "    tot = y_true.shape[0]\n",
        "    hit = torch.sum(y_true == y_hat)\n",
        "    return hit.data.float() * 1.0 / tot"
      ],
      "metadata": {
        "id": "BFV0rTrlXvrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt 1: Vanilla MSFastformer"
      ],
      "metadata": {
        "id": "X7olyFrUOUR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class AttentionPooling(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(AttentionPooling, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.att_fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "#         self.att_fc2 = nn.Linear(config.hidden_size, 1)\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x, attn_mask=None):\n",
        "#         bz = x.shape[0]\n",
        "#         e = self.att_fc1(x)\n",
        "#         e = nn.Tanh()(e)\n",
        "#         alpha = self.att_fc2(e)\n",
        "#         alpha = torch.exp(alpha)\n",
        "#         if attn_mask is not None:\n",
        "#             alpha = alpha * attn_mask.unsqueeze(2)\n",
        "#         alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "#         x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "#         x = x.reshape(bz, -1)\n",
        "#         return x\n",
        "\n",
        "# class FastSelfAttention(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastSelfAttention, self).__init__()\n",
        "#         self.config = config\n",
        "#         if config.hidden_size % config.num_attention_heads != 0:\n",
        "#             raise ValueError(\n",
        "#                 \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "#                 \"heads (%d)\" %\n",
        "#                 (config.hidden_size, config.num_attention_heads))\n",
        "#         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "#         self.num_attention_heads = config.num_attention_heads\n",
        "#         self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "#         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "#         self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "#         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "#         self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "#         self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "#         self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "\n",
        "#     def transpose_for_scores(self, x):\n",
        "#         new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "#                                        self.attention_head_size)\n",
        "#         x = x.view(*new_x_shape)\n",
        "#         return x.permute(0, 2, 1, 3)\n",
        "\n",
        "#     def forward(self, q_input, k_input, attention_mask=None):\n",
        "#         batch_size, seq_len, _ = q_input.shape\n",
        "#         # Project queries and keys\n",
        "#         mixed_query_layer = self.query(q_input)\n",
        "#         mixed_key_layer = self.key(k_input)\n",
        "\n",
        "#         # Compute query scores\n",
        "#         query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5\n",
        "#         if attention_mask is not None:\n",
        "#             query_for_score = query_for_score + attention_mask\n",
        "\n",
        "#         query_weight = self.softmax(query_for_score).unsqueeze(2)\n",
        "#         query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "#         pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).reshape(\n",
        "#             batch_size, 1, self.all_head_size)\n",
        "#         pooled_query_repeat = pooled_query.repeat(1, seq_len, 1)\n",
        "\n",
        "#         # Modulate keys with pooled queries\n",
        "#         mixed_query_key_layer = mixed_key_layer * pooled_query_repeat\n",
        "\n",
        "#         # Compute key scores\n",
        "#         query_key_score = (self.key_att(mixed_query_key_layer) / self.attention_head_size**0.5).transpose(1, 2)\n",
        "#         if attention_mask is not None:\n",
        "#             query_key_score = query_key_score + attention_mask\n",
        "\n",
        "#         query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
        "#         key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
        "#         pooled_key = torch.matmul(query_key_weight, key_layer)\n",
        "\n",
        "#         # Compute final attention output\n",
        "#         weighted_value = (pooled_key * query_layer).transpose(1, 2)\n",
        "#         weighted_value = weighted_value.reshape(\n",
        "#             weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
        "#         attention_output = self.transform(weighted_value) + mixed_query_layer\n",
        "\n",
        "#         return attention_output\n",
        "\n",
        "# class FastAttention(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastAttention, self).__init__()\n",
        "#         self.self = FastSelfAttention(config)\n",
        "#         self.output = nn.Identity()  # Replace BertSelfOutput with Identity for simplicity\n",
        "\n",
        "#     def forward(self, input_tensor, attention_mask):\n",
        "#         self_output = self.self(input_tensor, attention_mask)\n",
        "#         attention_output = self.output(self_output)  # Assuming residual connection is handled elsewhere\n",
        "#         return attention_output\n",
        "\n",
        "# class FastformerLayer(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastformerLayer, self).__init__()\n",
        "#         self.attention = FastAttention(config)\n",
        "#         self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "#         self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "#         self.LayerNorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.LayerNorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#         self.activation = nn.GELU()\n",
        "\n",
        "#     def forward(self, hidden_states, attention_mask):\n",
        "#         attention_output = self.attention(hidden_states, attention_mask)\n",
        "#         attention_output = self.dropout(attention_output)\n",
        "#         attention_output = self.LayerNorm1(hidden_states + attention_output)\n",
        "\n",
        "#         intermediate_output = self.intermediate(attention_output)\n",
        "#         intermediate_output = self.activation(intermediate_output)\n",
        "\n",
        "#         layer_output = self.output(intermediate_output)\n",
        "#         layer_output = self.dropout(layer_output)\n",
        "#         layer_output = self.LayerNorm2(attention_output + layer_output)\n",
        "\n",
        "#         return layer_output\n",
        "\n",
        "# class FastformerEncoder(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(FastformerEncoder, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.encoders = nn.ModuleList([FastformerLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "#         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "#         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if isinstance(module, nn.Embedding) and module.padding_idx is not None:\n",
        "#                 with torch.no_grad():\n",
        "#                     module.weight[module.padding_idx].fill_(0)\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "#         if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "#             module.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, input_embs, attention_mask):\n",
        "#         batch_size, seq_length, emb_dim = input_embs.shape\n",
        "#         position_ids = torch.arange(seq_length, dtype=torch.long, device=input_embs.device)\n",
        "#         position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "#         position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "#         embeddings = input_embs + position_embeddings\n",
        "#         embeddings = self.LayerNorm(embeddings)\n",
        "#         embeddings = self.dropout(embeddings)\n",
        "\n",
        "#         extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "#         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "#         hidden_states = embeddings\n",
        "#         for layer_module in self.encoders:\n",
        "#             hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
        "\n",
        "#         return hidden_states\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f4QNrjKr92no"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomModel(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super(CustomModel, self).__init__()\n",
        "#         self.config = config\n",
        "#         self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "#         self.fastformer = FastSelfAttention(config)\n",
        "#         self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "#         self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "#         self.output_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
        "#         self.gelu = nn.GELU()\n",
        "#         self.apply(self.init_weights)\n",
        "\n",
        "#     def init_weights(self, module):\n",
        "#         if isinstance(module, (nn.Linear)):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "\n",
        "#     def compute_covariance(self, x):\n",
        "#         # x: [batch_size, seq_len, hidden_size]\n",
        "#         batch_size, seq_len, hidden_size = x.size()\n",
        "#         x_centered = x - x.mean(dim=1, keepdim=True)\n",
        "#         covariance = torch.bmm(x_centered.transpose(1, 2), x_centered) / (seq_len - 1 + 1e-5)\n",
        "#         # Reshape to [batch_size, 1, hidden_size]\n",
        "#         covariance = covariance.mean(dim=2, keepdim=True).permute(0, 2, 1)\n",
        "#         return covariance  # [batch_size, 1, hidden_size]\n",
        "\n",
        "#     def forward(self, x, attention_mask):\n",
        "#         # x: [batch_size, seq_len, embedding_dim]\n",
        "#         batch_size, seq_len, _ = x.size()\n",
        "\n",
        "#         # Normalize x\n",
        "#         x_norm = self.layernorm(x)\n",
        "\n",
        "#         # Compute U_i for i = 1, 3, 5\n",
        "#         U_indices = [1, 3, 5]\n",
        "#         U_list = []\n",
        "#         for i in U_indices:\n",
        "#             if i <= seq_len:\n",
        "#                 U_i = self.compute_covariance(x_norm[:, :i, :])\n",
        "#             else:\n",
        "#                 U_i = self.compute_covariance(x_norm)\n",
        "#             U_list.append(U_i)\n",
        "\n",
        "#         U1, U3, U5 = U_list\n",
        "\n",
        "#         # Prepare attention mask\n",
        "#         extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(dtype=x.dtype)\n",
        "#         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "#         # Compute P\n",
        "#         P1 = self.fastformer(U5, U3, extended_attention_mask)\n",
        "#         P2 = self.fastformer(U3, U1, extended_attention_mask)\n",
        "#         P3 = self.fastformer(U1, U5, extended_attention_mask)\n",
        "#         P = P1 + P2 + P3\n",
        "#         P_norm = self.layernorm(P)\n",
        "\n",
        "#         # Compute I = FC(GELU(FC(Norm(P))))\n",
        "#         I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "\n",
        "#         # Compute J = I + X\n",
        "#         seq_len_I = I.size(1)\n",
        "#         J = I + x[:, :seq_len_I, :]\n",
        "\n",
        "#         # Final output y = FC(Norm(J))\n",
        "#         J_norm = self.layernorm(J)\n",
        "#         y = self.output_fc(J_norm)\n",
        "#         y = y.mean(dim=1)  # Aggregate over sequence length\n",
        "\n",
        "#         return y\n",
        "\n",
        "# # Example usage:\n",
        "# # Assuming you have input embeddings x of shape [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "# batch_size = 10\n",
        "# sequence_length = 169  # Adjust as needed\n",
        "# embedding_dim = 768\n",
        "\n",
        "# # Create random input embeddings\n",
        "# x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# # Create attention mask\n",
        "# attention_mask = torch.ones(batch_size, sequence_length)\n",
        "\n",
        "# # Instantiate the configuration\n",
        "# class Config:\n",
        "#     def __init__(self):\n",
        "#         self.hidden_size = 768\n",
        "#         self.num_attention_heads = 12\n",
        "#         self.num_hidden_layers = 12\n",
        "#         self.intermediate_size = 3072\n",
        "#         self.hidden_dropout_prob = 0.1\n",
        "#         self.layer_norm_eps = 1e-12\n",
        "#         self.initializer_range = 0.02\n",
        "#         self.num_labels = 4\n",
        "#         self.max_position_embeddings = 512\n",
        "#         self.pooler_type = 'weightpooler'\n",
        "\n",
        "# config = Config()"
      ],
      "metadata": {
        "id": "izMmnkXBACEI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 10\n",
        "# sequence_length = 1  # Adjust as needed\n",
        "# embedding_dim = config.hidden_size\n",
        "\n",
        "# x = input_text_tensor\n",
        "\n",
        "# # Create attention mask\n",
        "# attention_mask = torch.ones(batch_size, sequence_length)\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = CustomModel(config)\n",
        "\n",
        "# # Forward pass\n",
        "# output = model(x, attention_mask)\n",
        "\n",
        "# print(\"Output shape:\", output.shape)  # Should be [batch_size, num_labels]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "BcyKUWuuBu3w",
        "outputId": "bce44e6a-7882-4999-aefb-9e1ac0548b83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[10, 1, 768]' is invalid for input of size 921600",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-343add14cc6c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should be [batch_size, num_labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e51ecba7da9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compute P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mP2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mP3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1a6f3be939dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_input, k_input, attention_mask)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).reshape(\n\u001b[0m\u001b[1;32m     77\u001b[0m             batch_size, 1, self.all_head_size)\n\u001b[1;32m     78\u001b[0m         \u001b[0mpooled_query_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 1, 768]' is invalid for input of size 921600"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt 2: Also Vanilla MSFastformer"
      ],
      "metadata": {
        "id": "yt9wkZg-Ojig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastSelfAttention, self).__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" %\n",
        "                (config.hidden_size, config.num_attention_heads))\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.input_dim = config.hidden_size\n",
        "\n",
        "        self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                       self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        # hidden_states: [batch_size, seq_len_hidden, hidden_size]\n",
        "        # context_states: [batch_size, seq_len_context, hidden_size]\n",
        "        batch_size, seq_len_hidden, _ = hidden_states.size()\n",
        "        seq_len_context = context_states.size(1)\n",
        "\n",
        "        # Compute queries and keys\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(context_states)\n",
        "\n",
        "        # Compute attention scores\n",
        "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "        key_for_score = self.key_att(mixed_key_layer).transpose(1, 2) / self.attention_head_size ** 0.5\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(query_for_score.transpose(1, 2), key_for_score)  # [batch_size, seq_len_hidden, seq_len_context]\n",
        "\n",
        "        # If attention_mask is provided, add it\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask should be broadcastable to [batch_size, seq_len_hidden, seq_len_context]\n",
        "            attention_scores += attention_mask\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_layer = torch.matmul(attention_weights, mixed_key_layer)  # [batch_size, seq_len_hidden, hidden_size]\n",
        "\n",
        "        # Combine with value projection\n",
        "        output = self.transform(context_layer) + mixed_query_layer\n",
        "\n",
        "        return output\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastAttention, self).__init__()\n",
        "        self.self = FastSelfAttention(config)\n",
        "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, input_tensor, context_tensor, attention_mask=None):\n",
        "        self_output = self.self(input_tensor, context_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output) + input_tensor\n",
        "        return attention_output\n",
        "\n",
        "class FastformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FastformerLayer, self).__init__()\n",
        "        self.attention = FastAttention(config)\n",
        "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.activation = nn.GELU()\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, context_states, attention_mask=None):\n",
        "        attention_output = self.attention(hidden_states, context_states, attention_mask)\n",
        "        attention_output = self.layernorm(attention_output)\n",
        "        intermediate_output = self.activation(self.intermediate(attention_output))\n",
        "        layer_output = self.output(intermediate_output) + attention_output\n",
        "        layer_output = self.layernorm(layer_output)\n",
        "        return layer_output"
      ],
      "metadata": {
        "id": "aV8rvRAFOqaf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSFastformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MSFastformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.fastformer = FastformerLayer(config)\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.output_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def compute_covariance(self, x):\n",
        "        # x: [batch_size, seq_len, hidden_size]\n",
        "        batch_size, seq_len, hidden_size = x.size()\n",
        "        x_centered = x - x.mean(dim=1, keepdim=True)\n",
        "        covariance = torch.bmm(x_centered.transpose(1, 2), x_centered) / (seq_len - 1 + 1e-5)\n",
        "        # Reshape to [batch_size, seq_len_cov, hidden_size]\n",
        "        covariance = covariance.mean(dim=2, keepdim=True).permute(0, 2, 1)\n",
        "        return covariance  # [batch_size, 1, hidden_size]\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        # x: [batch_size, seq_len, embedding_dim]\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Normalize x\n",
        "        x_norm = self.layernorm(x)\n",
        "\n",
        "        # Compute U_i for i = 1, 3, 5\n",
        "        U_indices = [1, 3, 5]\n",
        "        U_list = []\n",
        "        for i in U_indices:\n",
        "            if i <= seq_len:\n",
        "                U_i = self.compute_covariance(x_norm[:, :i, :])\n",
        "            else:\n",
        "                U_i = self.compute_covariance(x_norm)\n",
        "            U_list.append(U_i)\n",
        "\n",
        "        U1, U3, U5 = U_list\n",
        "\n",
        "        # Compute P\n",
        "        P1 = self.fastformer(U5, U3, attention_mask=None)\n",
        "        P2 = self.fastformer(U3, U1, attention_mask=None)\n",
        "        P3 = self.fastformer(U1, U5, attention_mask=None)\n",
        "        P = P1 + P2 + P3\n",
        "        P_norm = self.layernorm(P)\n",
        "\n",
        "        # Compute I = FC(GELU(FC(Norm(P))))\n",
        "        I = self.fc2(self.gelu(self.fc1(P_norm)))\n",
        "\n",
        "        # Compute J = I + x\n",
        "        seq_len_I = I.size(1)\n",
        "        J = I + x[:, :seq_len_I, :]\n",
        "\n",
        "        # Final output y = FC(Norm(J))\n",
        "        J_norm = self.layernorm(J)\n",
        "        y = self.output_fc(J_norm)\n",
        "        y = y.mean(dim=1)  # Aggregate over sequence length\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "MJk0m6hKYmeo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                 hidden_size=768,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 num_labels=2,\n",
        "                 num_hidden_layers=12,\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 layer_norm_eps=1e-12,\n",
        "                 initializer_range=0.02,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 pooler_type='weightpooler',\n",
        "                 num_attention_layers=12):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_labels = num_labels\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.initializer_range = initializer_range\n",
        "        self.hidden_act = hidden_act\n",
        "        self.pooler_type = pooler_type\n",
        "        self.num_attention_layers = num_attention_layers"
      ],
      "metadata": {
        "id": "lPtYPFJXYj5R"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(hidden_size=768, intermediate_size=3072, num_labels=2)\n",
        "model = MSFastformer(config)\n",
        "\n",
        "batch_size = 10\n",
        "seq_length = 169\n",
        "hidden_dim = 768\n",
        "x = input_text_tensor\n",
        "attention_mask = torch.ones(batch_size, seq_length)\n",
        "\n",
        "output = model(x, attention_mask)\n",
        "\n",
        "print(output.shape)  # Should be [batch_size, num_labels]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCzn1OGqRFTR",
        "outputId": "2bd37960-31dd-4155-b667-456447946ae5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 2])\n"
          ]
        }
      ]
    }
  ]
}